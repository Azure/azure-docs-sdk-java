### YamlMime:JavaType
uid: "com.azure.storage.file.datalake.DataLakeFileAsyncClient"
fullName: "com.azure.storage.file.datalake.DataLakeFileAsyncClient"
name: "DataLakeFileAsyncClient"
nameWithType: "DataLakeFileAsyncClient"
summary: "This class provides a client that contains file operations for Azure Storage Data Lake."
inheritances:
- "<xref href=\"java.lang.Object?displayProperty=fullName\" data-throw-if-not-resolved=\"False\" />"
- "<xref href=\"com.azure.storage.file.datalake.DataLakePathAsyncClient?displayProperty=fullName\" data-throw-if-not-resolved=\"False\" />"
inheritedMembers:
- "com.azure.storage.file.datalake.DataLakePathAsyncClient.create()"
- "com.azure.storage.file.datalake.DataLakePathAsyncClient.create(boolean)"
- "com.azure.storage.file.datalake.DataLakePathAsyncClient.createIfNotExists()"
- "com.azure.storage.file.datalake.DataLakePathAsyncClient.createIfNotExistsWithResponse(com.azure.storage.file.datalake.options.DataLakePathCreateOptions)"
- "com.azure.storage.file.datalake.DataLakePathAsyncClient.createWithResponse(com.azure.storage.file.datalake.options.DataLakePathCreateOptions)"
- "com.azure.storage.file.datalake.DataLakePathAsyncClient.createWithResponse(java.lang.String,java.lang.String,com.azure.storage.file.datalake.models.PathHttpHeaders,java.util.Map<java.lang.String,java.lang.String>,com.azure.storage.file.datalake.models.DataLakeRequestConditions)"
- "com.azure.storage.file.datalake.DataLakePathAsyncClient.deleteIfExists()"
- "com.azure.storage.file.datalake.DataLakePathAsyncClient.deleteIfExistsWithResponse(com.azure.storage.file.datalake.options.DataLakePathDeleteOptions)"
- "com.azure.storage.file.datalake.DataLakePathAsyncClient.exists()"
- "com.azure.storage.file.datalake.DataLakePathAsyncClient.existsWithResponse()"
- "com.azure.storage.file.datalake.DataLakePathAsyncClient.generateSas(com.azure.storage.file.datalake.sas.DataLakeServiceSasSignatureValues)"
- "com.azure.storage.file.datalake.DataLakePathAsyncClient.generateSas(com.azure.storage.file.datalake.sas.DataLakeServiceSasSignatureValues,com.azure.core.util.Context)"
- "com.azure.storage.file.datalake.DataLakePathAsyncClient.generateUserDelegationSas(com.azure.storage.file.datalake.sas.DataLakeServiceSasSignatureValues,com.azure.storage.file.datalake.models.UserDelegationKey)"
- "com.azure.storage.file.datalake.DataLakePathAsyncClient.generateUserDelegationSas(com.azure.storage.file.datalake.sas.DataLakeServiceSasSignatureValues,com.azure.storage.file.datalake.models.UserDelegationKey,java.lang.String,com.azure.core.util.Context)"
- "com.azure.storage.file.datalake.DataLakePathAsyncClient.getAccessControl()"
- "com.azure.storage.file.datalake.DataLakePathAsyncClient.getAccessControlWithResponse(boolean,com.azure.storage.file.datalake.models.DataLakeRequestConditions)"
- "com.azure.storage.file.datalake.DataLakePathAsyncClient.getAccountName()"
- "com.azure.storage.file.datalake.DataLakePathAsyncClient.getCustomerProvidedKey()"
- "com.azure.storage.file.datalake.DataLakePathAsyncClient.getCustomerProvidedKeyAsyncClient(com.azure.storage.file.datalake.models.CustomerProvidedKey)"
- "com.azure.storage.file.datalake.DataLakePathAsyncClient.getFileSystemName()"
- "com.azure.storage.file.datalake.DataLakePathAsyncClient.getHttpPipeline()"
- "com.azure.storage.file.datalake.DataLakePathAsyncClient.getProperties()"
- "com.azure.storage.file.datalake.DataLakePathAsyncClient.getPropertiesWithResponse(com.azure.storage.file.datalake.models.DataLakeRequestConditions)"
- "com.azure.storage.file.datalake.DataLakePathAsyncClient.getServiceVersion()"
- "com.azure.storage.file.datalake.DataLakePathAsyncClient.removeAccessControlRecursive(java.util.List<com.azure.storage.file.datalake.models.PathRemoveAccessControlEntry>)"
- "com.azure.storage.file.datalake.DataLakePathAsyncClient.removeAccessControlRecursiveWithResponse(com.azure.storage.file.datalake.options.PathRemoveAccessControlRecursiveOptions)"
- "com.azure.storage.file.datalake.DataLakePathAsyncClient.setAccessControlList(java.util.List<com.azure.storage.file.datalake.models.PathAccessControlEntry>,java.lang.String,java.lang.String)"
- "com.azure.storage.file.datalake.DataLakePathAsyncClient.setAccessControlListWithResponse(java.util.List<com.azure.storage.file.datalake.models.PathAccessControlEntry>,java.lang.String,java.lang.String,com.azure.storage.file.datalake.models.DataLakeRequestConditions)"
- "com.azure.storage.file.datalake.DataLakePathAsyncClient.setAccessControlRecursive(java.util.List<com.azure.storage.file.datalake.models.PathAccessControlEntry>)"
- "com.azure.storage.file.datalake.DataLakePathAsyncClient.setAccessControlRecursiveWithResponse(com.azure.storage.file.datalake.options.PathSetAccessControlRecursiveOptions)"
- "com.azure.storage.file.datalake.DataLakePathAsyncClient.setHttpHeaders(com.azure.storage.file.datalake.models.PathHttpHeaders)"
- "com.azure.storage.file.datalake.DataLakePathAsyncClient.setHttpHeadersWithResponse(com.azure.storage.file.datalake.models.PathHttpHeaders,com.azure.storage.file.datalake.models.DataLakeRequestConditions)"
- "com.azure.storage.file.datalake.DataLakePathAsyncClient.setMetadata(java.util.Map<java.lang.String,java.lang.String>)"
- "com.azure.storage.file.datalake.DataLakePathAsyncClient.setMetadataWithResponse(java.util.Map<java.lang.String,java.lang.String>,com.azure.storage.file.datalake.models.DataLakeRequestConditions)"
- "com.azure.storage.file.datalake.DataLakePathAsyncClient.setPermissions(com.azure.storage.file.datalake.models.PathPermissions,java.lang.String,java.lang.String)"
- "com.azure.storage.file.datalake.DataLakePathAsyncClient.setPermissionsWithResponse(com.azure.storage.file.datalake.models.PathPermissions,java.lang.String,java.lang.String,com.azure.storage.file.datalake.models.DataLakeRequestConditions)"
- "com.azure.storage.file.datalake.DataLakePathAsyncClient.updateAccessControlRecursive(java.util.List<com.azure.storage.file.datalake.models.PathAccessControlEntry>)"
- "com.azure.storage.file.datalake.DataLakePathAsyncClient.updateAccessControlRecursiveWithResponse(com.azure.storage.file.datalake.options.PathUpdateAccessControlRecursiveOptions)"
- "java.lang.Object.clone()"
- "java.lang.Object.equals(java.lang.Object)"
- "java.lang.Object.finalize()"
- "java.lang.Object.getClass()"
- "java.lang.Object.hashCode()"
- "java.lang.Object.notify()"
- "java.lang.Object.notifyAll()"
- "java.lang.Object.toString()"
- "java.lang.Object.wait()"
- "java.lang.Object.wait(long)"
- "java.lang.Object.wait(long,int)"
syntax: "public class DataLakeFileAsyncClient extends DataLakePathAsyncClient"
methods:
- uid: "com.azure.storage.file.datalake.DataLakeFileAsyncClient.append(reactor.core.publisher.Flux<java.nio.ByteBuffer>,long,long)"
  fullName: "com.azure.storage.file.datalake.DataLakeFileAsyncClient.append(Flux<ByteBuffer> data, long fileOffset, long length)"
  name: "append(Flux<ByteBuffer> data, long fileOffset, long length)"
  nameWithType: "DataLakeFileAsyncClient.append(Flux<ByteBuffer> data, long fileOffset, long length)"
  summary: "Appends data to the specified resource to later be flushed (written) by a call to flush"
  parameters:
  - description: "The data to write to the file."
    name: "data"
    type: "<xref href=\"reactor.core.publisher.Flux?alt=reactor.core.publisher.Flux&text=Flux\" data-throw-if-not-resolved=\"False\" />&lt;<xref href=\"java.nio.ByteBuffer?alt=java.nio.ByteBuffer&text=ByteBuffer\" data-throw-if-not-resolved=\"False\" />&gt;"
  - description: "The position where the data is to be appended."
    name: "fileOffset"
    type: "<xref href=\"long?alt=long&text=long\" data-throw-if-not-resolved=\"False\" />"
  - description: "The exact length of the data. It is important that this value match precisely the length of the\n data emitted by the <code>Flux</code>."
    name: "length"
    type: "<xref href=\"long?alt=long&text=long\" data-throw-if-not-resolved=\"False\" />"
  syntax: "public Mono<Void> append(Flux<ByteBuffer> data, long fileOffset, long length)"
  desc: "Appends data to the specified resource to later be flushed (written) by a call to flush\n\n**Code Samples**\n\n```java\nclient.append(data, offset, length)\n     .subscribe(\n         response -> System.out.println(\"Append data completed\"),\n         error -> System.out.printf(\"Error when calling append data: %s\", error));\n```\n\nFor more information, see the [Azure Docs][]\n\n\n[Azure Docs]: https://docs.microsoft.com/rest/api/storageservices/datalakestoragegen2/path/update"
  returns:
    description: "A reactive response signalling completion."
    type: "<xref href=\"reactor.core.publisher.Mono?alt=reactor.core.publisher.Mono&text=Mono\" data-throw-if-not-resolved=\"False\" />&lt;<xref href=\"java.lang.Void?alt=java.lang.Void&text=Void\" data-throw-if-not-resolved=\"False\" />&gt;"
- uid: "com.azure.storage.file.datalake.DataLakeFileAsyncClient.appendWithResponse(reactor.core.publisher.Flux<java.nio.ByteBuffer>,long,long,byte[],java.lang.String)"
  fullName: "com.azure.storage.file.datalake.DataLakeFileAsyncClient.appendWithResponse(Flux<ByteBuffer> data, long fileOffset, long length, byte[] contentMd5, String leaseId)"
  name: "appendWithResponse(Flux<ByteBuffer> data, long fileOffset, long length, byte[] contentMd5, String leaseId)"
  nameWithType: "DataLakeFileAsyncClient.appendWithResponse(Flux<ByteBuffer> data, long fileOffset, long length, byte[] contentMd5, String leaseId)"
  summary: "Appends data to the specified resource to later be flushed (written) by a call to flush"
  parameters:
  - description: "The data to write to the file."
    name: "data"
    type: "<xref href=\"reactor.core.publisher.Flux?alt=reactor.core.publisher.Flux&text=Flux\" data-throw-if-not-resolved=\"False\" />&lt;<xref href=\"java.nio.ByteBuffer?alt=java.nio.ByteBuffer&text=ByteBuffer\" data-throw-if-not-resolved=\"False\" />&gt;"
  - description: "The position where the data is to be appended."
    name: "fileOffset"
    type: "<xref href=\"long?alt=long&text=long\" data-throw-if-not-resolved=\"False\" />"
  - description: "The exact length of the data. It is important that this value match precisely the length of the\n data emitted by the <code>Flux</code>."
    name: "length"
    type: "<xref href=\"long?alt=long&text=long\" data-throw-if-not-resolved=\"False\" />"
  - description: "An MD5 hash of the content of the data. If specified, the service will calculate the MD5 of the\n received data and fail the request if it does not match the provided MD5."
    name: "contentMd5"
    type: "<xref href=\"byte?alt=byte&text=byte\" data-throw-if-not-resolved=\"False\" />[]"
  - description: "By setting lease id, requests will fail if the provided lease does not match the active lease on\n the file."
    name: "leaseId"
    type: "<xref href=\"java.lang.String?alt=java.lang.String&text=String\" data-throw-if-not-resolved=\"False\" />"
  syntax: "public Mono<Response<Void>> appendWithResponse(Flux<ByteBuffer> data, long fileOffset, long length, byte[] contentMd5, String leaseId)"
  desc: "Appends data to the specified resource to later be flushed (written) by a call to flush\n\n**Code Samples**\n\n```java\nFileRange range = new FileRange(1024, 2048L);\n DownloadRetryOptions options = new DownloadRetryOptions().setMaxRetryRequests(5);\n byte[] contentMd5 = new byte[0]; // Replace with valid md5\n\n client.appendWithResponse(data, offset, length, contentMd5, leaseId).subscribe(response ->\n     System.out.printf(\"Append data completed with status %d%n\", response.getStatusCode()));\n```\n\nFor more information, see the [Azure Docs][]\n\n\n[Azure Docs]: https://docs.microsoft.com/rest/api/storageservices/datalakestoragegen2/path/update"
  returns:
    description: "A reactive response signalling completion."
    type: "<xref href=\"reactor.core.publisher.Mono?alt=reactor.core.publisher.Mono&text=Mono\" data-throw-if-not-resolved=\"False\" />&lt;<xref href=\"com.azure.core.http.rest.Response?alt=com.azure.core.http.rest.Response&text=Response\" data-throw-if-not-resolved=\"False\" />&lt;<xref href=\"java.lang.Void?alt=java.lang.Void&text=Void\" data-throw-if-not-resolved=\"False\" />&gt;&gt;"
- uid: "com.azure.storage.file.datalake.DataLakeFileAsyncClient.delete()"
  fullName: "com.azure.storage.file.datalake.DataLakeFileAsyncClient.delete()"
  name: "delete()"
  nameWithType: "DataLakeFileAsyncClient.delete()"
  summary: "Deletes a file."
  syntax: "public Mono<Void> delete()"
  desc: "Deletes a file.\n\n**Code Samples**\n\n```java\nclient.delete().subscribe(response ->\n     System.out.println(\"Delete request completed\"));\n```\n\nFor more information see the [Azure Docs][]\n\n\n[Azure Docs]: https://docs.microsoft.com/rest/api/storageservices/datalakestoragegen2/path/delete"
  returns:
    description: "A reactive response signalling completion."
    type: "<xref href=\"reactor.core.publisher.Mono?alt=reactor.core.publisher.Mono&text=Mono\" data-throw-if-not-resolved=\"False\" />&lt;<xref href=\"java.lang.Void?alt=java.lang.Void&text=Void\" data-throw-if-not-resolved=\"False\" />&gt;"
- uid: "com.azure.storage.file.datalake.DataLakeFileAsyncClient.deleteIfExists()"
  fullName: "com.azure.storage.file.datalake.DataLakeFileAsyncClient.deleteIfExists()"
  name: "deleteIfExists()"
  nameWithType: "DataLakeFileAsyncClient.deleteIfExists()"
  summary: "Deletes a file if it exists."
  overridden: "com.azure.storage.file.datalake.DataLakePathAsyncClient.deleteIfExists()"
  syntax: "public Mono<Boolean> deleteIfExists()"
  desc: "Deletes a file if it exists.\n\n**Code Samples**\n\n```java\nclient.deleteIfExists().subscribe(deleted -> {\n     if (deleted) {\n         System.out.println(\"Successfully deleted.\");\n     } else {\n         System.out.println(\"Does not exist.\");\n     }\n });\n```\n\nFor more information see the [Azure Docs][]\n\n\n[Azure Docs]: https://docs.microsoft.com/rest/api/storageservices/datalakestoragegen2/path/delete"
  returns:
    description: "a reactive response signaling completion. <code>true</code> indicates that the file was successfully\n deleted, <code>false</code> indicates that the file did not exist."
    type: "<xref href=\"reactor.core.publisher.Mono?alt=reactor.core.publisher.Mono&text=Mono\" data-throw-if-not-resolved=\"False\" />&lt;<xref href=\"java.lang.Boolean?alt=java.lang.Boolean&text=Boolean\" data-throw-if-not-resolved=\"False\" />&gt;"
- uid: "com.azure.storage.file.datalake.DataLakeFileAsyncClient.deleteIfExistsWithResponse(com.azure.storage.file.datalake.options.DataLakePathDeleteOptions)"
  fullName: "com.azure.storage.file.datalake.DataLakeFileAsyncClient.deleteIfExistsWithResponse(DataLakePathDeleteOptions options)"
  name: "deleteIfExistsWithResponse(DataLakePathDeleteOptions options)"
  nameWithType: "DataLakeFileAsyncClient.deleteIfExistsWithResponse(DataLakePathDeleteOptions options)"
  summary: "Deletes a file if it exists."
  overridden: "com.azure.storage.file.datalake.DataLakePathAsyncClient.deleteIfExistsWithResponse(com.azure.storage.file.datalake.options.DataLakePathDeleteOptions)"
  parameters:
  - description: "<xref uid=\"com.azure.storage.file.datalake.options.DataLakePathDeleteOptions\" data-throw-if-not-resolved=\"false\" data-raw-source=\"DataLakePathDeleteOptions\"></xref>"
    name: "options"
    type: "<xref href=\"com.azure.storage.file.datalake.options.DataLakePathDeleteOptions?alt=com.azure.storage.file.datalake.options.DataLakePathDeleteOptions&text=DataLakePathDeleteOptions\" data-throw-if-not-resolved=\"False\" />"
  syntax: "public Mono<Response<Boolean>> deleteIfExistsWithResponse(DataLakePathDeleteOptions options)"
  desc: "Deletes a file if it exists.\n\n**Code Samples**\n\n```java\nDataLakeRequestConditions requestConditions = new DataLakeRequestConditions()\n     .setLeaseId(leaseId);\n DataLakePathDeleteOptions options = new DataLakePathDeleteOptions().setIsRecursive(false)\n     .setRequestConditions(requestConditions);\n\n client.deleteIfExistsWithResponse(options).subscribe(response -> {\n     if (response.getStatusCode() == 404) {\n         System.out.println(\"Does not exist.\");\n     } else {\n         System.out.println(\"successfully deleted.\");\n     }\n });\n```\n\nFor more information see the [Azure Docs][]\n\n\n[Azure Docs]: https://docs.microsoft.com/rest/api/storageservices/datalakestoragegen2/path/delete"
  returns:
    description: "A reactive response signaling completion. If <xref uid=\"com.azure.core.http.rest.Response\" data-throw-if-not-resolved=\"false\" data-raw-source=\"Response\"></xref>'s status code is 200, the file was\n successfully deleted. If status code is 404, the file does not exist."
    type: "<xref href=\"reactor.core.publisher.Mono?alt=reactor.core.publisher.Mono&text=Mono\" data-throw-if-not-resolved=\"False\" />&lt;<xref href=\"com.azure.core.http.rest.Response?alt=com.azure.core.http.rest.Response&text=Response\" data-throw-if-not-resolved=\"False\" />&lt;<xref href=\"java.lang.Boolean?alt=java.lang.Boolean&text=Boolean\" data-throw-if-not-resolved=\"False\" />&gt;&gt;"
- uid: "com.azure.storage.file.datalake.DataLakeFileAsyncClient.deleteWithResponse(com.azure.storage.file.datalake.models.DataLakeRequestConditions)"
  fullName: "com.azure.storage.file.datalake.DataLakeFileAsyncClient.deleteWithResponse(DataLakeRequestConditions requestConditions)"
  name: "deleteWithResponse(DataLakeRequestConditions requestConditions)"
  nameWithType: "DataLakeFileAsyncClient.deleteWithResponse(DataLakeRequestConditions requestConditions)"
  summary: "Deletes a file."
  parameters:
  - description: "<xref uid=\"com.azure.storage.file.datalake.models.DataLakeRequestConditions\" data-throw-if-not-resolved=\"false\" data-raw-source=\"DataLakeRequestConditions\"></xref>"
    name: "requestConditions"
    type: "<xref href=\"com.azure.storage.file.datalake.models.DataLakeRequestConditions?alt=com.azure.storage.file.datalake.models.DataLakeRequestConditions&text=DataLakeRequestConditions\" data-throw-if-not-resolved=\"False\" />"
  syntax: "public Mono<Response<Void>> deleteWithResponse(DataLakeRequestConditions requestConditions)"
  desc: "Deletes a file.\n\n**Code Samples**\n\n```java\nDataLakeRequestConditions requestConditions = new DataLakeRequestConditions()\n     .setLeaseId(leaseId);\n\n client.deleteWithResponse(requestConditions)\n     .subscribe(response -> System.out.println(\"Delete request completed\"));\n```\n\nFor more information see the [Azure Docs][]\n\n\n[Azure Docs]: https://docs.microsoft.com/rest/api/storageservices/datalakestoragegen2/path/delete"
  returns:
    description: "A reactive response signalling completion."
    type: "<xref href=\"reactor.core.publisher.Mono?alt=reactor.core.publisher.Mono&text=Mono\" data-throw-if-not-resolved=\"False\" />&lt;<xref href=\"com.azure.core.http.rest.Response?alt=com.azure.core.http.rest.Response&text=Response\" data-throw-if-not-resolved=\"False\" />&lt;<xref href=\"java.lang.Void?alt=java.lang.Void&text=Void\" data-throw-if-not-resolved=\"False\" />&gt;&gt;"
- uid: "com.azure.storage.file.datalake.DataLakeFileAsyncClient.flush(long)"
  fullName: "com.azure.storage.file.datalake.DataLakeFileAsyncClient.flush(long position)"
  name: "flush(long position)"
  nameWithType: "DataLakeFileAsyncClient.flush(long position)"
  summary: "Flushes (writes) data previously appended to the file through a call to append."
  parameters:
  - description: "The length of the file after all data has been written."
    name: "position"
    type: "<xref href=\"long?alt=long&text=long\" data-throw-if-not-resolved=\"False\" />"
  syntax: "public Mono<PathInfo> flush(long position)"
  desc: "Flushes (writes) data previously appended to the file through a call to append. The previously uploaded data must be contiguous.\n\nBy default this method will not overwrite existing data.\n\n**Code Samples**\n\n```java\nclient.flush(position).subscribe(response ->\n     System.out.println(\"Flush data completed\"));\n```\n\nFor more information, see the [Azure Docs][]\n\n\n[Azure Docs]: https://docs.microsoft.com/rest/api/storageservices/datalakestoragegen2/path/update"
  returns:
    description: "A reactive response containing the information of the created resource."
    type: "<xref href=\"reactor.core.publisher.Mono?alt=reactor.core.publisher.Mono&text=Mono\" data-throw-if-not-resolved=\"False\" />&lt;<xref href=\"com.azure.storage.file.datalake.models.PathInfo?alt=com.azure.storage.file.datalake.models.PathInfo&text=PathInfo\" data-throw-if-not-resolved=\"False\" />&gt;"
- uid: "com.azure.storage.file.datalake.DataLakeFileAsyncClient.flush(long,boolean)"
  fullName: "com.azure.storage.file.datalake.DataLakeFileAsyncClient.flush(long position, boolean overwrite)"
  name: "flush(long position, boolean overwrite)"
  nameWithType: "DataLakeFileAsyncClient.flush(long position, boolean overwrite)"
  summary: "Flushes (writes) data previously appended to the file through a call to append."
  parameters:
  - description: "The length of the file after all data has been written."
    name: "position"
    type: "<xref href=\"long?alt=long&text=long\" data-throw-if-not-resolved=\"False\" />"
  - description: "Whether to overwrite, should data exist on the file."
    name: "overwrite"
    type: "<xref href=\"boolean?alt=boolean&text=boolean\" data-throw-if-not-resolved=\"False\" />"
  syntax: "public Mono<PathInfo> flush(long position, boolean overwrite)"
  desc: "Flushes (writes) data previously appended to the file through a call to append. The previously uploaded data must be contiguous.\n\n**Code Samples**\n\n```java\nboolean overwrite = true;\n client.flush(position, overwrite).subscribe(response ->\n     System.out.println(\"Flush data completed\"));\n```\n\nFor more information, see the [Azure Docs][]\n\n\n[Azure Docs]: https://docs.microsoft.com/rest/api/storageservices/datalakestoragegen2/path/update"
  returns:
    description: "A reactive response containing the information of the created resource."
    type: "<xref href=\"reactor.core.publisher.Mono?alt=reactor.core.publisher.Mono&text=Mono\" data-throw-if-not-resolved=\"False\" />&lt;<xref href=\"com.azure.storage.file.datalake.models.PathInfo?alt=com.azure.storage.file.datalake.models.PathInfo&text=PathInfo\" data-throw-if-not-resolved=\"False\" />&gt;"
- uid: "com.azure.storage.file.datalake.DataLakeFileAsyncClient.flushWithResponse(long,boolean,boolean,com.azure.storage.file.datalake.models.PathHttpHeaders,com.azure.storage.file.datalake.models.DataLakeRequestConditions)"
  fullName: "com.azure.storage.file.datalake.DataLakeFileAsyncClient.flushWithResponse(long position, boolean retainUncommittedData, boolean close, PathHttpHeaders httpHeaders, DataLakeRequestConditions requestConditions)"
  name: "flushWithResponse(long position, boolean retainUncommittedData, boolean close, PathHttpHeaders httpHeaders, DataLakeRequestConditions requestConditions)"
  nameWithType: "DataLakeFileAsyncClient.flushWithResponse(long position, boolean retainUncommittedData, boolean close, PathHttpHeaders httpHeaders, DataLakeRequestConditions requestConditions)"
  summary: "Flushes (writes) data previously appended to the file through a call to append."
  parameters:
  - description: "The length of the file after all data has been written."
    name: "position"
    type: "<xref href=\"long?alt=long&text=long\" data-throw-if-not-resolved=\"False\" />"
  - description: "Whether uncommitted data is to be retained after the operation."
    name: "retainUncommittedData"
    type: "<xref href=\"boolean?alt=boolean&text=boolean\" data-throw-if-not-resolved=\"False\" />"
  - description: "Whether a file changed event raised indicates completion (true) or modification (false)."
    name: "close"
    type: "<xref href=\"boolean?alt=boolean&text=boolean\" data-throw-if-not-resolved=\"False\" />"
  - description: "<xref uid=\"com.azure.storage.file.datalake.models.PathHttpHeaders\" data-throw-if-not-resolved=\"false\" data-raw-source=\"httpHeaders\"></xref>"
    name: "httpHeaders"
    type: "<xref href=\"com.azure.storage.file.datalake.models.PathHttpHeaders?alt=com.azure.storage.file.datalake.models.PathHttpHeaders&text=PathHttpHeaders\" data-throw-if-not-resolved=\"False\" />"
  - description: "<xref uid=\"com.azure.storage.file.datalake.models.DataLakeRequestConditions\" data-throw-if-not-resolved=\"false\" data-raw-source=\"requestConditions\"></xref>"
    name: "requestConditions"
    type: "<xref href=\"com.azure.storage.file.datalake.models.DataLakeRequestConditions?alt=com.azure.storage.file.datalake.models.DataLakeRequestConditions&text=DataLakeRequestConditions\" data-throw-if-not-resolved=\"False\" />"
  syntax: "public Mono<Response<PathInfo>> flushWithResponse(long position, boolean retainUncommittedData, boolean close, PathHttpHeaders httpHeaders, DataLakeRequestConditions requestConditions)"
  desc: "Flushes (writes) data previously appended to the file through a call to append. The previously uploaded data must be contiguous.\n\n**Code Samples**\n\n```java\nFileRange range = new FileRange(1024, 2048L);\n DownloadRetryOptions options = new DownloadRetryOptions().setMaxRetryRequests(5);\n byte[] contentMd5 = new byte[0]; // Replace with valid md5\n boolean retainUncommittedData = false;\n boolean close = false;\n PathHttpHeaders httpHeaders = new PathHttpHeaders()\n     .setContentLanguage(\"en-US\")\n     .setContentType(\"binary\");\n DataLakeRequestConditions requestConditions = new DataLakeRequestConditions()\n     .setLeaseId(leaseId);\n\n client.flushWithResponse(position, retainUncommittedData, close, httpHeaders,\n     requestConditions).subscribe(response ->\n     System.out.printf(\"Flush data completed with status %d%n\", response.getStatusCode()));\n```\n\nFor more information, see the [Azure Docs][]\n\n\n[Azure Docs]: https://docs.microsoft.com/rest/api/storageservices/datalakestoragegen2/path/update"
  returns:
    description: "A reactive response containing the information of the created resource."
    type: "<xref href=\"reactor.core.publisher.Mono?alt=reactor.core.publisher.Mono&text=Mono\" data-throw-if-not-resolved=\"False\" />&lt;<xref href=\"com.azure.core.http.rest.Response?alt=com.azure.core.http.rest.Response&text=Response\" data-throw-if-not-resolved=\"False\" />&lt;<xref href=\"com.azure.storage.file.datalake.models.PathInfo?alt=com.azure.storage.file.datalake.models.PathInfo&text=PathInfo\" data-throw-if-not-resolved=\"False\" />&gt;&gt;"
- uid: "com.azure.storage.file.datalake.DataLakeFileAsyncClient.getCustomerProvidedKeyAsyncClient(com.azure.storage.file.datalake.models.CustomerProvidedKey)"
  fullName: "com.azure.storage.file.datalake.DataLakeFileAsyncClient.getCustomerProvidedKeyAsyncClient(CustomerProvidedKey customerProvidedKey)"
  name: "getCustomerProvidedKeyAsyncClient(CustomerProvidedKey customerProvidedKey)"
  nameWithType: "DataLakeFileAsyncClient.getCustomerProvidedKeyAsyncClient(CustomerProvidedKey customerProvidedKey)"
  summary: "Creates a new <xref uid=\"com.azure.storage.file.datalake.DataLakeFileAsyncClient\" data-throw-if-not-resolved=\"false\" data-raw-source=\"DataLakeFileAsyncClient\"></xref> with the specified `customerProvidedKey`."
  overridden: "com.azure.storage.file.datalake.DataLakePathAsyncClient.getCustomerProvidedKeyAsyncClient(com.azure.storage.file.datalake.models.CustomerProvidedKey)"
  parameters:
  - description: "the <xref uid=\"com.azure.storage.file.datalake.models.CustomerProvidedKey\" data-throw-if-not-resolved=\"false\" data-raw-source=\"CustomerProvidedKey\"></xref> for the file,\n pass <code>null</code> to use no customer provided key."
    name: "customerProvidedKey"
    type: "<xref href=\"com.azure.storage.file.datalake.models.CustomerProvidedKey?alt=com.azure.storage.file.datalake.models.CustomerProvidedKey&text=CustomerProvidedKey\" data-throw-if-not-resolved=\"False\" />"
  syntax: "public DataLakeFileAsyncClient getCustomerProvidedKeyAsyncClient(CustomerProvidedKey customerProvidedKey)"
  desc: "Creates a new <xref uid=\"com.azure.storage.file.datalake.DataLakeFileAsyncClient\" data-throw-if-not-resolved=\"false\" data-raw-source=\"DataLakeFileAsyncClient\"></xref> with the specified `customerProvidedKey`."
  returns:
    description: "a <xref uid=\"com.azure.storage.file.datalake.DataLakeFileAsyncClient\" data-throw-if-not-resolved=\"false\" data-raw-source=\"DataLakeFileAsyncClient\"></xref> with the specified <code>customerProvidedKey</code>."
    type: "<xref href=\"com.azure.storage.file.datalake.DataLakeFileAsyncClient?alt=com.azure.storage.file.datalake.DataLakeFileAsyncClient&text=DataLakeFileAsyncClient\" data-throw-if-not-resolved=\"False\" />"
- uid: "com.azure.storage.file.datalake.DataLakeFileAsyncClient.getFileName()"
  fullName: "com.azure.storage.file.datalake.DataLakeFileAsyncClient.getFileName()"
  name: "getFileName()"
  nameWithType: "DataLakeFileAsyncClient.getFileName()"
  summary: "Gets the name of this file, not including its full path."
  syntax: "public String getFileName()"
  desc: "Gets the name of this file, not including its full path."
  returns:
    description: "The name of the file."
    type: "<xref href=\"java.lang.String?alt=java.lang.String&text=String\" data-throw-if-not-resolved=\"False\" />"
- uid: "com.azure.storage.file.datalake.DataLakeFileAsyncClient.getFilePath()"
  fullName: "com.azure.storage.file.datalake.DataLakeFileAsyncClient.getFilePath()"
  name: "getFilePath()"
  nameWithType: "DataLakeFileAsyncClient.getFilePath()"
  summary: "Gets the path of this file, not including the name of the resource itself."
  syntax: "public String getFilePath()"
  desc: "Gets the path of this file, not including the name of the resource itself."
  returns:
    description: "The path of the file."
    type: "<xref href=\"java.lang.String?alt=java.lang.String&text=String\" data-throw-if-not-resolved=\"False\" />"
- uid: "com.azure.storage.file.datalake.DataLakeFileAsyncClient.getFileUrl()"
  fullName: "com.azure.storage.file.datalake.DataLakeFileAsyncClient.getFileUrl()"
  name: "getFileUrl()"
  nameWithType: "DataLakeFileAsyncClient.getFileUrl()"
  summary: "Gets the URL of the file represented by this client on the Data Lake service."
  syntax: "public String getFileUrl()"
  desc: "Gets the URL of the file represented by this client on the Data Lake service."
  returns:
    description: "the URL."
    type: "<xref href=\"java.lang.String?alt=java.lang.String&text=String\" data-throw-if-not-resolved=\"False\" />"
- uid: "com.azure.storage.file.datalake.DataLakeFileAsyncClient.query(java.lang.String)"
  fullName: "com.azure.storage.file.datalake.DataLakeFileAsyncClient.query(String expression)"
  name: "query(String expression)"
  nameWithType: "DataLakeFileAsyncClient.query(String expression)"
  summary: "Queries the entire file."
  parameters:
  - description: "The query expression."
    name: "expression"
    type: "<xref href=\"java.lang.String?alt=java.lang.String&text=String\" data-throw-if-not-resolved=\"False\" />"
  syntax: "public Flux<ByteBuffer> query(String expression)"
  desc: "Queries the entire file.\n\nFor more information, see the [Azure Docs][]\n\n**Code Samples**\n\n```java\nByteArrayOutputStream queryData = new ByteArrayOutputStream();\n String expression = \"SELECT * from BlobStorage\";\n client.query(expression).subscribe(piece -> {\n     try {\n         queryData.write(piece.array());\n     } catch (IOException ex) {\n         throw new UncheckedIOException(ex);\n     }\n });\n```\n\n\n[Azure Docs]: https://docs.microsoft.com/rest/api/storageservices/query-blob-contents"
  returns:
    description: "A reactive response containing the queried data."
    type: "<xref href=\"reactor.core.publisher.Flux?alt=reactor.core.publisher.Flux&text=Flux\" data-throw-if-not-resolved=\"False\" />&lt;<xref href=\"java.nio.ByteBuffer?alt=java.nio.ByteBuffer&text=ByteBuffer\" data-throw-if-not-resolved=\"False\" />&gt;"
- uid: "com.azure.storage.file.datalake.DataLakeFileAsyncClient.queryWithResponse(com.azure.storage.file.datalake.options.FileQueryOptions)"
  fullName: "com.azure.storage.file.datalake.DataLakeFileAsyncClient.queryWithResponse(FileQueryOptions queryOptions)"
  name: "queryWithResponse(FileQueryOptions queryOptions)"
  nameWithType: "DataLakeFileAsyncClient.queryWithResponse(FileQueryOptions queryOptions)"
  summary: "Queries the entire file."
  parameters:
  - description: "<xref uid=\"com.azure.storage.file.datalake.options.FileQueryOptions\" data-throw-if-not-resolved=\"false\" data-raw-source=\"The query options\"></xref>"
    name: "queryOptions"
    type: "<xref href=\"com.azure.storage.file.datalake.options.FileQueryOptions?alt=com.azure.storage.file.datalake.options.FileQueryOptions&text=FileQueryOptions\" data-throw-if-not-resolved=\"False\" />"
  syntax: "public Mono<FileQueryAsyncResponse> queryWithResponse(FileQueryOptions queryOptions)"
  desc: "Queries the entire file.\n\nFor more information, see the [Azure Docs][]\n\n**Code Samples**\n\n```java\nString expression = \"SELECT * from BlobStorage\";\n FileQueryJsonSerialization input = new FileQueryJsonSerialization()\n     .setRecordSeparator('\\n');\n FileQueryDelimitedSerialization output = new FileQueryDelimitedSerialization()\n     .setEscapeChar('\\0')\n     .setColumnSeparator(',')\n     .setRecordSeparator('\\n')\n     .setFieldQuote('\\'')\n     .setHeadersPresent(true);\n DataLakeRequestConditions requestConditions = new DataLakeRequestConditions().setLeaseId(leaseId);\n Consumer<FileQueryError> errorConsumer = System.out::println;\n Consumer<FileQueryProgress> progressConsumer = progress -> System.out.println(\"total file bytes read: \"\n     + progress.getBytesScanned());\n FileQueryOptions queryOptions = new FileQueryOptions(expression)\n     .setInputSerialization(input)\n     .setOutputSerialization(output)\n     .setRequestConditions(requestConditions)\n     .setErrorConsumer(errorConsumer)\n     .setProgressConsumer(progressConsumer);\n\n client.queryWithResponse(queryOptions)\n     .subscribe(response -> {\n         ByteArrayOutputStream queryData = new ByteArrayOutputStream();\n         response.getValue().subscribe(piece -> {\n             try {\n                 queryData.write(piece.array());\n             } catch (IOException ex) {\n                 throw new UncheckedIOException(ex);\n             }\n         });\n     });\n```\n\n\n[Azure Docs]: https://docs.microsoft.com/rest/api/storageservices/query-blob-contents"
  returns:
    description: "A reactive response containing the queried data."
    type: "<xref href=\"reactor.core.publisher.Mono?alt=reactor.core.publisher.Mono&text=Mono\" data-throw-if-not-resolved=\"False\" />&lt;<xref href=\"com.azure.storage.file.datalake.models.FileQueryAsyncResponse?alt=com.azure.storage.file.datalake.models.FileQueryAsyncResponse&text=FileQueryAsyncResponse\" data-throw-if-not-resolved=\"False\" />&gt;"
- uid: "com.azure.storage.file.datalake.DataLakeFileAsyncClient.read()"
  fullName: "com.azure.storage.file.datalake.DataLakeFileAsyncClient.read()"
  name: "read()"
  nameWithType: "DataLakeFileAsyncClient.read()"
  summary: "Reads the entire file."
  syntax: "public Flux<ByteBuffer> read()"
  desc: "Reads the entire file.\n\n**Code Samples**\n\n```java\nByteArrayOutputStream downloadData = new ByteArrayOutputStream();\n client.read().subscribe(piece -> {\n     try {\n         downloadData.write(piece.array());\n     } catch (IOException ex) {\n         throw new UncheckedIOException(ex);\n     }\n });\n```\n\nFor more information, see the [Azure Docs][]\n\n\n[Azure Docs]: https://docs.microsoft.com/rest/api/storageservices/get-blob"
  returns:
    description: "A reactive response containing the file data."
    type: "<xref href=\"reactor.core.publisher.Flux?alt=reactor.core.publisher.Flux&text=Flux\" data-throw-if-not-resolved=\"False\" />&lt;<xref href=\"java.nio.ByteBuffer?alt=java.nio.ByteBuffer&text=ByteBuffer\" data-throw-if-not-resolved=\"False\" />&gt;"
- uid: "com.azure.storage.file.datalake.DataLakeFileAsyncClient.readToFile(java.lang.String)"
  fullName: "com.azure.storage.file.datalake.DataLakeFileAsyncClient.readToFile(String filePath)"
  name: "readToFile(String filePath)"
  nameWithType: "DataLakeFileAsyncClient.readToFile(String filePath)"
  summary: "Reads the entire file into a file specified by the path."
  parameters:
  - description: "A <xref uid=\"java.lang.String\" data-throw-if-not-resolved=\"false\" data-raw-source=\"String\"></xref> representing the filePath where the downloaded data will be written."
    name: "filePath"
    type: "<xref href=\"java.lang.String?alt=java.lang.String&text=String\" data-throw-if-not-resolved=\"False\" />"
  syntax: "public Mono<PathProperties> readToFile(String filePath)"
  desc: "Reads the entire file into a file specified by the path.\n\nThe file will be created and must not exist, if the file already exists a <xref uid=\"\" data-throw-if-not-resolved=\"false\" data-raw-source=\"FileAlreadyExistsException\"></xref> will be thrown.\n\n**Code Samples**\n\n```java\nclient.readToFile(file).subscribe(response -> System.out.println(\"Completed download to file\"));\n```\n\nFor more information, see the [Azure Docs][]\n\n\n[Azure Docs]: https://docs.microsoft.com/rest/api/storageservices/get-blob"
  returns:
    description: "A reactive response containing the file properties and metadata."
    type: "<xref href=\"reactor.core.publisher.Mono?alt=reactor.core.publisher.Mono&text=Mono\" data-throw-if-not-resolved=\"False\" />&lt;<xref href=\"com.azure.storage.file.datalake.models.PathProperties?alt=com.azure.storage.file.datalake.models.PathProperties&text=PathProperties\" data-throw-if-not-resolved=\"False\" />&gt;"
- uid: "com.azure.storage.file.datalake.DataLakeFileAsyncClient.readToFile(java.lang.String,boolean)"
  fullName: "com.azure.storage.file.datalake.DataLakeFileAsyncClient.readToFile(String filePath, boolean overwrite)"
  name: "readToFile(String filePath, boolean overwrite)"
  nameWithType: "DataLakeFileAsyncClient.readToFile(String filePath, boolean overwrite)"
  summary: "Reads the entire file into a file specified by the path."
  parameters:
  - description: "A <xref uid=\"java.lang.String\" data-throw-if-not-resolved=\"false\" data-raw-source=\"String\"></xref> representing the filePath where the downloaded data will be written."
    name: "filePath"
    type: "<xref href=\"java.lang.String?alt=java.lang.String&text=String\" data-throw-if-not-resolved=\"False\" />"
  - description: "Whether to overwrite the file, should the file exist."
    name: "overwrite"
    type: "<xref href=\"boolean?alt=boolean&text=boolean\" data-throw-if-not-resolved=\"False\" />"
  syntax: "public Mono<PathProperties> readToFile(String filePath, boolean overwrite)"
  desc: "Reads the entire file into a file specified by the path.\n\nIf overwrite is set to false, the file will be created and must not exist, if the file already exists a <xref uid=\"\" data-throw-if-not-resolved=\"false\" data-raw-source=\"FileAlreadyExistsException\"></xref> will be thrown.\n\n**Code Samples**\n\n```java\nboolean overwrite = false; // Default value\n client.readToFile(file, overwrite).subscribe(response -> System.out.println(\"Completed download to file\"));\n```\n\nFor more information, see the [Azure Docs][]\n\n\n[Azure Docs]: https://docs.microsoft.com/rest/api/storageservices/get-blob"
  returns:
    description: "A reactive response containing the file properties and metadata."
    type: "<xref href=\"reactor.core.publisher.Mono?alt=reactor.core.publisher.Mono&text=Mono\" data-throw-if-not-resolved=\"False\" />&lt;<xref href=\"com.azure.storage.file.datalake.models.PathProperties?alt=com.azure.storage.file.datalake.models.PathProperties&text=PathProperties\" data-throw-if-not-resolved=\"False\" />&gt;"
- uid: "com.azure.storage.file.datalake.DataLakeFileAsyncClient.readToFileWithResponse(java.lang.String,com.azure.storage.file.datalake.models.FileRange,com.azure.storage.common.ParallelTransferOptions,com.azure.storage.file.datalake.models.DownloadRetryOptions,com.azure.storage.file.datalake.models.DataLakeRequestConditions,boolean,java.util.Set<java.nio.file.OpenOption>)"
  fullName: "com.azure.storage.file.datalake.DataLakeFileAsyncClient.readToFileWithResponse(String filePath, FileRange range, ParallelTransferOptions parallelTransferOptions, DownloadRetryOptions options, DataLakeRequestConditions requestConditions, boolean rangeGetContentMd5, Set<OpenOption> openOptions)"
  name: "readToFileWithResponse(String filePath, FileRange range, ParallelTransferOptions parallelTransferOptions, DownloadRetryOptions options, DataLakeRequestConditions requestConditions, boolean rangeGetContentMd5, Set<OpenOption> openOptions)"
  nameWithType: "DataLakeFileAsyncClient.readToFileWithResponse(String filePath, FileRange range, ParallelTransferOptions parallelTransferOptions, DownloadRetryOptions options, DataLakeRequestConditions requestConditions, boolean rangeGetContentMd5, Set<OpenOption> openOptions)"
  summary: "Reads the entire file into a file specified by the path."
  parameters:
  - description: "A <xref uid=\"java.lang.String\" data-throw-if-not-resolved=\"false\" data-raw-source=\"String\"></xref> representing the filePath where the downloaded data will be written."
    name: "filePath"
    type: "<xref href=\"java.lang.String?alt=java.lang.String&text=String\" data-throw-if-not-resolved=\"False\" />"
  - description: "<xref uid=\"com.azure.storage.file.datalake.models.FileRange\" data-throw-if-not-resolved=\"false\" data-raw-source=\"FileRange\"></xref>"
    name: "range"
    type: "<xref href=\"com.azure.storage.file.datalake.models.FileRange?alt=com.azure.storage.file.datalake.models.FileRange&text=FileRange\" data-throw-if-not-resolved=\"False\" />"
  - description: "<xref uid=\"com.azure.storage.common.ParallelTransferOptions\" data-throw-if-not-resolved=\"false\" data-raw-source=\"ParallelTransferOptions\"></xref> to use to download to file. Number of parallel\n transfers parameter is ignored."
    name: "parallelTransferOptions"
    type: "<xref href=\"com.azure.storage.common.ParallelTransferOptions?alt=com.azure.storage.common.ParallelTransferOptions&text=ParallelTransferOptions\" data-throw-if-not-resolved=\"False\" />"
  - description: "<xref uid=\"com.azure.storage.file.datalake.models.DownloadRetryOptions\" data-throw-if-not-resolved=\"false\" data-raw-source=\"DownloadRetryOptions\"></xref>"
    name: "options"
    type: "<xref href=\"com.azure.storage.file.datalake.models.DownloadRetryOptions?alt=com.azure.storage.file.datalake.models.DownloadRetryOptions&text=DownloadRetryOptions\" data-throw-if-not-resolved=\"False\" />"
  - description: "<xref uid=\"com.azure.storage.file.datalake.models.DataLakeRequestConditions\" data-throw-if-not-resolved=\"false\" data-raw-source=\"DataLakeRequestConditions\"></xref>"
    name: "requestConditions"
    type: "<xref href=\"com.azure.storage.file.datalake.models.DataLakeRequestConditions?alt=com.azure.storage.file.datalake.models.DataLakeRequestConditions&text=DataLakeRequestConditions\" data-throw-if-not-resolved=\"False\" />"
  - description: "Whether the contentMD5 for the specified file range should be returned."
    name: "rangeGetContentMd5"
    type: "<xref href=\"boolean?alt=boolean&text=boolean\" data-throw-if-not-resolved=\"False\" />"
  - description: "<xref uid=\"java.nio.file.OpenOption\" data-throw-if-not-resolved=\"false\" data-raw-source=\"OpenOptions\"></xref> to use to configure how to open or create the file."
    name: "openOptions"
    type: "<xref href=\"java.util.Set?alt=java.util.Set&text=Set\" data-throw-if-not-resolved=\"False\" />&lt;<xref href=\"java.nio.file.OpenOption?alt=java.nio.file.OpenOption&text=OpenOption\" data-throw-if-not-resolved=\"False\" />&gt;"
  syntax: "public Mono<Response<PathProperties>> readToFileWithResponse(String filePath, FileRange range, ParallelTransferOptions parallelTransferOptions, DownloadRetryOptions options, DataLakeRequestConditions requestConditions, boolean rangeGetContentMd5, Set<OpenOption> openOptions)"
  desc: "Reads the entire file into a file specified by the path.\n\nBy default the file will be created and must not exist, if the file already exists a <xref uid=\"\" data-throw-if-not-resolved=\"false\" data-raw-source=\"FileAlreadyExistsException\"></xref> will be thrown. To override this behavior, provide appropriate <xref uid=\"java.nio.file.OpenOption\" data-throw-if-not-resolved=\"false\" data-raw-source=\"OpenOptions\"></xref>\n\n**Code Samples**\n\n```java\nFileRange fileRange = new FileRange(1024, 2048L);\n DownloadRetryOptions downloadRetryOptions = new DownloadRetryOptions().setMaxRetryRequests(5);\n Set<OpenOption> openOptions = new HashSet<>(Arrays.asList(StandardOpenOption.CREATE_NEW,\n     StandardOpenOption.WRITE, StandardOpenOption.READ)); // Default options\n\n client.readToFileWithResponse(file, fileRange, null, downloadRetryOptions, null, false, openOptions)\n     .subscribe(response -> System.out.println(\"Completed download to file\"));\n```\n\nFor more information, see the [Azure Docs][]\n\n\n[Azure Docs]: https://docs.microsoft.com/rest/api/storageservices/get-blob"
  returns:
    description: "A reactive response containing the file properties and metadata."
    type: "<xref href=\"reactor.core.publisher.Mono?alt=reactor.core.publisher.Mono&text=Mono\" data-throw-if-not-resolved=\"False\" />&lt;<xref href=\"com.azure.core.http.rest.Response?alt=com.azure.core.http.rest.Response&text=Response\" data-throw-if-not-resolved=\"False\" />&lt;<xref href=\"com.azure.storage.file.datalake.models.PathProperties?alt=com.azure.storage.file.datalake.models.PathProperties&text=PathProperties\" data-throw-if-not-resolved=\"False\" />&gt;&gt;"
- uid: "com.azure.storage.file.datalake.DataLakeFileAsyncClient.readWithResponse(com.azure.storage.file.datalake.models.FileRange,com.azure.storage.file.datalake.models.DownloadRetryOptions,com.azure.storage.file.datalake.models.DataLakeRequestConditions,boolean)"
  fullName: "com.azure.storage.file.datalake.DataLakeFileAsyncClient.readWithResponse(FileRange range, DownloadRetryOptions options, DataLakeRequestConditions requestConditions, boolean getRangeContentMd5)"
  name: "readWithResponse(FileRange range, DownloadRetryOptions options, DataLakeRequestConditions requestConditions, boolean getRangeContentMd5)"
  nameWithType: "DataLakeFileAsyncClient.readWithResponse(FileRange range, DownloadRetryOptions options, DataLakeRequestConditions requestConditions, boolean getRangeContentMd5)"
  summary: "Reads a range of bytes from a file."
  parameters:
  - description: "<xref uid=\"com.azure.storage.file.datalake.models.FileRange\" data-throw-if-not-resolved=\"false\" data-raw-source=\"FileRange\"></xref>"
    name: "range"
    type: "<xref href=\"com.azure.storage.file.datalake.models.FileRange?alt=com.azure.storage.file.datalake.models.FileRange&text=FileRange\" data-throw-if-not-resolved=\"False\" />"
  - description: "<xref uid=\"com.azure.storage.file.datalake.models.DownloadRetryOptions\" data-throw-if-not-resolved=\"false\" data-raw-source=\"DownloadRetryOptions\"></xref>"
    name: "options"
    type: "<xref href=\"com.azure.storage.file.datalake.models.DownloadRetryOptions?alt=com.azure.storage.file.datalake.models.DownloadRetryOptions&text=DownloadRetryOptions\" data-throw-if-not-resolved=\"False\" />"
  - description: "<xref uid=\"com.azure.storage.file.datalake.models.DataLakeRequestConditions\" data-throw-if-not-resolved=\"false\" data-raw-source=\"DataLakeRequestConditions\"></xref>"
    name: "requestConditions"
    type: "<xref href=\"com.azure.storage.file.datalake.models.DataLakeRequestConditions?alt=com.azure.storage.file.datalake.models.DataLakeRequestConditions&text=DataLakeRequestConditions\" data-throw-if-not-resolved=\"False\" />"
  - description: "Whether the contentMD5 for the specified file range should be returned."
    name: "getRangeContentMd5"
    type: "<xref href=\"boolean?alt=boolean&text=boolean\" data-throw-if-not-resolved=\"False\" />"
  syntax: "public Mono<FileReadAsyncResponse> readWithResponse(FileRange range, DownloadRetryOptions options, DataLakeRequestConditions requestConditions, boolean getRangeContentMd5)"
  desc: "Reads a range of bytes from a file.\n\n**Code Samples**\n\n```java\nFileRange range = new FileRange(1024, 2048L);\n DownloadRetryOptions options = new DownloadRetryOptions().setMaxRetryRequests(5);\n\n client.readWithResponse(range, options, null, false).subscribe(response -> {\n     ByteArrayOutputStream readData = new ByteArrayOutputStream();\n     response.getValue().subscribe(piece -> {\n         try {\n             readData.write(piece.array());\n         } catch (IOException ex) {\n             throw new UncheckedIOException(ex);\n         }\n     });\n });\n```\n\nFor more information, see the [Azure Docs][]\n\n\n[Azure Docs]: https://docs.microsoft.com/rest/api/storageservices/get-blob"
  returns:
    description: "A reactive response containing the file data."
    type: "<xref href=\"reactor.core.publisher.Mono?alt=reactor.core.publisher.Mono&text=Mono\" data-throw-if-not-resolved=\"False\" />&lt;<xref href=\"com.azure.storage.file.datalake.models.FileReadAsyncResponse?alt=com.azure.storage.file.datalake.models.FileReadAsyncResponse&text=FileReadAsyncResponse\" data-throw-if-not-resolved=\"False\" />&gt;"
- uid: "com.azure.storage.file.datalake.DataLakeFileAsyncClient.rename(java.lang.String,java.lang.String)"
  fullName: "com.azure.storage.file.datalake.DataLakeFileAsyncClient.rename(String destinationFileSystem, String destinationPath)"
  name: "rename(String destinationFileSystem, String destinationPath)"
  nameWithType: "DataLakeFileAsyncClient.rename(String destinationFileSystem, String destinationPath)"
  summary: "Moves the file to another location within the file system."
  parameters:
  - description: "The file system of the destination within the account.\n <code>null</code> for the current file system."
    name: "destinationFileSystem"
    type: "<xref href=\"java.lang.String?alt=java.lang.String&text=String\" data-throw-if-not-resolved=\"False\" />"
  - description: "Relative path from the file system to rename the file to, excludes the file system name.\n For example if you want to move a file with fileSystem = \"myfilesystem\", path = \"mydir/hello.txt\" to another path\n in myfilesystem (ex: newdir/hi.txt) then set the destinationPath = \"newdir/hi.txt\""
    name: "destinationPath"
    type: "<xref href=\"java.lang.String?alt=java.lang.String&text=String\" data-throw-if-not-resolved=\"False\" />"
  syntax: "public Mono<DataLakeFileAsyncClient> rename(String destinationFileSystem, String destinationPath)"
  desc: "Moves the file to another location within the file system. For more information see the [Azure Docs][].\n\n**Code Samples**\n\n```java\nDataLakeFileAsyncClient renamedClient = client.rename(fileSystemName, destinationPath).block();\n System.out.println(\"Directory Client has been renamed\");\n```\n\n\n[Azure Docs]: https://docs.microsoft.com/rest/api/storageservices/datalakestoragegen2/path/create"
  returns:
    description: "A <xref uid=\"reactor.core.publisher.Mono\" data-throw-if-not-resolved=\"false\" data-raw-source=\"Mono\"></xref> containing a <xref uid=\"com.azure.storage.file.datalake.DataLakeFileAsyncClient\" data-throw-if-not-resolved=\"false\" data-raw-source=\"DataLakeFileAsyncClient\"></xref> used to interact with the new file created."
    type: "<xref href=\"reactor.core.publisher.Mono?alt=reactor.core.publisher.Mono&text=Mono\" data-throw-if-not-resolved=\"False\" />&lt;<xref href=\"com.azure.storage.file.datalake.DataLakeFileAsyncClient?alt=com.azure.storage.file.datalake.DataLakeFileAsyncClient&text=DataLakeFileAsyncClient\" data-throw-if-not-resolved=\"False\" />&gt;"
- uid: "com.azure.storage.file.datalake.DataLakeFileAsyncClient.renameWithResponse(java.lang.String,java.lang.String,com.azure.storage.file.datalake.models.DataLakeRequestConditions,com.azure.storage.file.datalake.models.DataLakeRequestConditions)"
  fullName: "com.azure.storage.file.datalake.DataLakeFileAsyncClient.renameWithResponse(String destinationFileSystem, String destinationPath, DataLakeRequestConditions sourceRequestConditions, DataLakeRequestConditions destinationRequestConditions)"
  name: "renameWithResponse(String destinationFileSystem, String destinationPath, DataLakeRequestConditions sourceRequestConditions, DataLakeRequestConditions destinationRequestConditions)"
  nameWithType: "DataLakeFileAsyncClient.renameWithResponse(String destinationFileSystem, String destinationPath, DataLakeRequestConditions sourceRequestConditions, DataLakeRequestConditions destinationRequestConditions)"
  summary: "Moves the file to another location within the file system."
  parameters:
  - description: "The file system of the destination within the account.\n <code>null</code> for the current file system."
    name: "destinationFileSystem"
    type: "<xref href=\"java.lang.String?alt=java.lang.String&text=String\" data-throw-if-not-resolved=\"False\" />"
  - description: "Relative path from the file system to rename the file to, excludes the file system name.\n For example if you want to move a file with fileSystem = \"myfilesystem\", path = \"mydir/hello.txt\" to another path\n in myfilesystem (ex: newdir/hi.txt) then set the destinationPath = \"newdir/hi.txt\""
    name: "destinationPath"
    type: "<xref href=\"java.lang.String?alt=java.lang.String&text=String\" data-throw-if-not-resolved=\"False\" />"
  - description: "<xref uid=\"com.azure.storage.file.datalake.models.DataLakeRequestConditions\" data-throw-if-not-resolved=\"false\" data-raw-source=\"DataLakeRequestConditions\"></xref> against the source."
    name: "sourceRequestConditions"
    type: "<xref href=\"com.azure.storage.file.datalake.models.DataLakeRequestConditions?alt=com.azure.storage.file.datalake.models.DataLakeRequestConditions&text=DataLakeRequestConditions\" data-throw-if-not-resolved=\"False\" />"
  - description: "<xref uid=\"com.azure.storage.file.datalake.models.DataLakeRequestConditions\" data-throw-if-not-resolved=\"false\" data-raw-source=\"DataLakeRequestConditions\"></xref> against the destination."
    name: "destinationRequestConditions"
    type: "<xref href=\"com.azure.storage.file.datalake.models.DataLakeRequestConditions?alt=com.azure.storage.file.datalake.models.DataLakeRequestConditions&text=DataLakeRequestConditions\" data-throw-if-not-resolved=\"False\" />"
  syntax: "public Mono<Response<DataLakeFileAsyncClient>> renameWithResponse(String destinationFileSystem, String destinationPath, DataLakeRequestConditions sourceRequestConditions, DataLakeRequestConditions destinationRequestConditions)"
  desc: "Moves the file to another location within the file system. For more information, see the [Azure Docs][].\n\n**Code Samples**\n\n```java\nDataLakeRequestConditions sourceRequestConditions = new DataLakeRequestConditions()\n     .setLeaseId(leaseId);\n DataLakeRequestConditions destinationRequestConditions = new DataLakeRequestConditions();\n\n DataLakeFileAsyncClient newRenamedClient = client.renameWithResponse(fileSystemName, destinationPath,\n     sourceRequestConditions, destinationRequestConditions).block().getValue();\n System.out.println(\"Directory Client has been renamed\");\n```\n\n\n[Azure Docs]: https://docs.microsoft.com/rest/api/storageservices/datalakestoragegen2/path/create"
  returns:
    description: "A <xref uid=\"reactor.core.publisher.Mono\" data-throw-if-not-resolved=\"false\" data-raw-source=\"Mono\"></xref> containing a <xref uid=\"com.azure.core.http.rest.Response\" data-throw-if-not-resolved=\"false\" data-raw-source=\"Response\"></xref> whose <xref uid=\"com.azure.core.http.rest.Response.getValue*\" data-throw-if-not-resolved=\"false\" data-raw-source=\"value\"></xref> contains a <xref uid=\"com.azure.storage.file.datalake.DataLakeFileAsyncClient\" data-throw-if-not-resolved=\"false\" data-raw-source=\"DataLakeFileAsyncClient\"></xref> used to interact with the file created."
    type: "<xref href=\"reactor.core.publisher.Mono?alt=reactor.core.publisher.Mono&text=Mono\" data-throw-if-not-resolved=\"False\" />&lt;<xref href=\"com.azure.core.http.rest.Response?alt=com.azure.core.http.rest.Response&text=Response\" data-throw-if-not-resolved=\"False\" />&lt;<xref href=\"com.azure.storage.file.datalake.DataLakeFileAsyncClient?alt=com.azure.storage.file.datalake.DataLakeFileAsyncClient&text=DataLakeFileAsyncClient\" data-throw-if-not-resolved=\"False\" />&gt;&gt;"
- uid: "com.azure.storage.file.datalake.DataLakeFileAsyncClient.scheduleDeletion(com.azure.storage.file.datalake.options.FileScheduleDeletionOptions)"
  fullName: "com.azure.storage.file.datalake.DataLakeFileAsyncClient.scheduleDeletion(FileScheduleDeletionOptions options)"
  name: "scheduleDeletion(FileScheduleDeletionOptions options)"
  nameWithType: "DataLakeFileAsyncClient.scheduleDeletion(FileScheduleDeletionOptions options)"
  summary: "Schedules the file for deletion."
  parameters:
  - description: "Schedule deletion parameters."
    name: "options"
    type: "<xref href=\"com.azure.storage.file.datalake.options.FileScheduleDeletionOptions?alt=com.azure.storage.file.datalake.options.FileScheduleDeletionOptions&text=FileScheduleDeletionOptions\" data-throw-if-not-resolved=\"False\" />"
  syntax: "public Mono<Void> scheduleDeletion(FileScheduleDeletionOptions options)"
  desc: "Schedules the file for deletion.\n\n**Code Samples**\n\n```java\nFileScheduleDeletionOptions options = new FileScheduleDeletionOptions(OffsetDateTime.now().plusDays(1));\n\n client.scheduleDeletion(options)\n     .subscribe(r -> System.out.println(\"File deletion has been scheduled\"));\n```"
  returns:
    description: "A reactive response signalling completion."
    type: "<xref href=\"reactor.core.publisher.Mono?alt=reactor.core.publisher.Mono&text=Mono\" data-throw-if-not-resolved=\"False\" />&lt;<xref href=\"java.lang.Void?alt=java.lang.Void&text=Void\" data-throw-if-not-resolved=\"False\" />&gt;"
- uid: "com.azure.storage.file.datalake.DataLakeFileAsyncClient.scheduleDeletionWithResponse(com.azure.storage.file.datalake.options.FileScheduleDeletionOptions)"
  fullName: "com.azure.storage.file.datalake.DataLakeFileAsyncClient.scheduleDeletionWithResponse(FileScheduleDeletionOptions options)"
  name: "scheduleDeletionWithResponse(FileScheduleDeletionOptions options)"
  nameWithType: "DataLakeFileAsyncClient.scheduleDeletionWithResponse(FileScheduleDeletionOptions options)"
  summary: "Schedules the file for deletion."
  parameters:
  - description: "Schedule deletion parameters."
    name: "options"
    type: "<xref href=\"com.azure.storage.file.datalake.options.FileScheduleDeletionOptions?alt=com.azure.storage.file.datalake.options.FileScheduleDeletionOptions&text=FileScheduleDeletionOptions\" data-throw-if-not-resolved=\"False\" />"
  syntax: "public Mono<Response<Void>> scheduleDeletionWithResponse(FileScheduleDeletionOptions options)"
  desc: "Schedules the file for deletion.\n\n**Code Samples**\n\n```java\nFileScheduleDeletionOptions options = new FileScheduleDeletionOptions(OffsetDateTime.now().plusDays(1));\n\n client.scheduleDeletionWithResponse(options)\n     .subscribe(r -> System.out.println(\"File deletion has been scheduled\"));\n```"
  returns:
    description: "A reactive response signalling completion."
    type: "<xref href=\"reactor.core.publisher.Mono?alt=reactor.core.publisher.Mono&text=Mono\" data-throw-if-not-resolved=\"False\" />&lt;<xref href=\"com.azure.core.http.rest.Response?alt=com.azure.core.http.rest.Response&text=Response\" data-throw-if-not-resolved=\"False\" />&lt;<xref href=\"java.lang.Void?alt=java.lang.Void&text=Void\" data-throw-if-not-resolved=\"False\" />&gt;&gt;"
- uid: "com.azure.storage.file.datalake.DataLakeFileAsyncClient.upload(reactor.core.publisher.Flux<java.nio.ByteBuffer>,com.azure.storage.common.ParallelTransferOptions)"
  fullName: "com.azure.storage.file.datalake.DataLakeFileAsyncClient.upload(Flux<ByteBuffer> data, ParallelTransferOptions parallelTransferOptions)"
  name: "upload(Flux<ByteBuffer> data, ParallelTransferOptions parallelTransferOptions)"
  nameWithType: "DataLakeFileAsyncClient.upload(Flux<ByteBuffer> data, ParallelTransferOptions parallelTransferOptions)"
  summary: "Creates a new file and uploads content."
  parameters:
  - description: "The data to write to the file. Unlike other upload methods, this method does not require that the\n <code>Flux</code> be replayable. In other words, it does not have to support multiple subscribers and is not expected\n to produce the same values across subscriptions."
    name: "data"
    type: "<xref href=\"reactor.core.publisher.Flux?alt=reactor.core.publisher.Flux&text=Flux\" data-throw-if-not-resolved=\"False\" />&lt;<xref href=\"java.nio.ByteBuffer?alt=java.nio.ByteBuffer&text=ByteBuffer\" data-throw-if-not-resolved=\"False\" />&gt;"
  - description: "<xref uid=\"com.azure.storage.common.ParallelTransferOptions\" data-throw-if-not-resolved=\"false\" data-raw-source=\"ParallelTransferOptions\"></xref> used to configure buffered uploading."
    name: "parallelTransferOptions"
    type: "<xref href=\"com.azure.storage.common.ParallelTransferOptions?alt=com.azure.storage.common.ParallelTransferOptions&text=ParallelTransferOptions\" data-throw-if-not-resolved=\"False\" />"
  syntax: "public Mono<PathInfo> upload(Flux<ByteBuffer> data, ParallelTransferOptions parallelTransferOptions)"
  desc: "Creates a new file and uploads content.\n\n**Code Samples**\n\n```java\nclient.uploadFromFile(filePath)\n     .doOnError(throwable -> System.err.printf(\"Failed to upload from file %s%n\", throwable.getMessage()))\n     .subscribe(completion -> System.out.println(\"Upload from file succeeded\"));\n```"
  returns:
    description: "A reactive response containing the information of the uploaded file."
    type: "<xref href=\"reactor.core.publisher.Mono?alt=reactor.core.publisher.Mono&text=Mono\" data-throw-if-not-resolved=\"False\" />&lt;<xref href=\"com.azure.storage.file.datalake.models.PathInfo?alt=com.azure.storage.file.datalake.models.PathInfo&text=PathInfo\" data-throw-if-not-resolved=\"False\" />&gt;"
- uid: "com.azure.storage.file.datalake.DataLakeFileAsyncClient.upload(reactor.core.publisher.Flux<java.nio.ByteBuffer>,com.azure.storage.common.ParallelTransferOptions,boolean)"
  fullName: "com.azure.storage.file.datalake.DataLakeFileAsyncClient.upload(Flux<ByteBuffer> data, ParallelTransferOptions parallelTransferOptions, boolean overwrite)"
  name: "upload(Flux<ByteBuffer> data, ParallelTransferOptions parallelTransferOptions, boolean overwrite)"
  nameWithType: "DataLakeFileAsyncClient.upload(Flux<ByteBuffer> data, ParallelTransferOptions parallelTransferOptions, boolean overwrite)"
  summary: "Creates a new file and uploads content."
  parameters:
  - description: "The data to write to the file. Unlike other upload methods, this method does not require that the\n <code>Flux</code> be replayable. In other words, it does not have to support multiple subscribers and is not expected\n to produce the same values across subscriptions."
    name: "data"
    type: "<xref href=\"reactor.core.publisher.Flux?alt=reactor.core.publisher.Flux&text=Flux\" data-throw-if-not-resolved=\"False\" />&lt;<xref href=\"java.nio.ByteBuffer?alt=java.nio.ByteBuffer&text=ByteBuffer\" data-throw-if-not-resolved=\"False\" />&gt;"
  - description: "<xref uid=\"com.azure.storage.common.ParallelTransferOptions\" data-throw-if-not-resolved=\"false\" data-raw-source=\"ParallelTransferOptions\"></xref> used to configure buffered uploading."
    name: "parallelTransferOptions"
    type: "<xref href=\"com.azure.storage.common.ParallelTransferOptions?alt=com.azure.storage.common.ParallelTransferOptions&text=ParallelTransferOptions\" data-throw-if-not-resolved=\"False\" />"
  - description: "Whether to overwrite, should the file already exist."
    name: "overwrite"
    type: "<xref href=\"boolean?alt=boolean&text=boolean\" data-throw-if-not-resolved=\"False\" />"
  syntax: "public Mono<PathInfo> upload(Flux<ByteBuffer> data, ParallelTransferOptions parallelTransferOptions, boolean overwrite)"
  desc: "Creates a new file and uploads content.\n\n**Code Samples**\n\n```java\nboolean overwrite = false; // Default behavior\n client.uploadFromFile(filePath, overwrite)\n     .doOnError(throwable -> System.err.printf(\"Failed to upload from file %s%n\", throwable.getMessage()))\n     .subscribe(completion -> System.out.println(\"Upload from file succeeded\"));\n```"
  returns:
    description: "A reactive response containing the information of the uploaded file."
    type: "<xref href=\"reactor.core.publisher.Mono?alt=reactor.core.publisher.Mono&text=Mono\" data-throw-if-not-resolved=\"False\" />&lt;<xref href=\"com.azure.storage.file.datalake.models.PathInfo?alt=com.azure.storage.file.datalake.models.PathInfo&text=PathInfo\" data-throw-if-not-resolved=\"False\" />&gt;"
- uid: "com.azure.storage.file.datalake.DataLakeFileAsyncClient.uploadFromFile(java.lang.String)"
  fullName: "com.azure.storage.file.datalake.DataLakeFileAsyncClient.uploadFromFile(String filePath)"
  name: "uploadFromFile(String filePath)"
  nameWithType: "DataLakeFileAsyncClient.uploadFromFile(String filePath)"
  summary: "Creates a new file, with the content of the specified file."
  parameters:
  - description: "Path to the upload file"
    name: "filePath"
    type: "<xref href=\"java.lang.String?alt=java.lang.String&text=String\" data-throw-if-not-resolved=\"False\" />"
  syntax: "public Mono<Void> uploadFromFile(String filePath)"
  desc: "Creates a new file, with the content of the specified file. By default, this method will not overwrite an existing file.\n\n**Code Samples**\n\n```java\nclient.uploadFromFile(filePath)\n     .doOnError(throwable -> System.err.printf(\"Failed to upload from file %s%n\", throwable.getMessage()))\n     .subscribe(completion -> System.out.println(\"Upload from file succeeded\"));\n```"
  returns:
    description: "An empty response"
    type: "<xref href=\"reactor.core.publisher.Mono?alt=reactor.core.publisher.Mono&text=Mono\" data-throw-if-not-resolved=\"False\" />&lt;<xref href=\"java.lang.Void?alt=java.lang.Void&text=Void\" data-throw-if-not-resolved=\"False\" />&gt;"
- uid: "com.azure.storage.file.datalake.DataLakeFileAsyncClient.uploadFromFile(java.lang.String,boolean)"
  fullName: "com.azure.storage.file.datalake.DataLakeFileAsyncClient.uploadFromFile(String filePath, boolean overwrite)"
  name: "uploadFromFile(String filePath, boolean overwrite)"
  nameWithType: "DataLakeFileAsyncClient.uploadFromFile(String filePath, boolean overwrite)"
  summary: "Creates a new file, with the content of the specified file."
  parameters:
  - description: "Path to the upload file"
    name: "filePath"
    type: "<xref href=\"java.lang.String?alt=java.lang.String&text=String\" data-throw-if-not-resolved=\"False\" />"
  - description: "Whether to overwrite, should the file already exist."
    name: "overwrite"
    type: "<xref href=\"boolean?alt=boolean&text=boolean\" data-throw-if-not-resolved=\"False\" />"
  syntax: "public Mono<Void> uploadFromFile(String filePath, boolean overwrite)"
  desc: "Creates a new file, with the content of the specified file.\n\n**Code Samples**\n\n```java\nboolean overwrite = false; // Default behavior\n client.uploadFromFile(filePath, overwrite)\n     .doOnError(throwable -> System.err.printf(\"Failed to upload from file %s%n\", throwable.getMessage()))\n     .subscribe(completion -> System.out.println(\"Upload from file succeeded\"));\n```"
  returns:
    description: "An empty response"
    type: "<xref href=\"reactor.core.publisher.Mono?alt=reactor.core.publisher.Mono&text=Mono\" data-throw-if-not-resolved=\"False\" />&lt;<xref href=\"java.lang.Void?alt=java.lang.Void&text=Void\" data-throw-if-not-resolved=\"False\" />&gt;"
- uid: "com.azure.storage.file.datalake.DataLakeFileAsyncClient.uploadFromFile(java.lang.String,com.azure.storage.common.ParallelTransferOptions,com.azure.storage.file.datalake.models.PathHttpHeaders,java.util.Map<java.lang.String,java.lang.String>,com.azure.storage.file.datalake.models.DataLakeRequestConditions)"
  fullName: "com.azure.storage.file.datalake.DataLakeFileAsyncClient.uploadFromFile(String filePath, ParallelTransferOptions parallelTransferOptions, PathHttpHeaders headers, Map<String,String> metadata, DataLakeRequestConditions requestConditions)"
  name: "uploadFromFile(String filePath, ParallelTransferOptions parallelTransferOptions, PathHttpHeaders headers, Map<String,String> metadata, DataLakeRequestConditions requestConditions)"
  nameWithType: "DataLakeFileAsyncClient.uploadFromFile(String filePath, ParallelTransferOptions parallelTransferOptions, PathHttpHeaders headers, Map<String,String> metadata, DataLakeRequestConditions requestConditions)"
  summary: "Creates a new file, with the content of the specified file."
  parameters:
  - description: "Path to the upload file"
    name: "filePath"
    type: "<xref href=\"java.lang.String?alt=java.lang.String&text=String\" data-throw-if-not-resolved=\"False\" />"
  - description: "<xref uid=\"com.azure.storage.common.ParallelTransferOptions\" data-throw-if-not-resolved=\"false\" data-raw-source=\"ParallelTransferOptions\"></xref> to use to upload from file. Number of parallel\n transfers parameter is ignored."
    name: "parallelTransferOptions"
    type: "<xref href=\"com.azure.storage.common.ParallelTransferOptions?alt=com.azure.storage.common.ParallelTransferOptions&text=ParallelTransferOptions\" data-throw-if-not-resolved=\"False\" />"
  - description: "<xref uid=\"com.azure.storage.file.datalake.models.PathHttpHeaders\" data-throw-if-not-resolved=\"false\" data-raw-source=\"PathHttpHeaders\"></xref>"
    name: "headers"
    type: "<xref href=\"com.azure.storage.file.datalake.models.PathHttpHeaders?alt=com.azure.storage.file.datalake.models.PathHttpHeaders&text=PathHttpHeaders\" data-throw-if-not-resolved=\"False\" />"
  - description: "Metadata to associate with the resource. If there is leading or trailing whitespace in any\n metadata key or value, it must be removed or encoded."
    name: "metadata"
    type: "<xref href=\"java.util.Map?alt=java.util.Map&text=Map\" data-throw-if-not-resolved=\"False\" />&lt;<xref href=\"java.lang.String?alt=java.lang.String&text=String\" data-throw-if-not-resolved=\"False\" />,<xref href=\"java.lang.String?alt=java.lang.String&text=String\" data-throw-if-not-resolved=\"False\" />&gt;"
  - description: "<xref uid=\"com.azure.storage.file.datalake.models.DataLakeRequestConditions\" data-throw-if-not-resolved=\"false\" data-raw-source=\"DataLakeRequestConditions\"></xref>"
    name: "requestConditions"
    type: "<xref href=\"com.azure.storage.file.datalake.models.DataLakeRequestConditions?alt=com.azure.storage.file.datalake.models.DataLakeRequestConditions&text=DataLakeRequestConditions\" data-throw-if-not-resolved=\"False\" />"
  syntax: "public Mono<Void> uploadFromFile(String filePath, ParallelTransferOptions parallelTransferOptions, PathHttpHeaders headers, Map<String,String> metadata, DataLakeRequestConditions requestConditions)"
  desc: "Creates a new file, with the content of the specified file.\n\nTo avoid overwriting, pass \"\\*\" to <xref uid=\"com.azure.storage.file.datalake.models.DataLakeRequestConditions.setIfNoneMatch(java.lang.String)\" data-throw-if-not-resolved=\"false\" data-raw-source=\"DataLakeRequestConditions#setIfNoneMatch(String)\"></xref>.\n\n**Code Samples**\n\n```java\nPathHttpHeaders headers = new PathHttpHeaders()\n     .setContentMd5(\"data\".getBytes(StandardCharsets.UTF_8))\n     .setContentLanguage(\"en-US\")\n     .setContentType(\"binary\");\n\n Map<String, String> metadata = Collections.singletonMap(\"metadata\", \"value\");\n DataLakeRequestConditions requestConditions = new DataLakeRequestConditions()\n     .setLeaseId(leaseId)\n     .setIfUnmodifiedSince(OffsetDateTime.now().minusDays(3));\n Long blockSize = 100L * 1024L * 1024L; // 100 MB;\n ParallelTransferOptions parallelTransferOptions = new ParallelTransferOptions().setBlockSizeLong(blockSize);\n\n client.uploadFromFile(filePath, parallelTransferOptions, headers, metadata, requestConditions)\n     .doOnError(throwable -> System.err.printf(\"Failed to upload from file %s%n\", throwable.getMessage()))\n     .subscribe(completion -> System.out.println(\"Upload from file succeeded\"));\n```"
  returns:
    description: "An empty response"
    type: "<xref href=\"reactor.core.publisher.Mono?alt=reactor.core.publisher.Mono&text=Mono\" data-throw-if-not-resolved=\"False\" />&lt;<xref href=\"java.lang.Void?alt=java.lang.Void&text=Void\" data-throw-if-not-resolved=\"False\" />&gt;"
- uid: "com.azure.storage.file.datalake.DataLakeFileAsyncClient.uploadWithResponse(com.azure.storage.file.datalake.options.FileParallelUploadOptions)"
  fullName: "com.azure.storage.file.datalake.DataLakeFileAsyncClient.uploadWithResponse(FileParallelUploadOptions options)"
  name: "uploadWithResponse(FileParallelUploadOptions options)"
  nameWithType: "DataLakeFileAsyncClient.uploadWithResponse(FileParallelUploadOptions options)"
  summary: "Creates a new file."
  parameters:
  - description: "<xref uid=\"com.azure.storage.file.datalake.options.FileParallelUploadOptions\" data-throw-if-not-resolved=\"false\" data-raw-source=\"FileParallelUploadOptions\"></xref>"
    name: "options"
    type: "<xref href=\"com.azure.storage.file.datalake.options.FileParallelUploadOptions?alt=com.azure.storage.file.datalake.options.FileParallelUploadOptions&text=FileParallelUploadOptions\" data-throw-if-not-resolved=\"False\" />"
  syntax: "public Mono<Response<PathInfo>> uploadWithResponse(FileParallelUploadOptions options)"
  desc: "Creates a new file.\n\nTo avoid overwriting, pass \"\\*\" to <xref uid=\"com.azure.storage.file.datalake.models.DataLakeRequestConditions.setIfNoneMatch(java.lang.String)\" data-throw-if-not-resolved=\"false\" data-raw-source=\"DataLakeRequestConditions#setIfNoneMatch(String)\"></xref>.\n\n**Code Samples**\n\n```java\nPathHttpHeaders headers = new PathHttpHeaders()\n     .setContentMd5(\"data\".getBytes(StandardCharsets.UTF_8))\n     .setContentLanguage(\"en-US\")\n     .setContentType(\"binary\");\n\n Map<String, String> metadata = Collections.singletonMap(\"metadata\", \"value\");\n DataLakeRequestConditions requestConditions = new DataLakeRequestConditions()\n     .setLeaseId(leaseId)\n     .setIfUnmodifiedSince(OffsetDateTime.now().minusDays(3));\n Long blockSize = 100L * 1024L * 1024L; // 100 MB;\n ParallelTransferOptions parallelTransferOptions = new ParallelTransferOptions().setBlockSizeLong(blockSize);\n\n client.uploadWithResponse(new FileParallelUploadOptions(data)\n     .setParallelTransferOptions(parallelTransferOptions).setHeaders(headers)\n     .setMetadata(metadata).setRequestConditions(requestConditions)\n     .setPermissions(\"permissions\").setUmask(\"umask\"))\n     .subscribe(response -> System.out.println(\"Uploaded file %n\"));\n```\n\n**Using Progress Reporting**\n\n```java\nPathHttpHeaders httpHeaders = new PathHttpHeaders()\n     .setContentMd5(\"data\".getBytes(StandardCharsets.UTF_8))\n     .setContentLanguage(\"en-US\")\n     .setContentType(\"binary\");\n\n Map<String, String> metadataMap = Collections.singletonMap(\"metadata\", \"value\");\n DataLakeRequestConditions conditions = new DataLakeRequestConditions()\n     .setLeaseId(leaseId)\n     .setIfUnmodifiedSince(OffsetDateTime.now().minusDays(3));\n ParallelTransferOptions pto = new ParallelTransferOptions()\n     .setBlockSizeLong(blockSize)\n     .setProgressReceiver(bytesTransferred -> System.out.printf(\"Upload progress: %s bytes sent\", bytesTransferred));\n\n client.uploadWithResponse(new FileParallelUploadOptions(data)\n     .setParallelTransferOptions(parallelTransferOptions).setHeaders(headers)\n     .setMetadata(metadata).setRequestConditions(requestConditions)\n     .setPermissions(\"permissions\").setUmask(\"umask\"))\n     .subscribe(response -> System.out.println(\"Uploaded file %n\"));\n```"
  returns:
    description: "A reactive response containing the information of the uploaded file."
    type: "<xref href=\"reactor.core.publisher.Mono?alt=reactor.core.publisher.Mono&text=Mono\" data-throw-if-not-resolved=\"False\" />&lt;<xref href=\"com.azure.core.http.rest.Response?alt=com.azure.core.http.rest.Response&text=Response\" data-throw-if-not-resolved=\"False\" />&lt;<xref href=\"com.azure.storage.file.datalake.models.PathInfo?alt=com.azure.storage.file.datalake.models.PathInfo&text=PathInfo\" data-throw-if-not-resolved=\"False\" />&gt;&gt;"
- uid: "com.azure.storage.file.datalake.DataLakeFileAsyncClient.uploadWithResponse(reactor.core.publisher.Flux<java.nio.ByteBuffer>,com.azure.storage.common.ParallelTransferOptions,com.azure.storage.file.datalake.models.PathHttpHeaders,java.util.Map<java.lang.String,java.lang.String>,com.azure.storage.file.datalake.models.DataLakeRequestConditions)"
  fullName: "com.azure.storage.file.datalake.DataLakeFileAsyncClient.uploadWithResponse(Flux<ByteBuffer> data, ParallelTransferOptions parallelTransferOptions, PathHttpHeaders headers, Map<String,String> metadata, DataLakeRequestConditions requestConditions)"
  name: "uploadWithResponse(Flux<ByteBuffer> data, ParallelTransferOptions parallelTransferOptions, PathHttpHeaders headers, Map<String,String> metadata, DataLakeRequestConditions requestConditions)"
  nameWithType: "DataLakeFileAsyncClient.uploadWithResponse(Flux<ByteBuffer> data, ParallelTransferOptions parallelTransferOptions, PathHttpHeaders headers, Map<String,String> metadata, DataLakeRequestConditions requestConditions)"
  summary: "Creates a new file."
  parameters:
  - description: "The data to write to the file. Unlike other upload methods, this method does not require that the\n <code>Flux</code> be replayable. In other words, it does not have to support multiple subscribers and is not expected\n to produce the same values across subscriptions."
    name: "data"
    type: "<xref href=\"reactor.core.publisher.Flux?alt=reactor.core.publisher.Flux&text=Flux\" data-throw-if-not-resolved=\"False\" />&lt;<xref href=\"java.nio.ByteBuffer?alt=java.nio.ByteBuffer&text=ByteBuffer\" data-throw-if-not-resolved=\"False\" />&gt;"
  - description: "<xref uid=\"com.azure.storage.common.ParallelTransferOptions\" data-throw-if-not-resolved=\"false\" data-raw-source=\"ParallelTransferOptions\"></xref> used to configure buffered uploading."
    name: "parallelTransferOptions"
    type: "<xref href=\"com.azure.storage.common.ParallelTransferOptions?alt=com.azure.storage.common.ParallelTransferOptions&text=ParallelTransferOptions\" data-throw-if-not-resolved=\"False\" />"
  - description: "<xref uid=\"com.azure.storage.file.datalake.models.PathHttpHeaders\" data-throw-if-not-resolved=\"false\" data-raw-source=\"PathHttpHeaders\"></xref>"
    name: "headers"
    type: "<xref href=\"com.azure.storage.file.datalake.models.PathHttpHeaders?alt=com.azure.storage.file.datalake.models.PathHttpHeaders&text=PathHttpHeaders\" data-throw-if-not-resolved=\"False\" />"
  - description: "Metadata to associate with the resource. If there is leading or trailing whitespace in any\n metadata key or value, it must be removed or encoded."
    name: "metadata"
    type: "<xref href=\"java.util.Map?alt=java.util.Map&text=Map\" data-throw-if-not-resolved=\"False\" />&lt;<xref href=\"java.lang.String?alt=java.lang.String&text=String\" data-throw-if-not-resolved=\"False\" />,<xref href=\"java.lang.String?alt=java.lang.String&text=String\" data-throw-if-not-resolved=\"False\" />&gt;"
  - description: "<xref uid=\"com.azure.storage.file.datalake.models.DataLakeRequestConditions\" data-throw-if-not-resolved=\"false\" data-raw-source=\"DataLakeRequestConditions\"></xref>"
    name: "requestConditions"
    type: "<xref href=\"com.azure.storage.file.datalake.models.DataLakeRequestConditions?alt=com.azure.storage.file.datalake.models.DataLakeRequestConditions&text=DataLakeRequestConditions\" data-throw-if-not-resolved=\"False\" />"
  syntax: "public Mono<Response<PathInfo>> uploadWithResponse(Flux<ByteBuffer> data, ParallelTransferOptions parallelTransferOptions, PathHttpHeaders headers, Map<String,String> metadata, DataLakeRequestConditions requestConditions)"
  desc: "Creates a new file. To avoid overwriting, pass \"\\*\" to <xref uid=\"com.azure.storage.file.datalake.models.DataLakeRequestConditions.setIfNoneMatch(java.lang.String)\" data-throw-if-not-resolved=\"false\" data-raw-source=\"DataLakeRequestConditions#setIfNoneMatch(String)\"></xref>.\n\n**Code Samples**\n\n```java\nPathHttpHeaders headers = new PathHttpHeaders()\n     .setContentMd5(\"data\".getBytes(StandardCharsets.UTF_8))\n     .setContentLanguage(\"en-US\")\n     .setContentType(\"binary\");\n\n Map<String, String> metadata = Collections.singletonMap(\"metadata\", \"value\");\n DataLakeRequestConditions requestConditions = new DataLakeRequestConditions()\n     .setLeaseId(leaseId)\n     .setIfUnmodifiedSince(OffsetDateTime.now().minusDays(3));\n Long blockSize = 100L * 1024L * 1024L; // 100 MB;\n ParallelTransferOptions parallelTransferOptions = new ParallelTransferOptions().setBlockSizeLong(blockSize);\n\n client.uploadWithResponse(data, parallelTransferOptions, headers, metadata, requestConditions)\n     .subscribe(response -> System.out.println(\"Uploaded file %n\"));\n```\n\n**Using Progress Reporting**\n\n```java\nPathHttpHeaders httpHeaders = new PathHttpHeaders()\n     .setContentMd5(\"data\".getBytes(StandardCharsets.UTF_8))\n     .setContentLanguage(\"en-US\")\n     .setContentType(\"binary\");\n\n Map<String, String> metadataMap = Collections.singletonMap(\"metadata\", \"value\");\n DataLakeRequestConditions conditions = new DataLakeRequestConditions()\n     .setLeaseId(leaseId)\n     .setIfUnmodifiedSince(OffsetDateTime.now().minusDays(3));\n ParallelTransferOptions pto = new ParallelTransferOptions()\n     .setBlockSizeLong(blockSize)\n     .setProgressReceiver(bytesTransferred -> System.out.printf(\"Upload progress: %s bytes sent\", bytesTransferred));\n\n client.uploadWithResponse(data, pto, httpHeaders, metadataMap, conditions)\n     .subscribe(response -> System.out.println(\"Uploaded file %n\"));\n```"
  returns:
    description: "A reactive response containing the information of the uploaded file."
    type: "<xref href=\"reactor.core.publisher.Mono?alt=reactor.core.publisher.Mono&text=Mono\" data-throw-if-not-resolved=\"False\" />&lt;<xref href=\"com.azure.core.http.rest.Response?alt=com.azure.core.http.rest.Response&text=Response\" data-throw-if-not-resolved=\"False\" />&lt;<xref href=\"com.azure.storage.file.datalake.models.PathInfo?alt=com.azure.storage.file.datalake.models.PathInfo&text=PathInfo\" data-throw-if-not-resolved=\"False\" />&gt;&gt;"
type: "class"
desc: "This class provides a client that contains file operations for Azure Storage Data Lake. Operations provided by this client include creating a file, deleting a file, renaming a file, setting metadata and http headers, setting and retrieving access control, getting properties, reading a file, and appending and flushing data to write to a file.\n\nThis client is instantiated through <xref uid=\"com.azure.storage.file.datalake.DataLakePathClientBuilder\" data-throw-if-not-resolved=\"false\" data-raw-source=\"DataLakePathClientBuilder\"></xref> or retrieved via <xref uid=\"com.azure.storage.file.datalake.DataLakeFileSystemAsyncClient.getFileAsyncClient(java.lang.String)\" data-throw-if-not-resolved=\"false\" data-raw-source=\"DataLakeFileSystemAsyncClient#getFileAsyncClient(String)\"></xref>.\n\nPlease refer to the [Azure Docs][] for more information.\n\n\n[Azure Docs]: https://docs.microsoft.com/azure/storage/blobs/data-lake-storage-introduction"
metadata: {}
package: "com.azure.storage.file.datalake"
artifact: com.azure:azure-storage-file-datalake:12.11.0-beta.1
