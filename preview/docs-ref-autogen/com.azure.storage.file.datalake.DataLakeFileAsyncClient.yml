### YamlMime:ManagedReference
items:
- uid: com.azure.storage.file.datalake.DataLakeFileAsyncClient
  id: DataLakeFileAsyncClient
  artifact: com.azure:azure-storage-file-datalake:12.3.0-beta.1
  parent: com.azure.storage.file.datalake
  children:
  - com.azure.storage.file.datalake.DataLakeFileAsyncClient.append(reactor.core.publisher.Flux<java.nio.ByteBuffer>,long,long)
  - com.azure.storage.file.datalake.DataLakeFileAsyncClient.appendWithResponse(reactor.core.publisher.Flux<java.nio.ByteBuffer>,long,long,byte[],java.lang.String)
  - com.azure.storage.file.datalake.DataLakeFileAsyncClient.delete()
  - com.azure.storage.file.datalake.DataLakeFileAsyncClient.deleteWithResponse(com.azure.storage.file.datalake.models.DataLakeRequestConditions)
  - com.azure.storage.file.datalake.DataLakeFileAsyncClient.flush(long)
  - com.azure.storage.file.datalake.DataLakeFileAsyncClient.flush(long,boolean)
  - com.azure.storage.file.datalake.DataLakeFileAsyncClient.flushWithResponse(long,boolean,boolean,com.azure.storage.file.datalake.models.PathHttpHeaders,com.azure.storage.file.datalake.models.DataLakeRequestConditions)
  - com.azure.storage.file.datalake.DataLakeFileAsyncClient.getFileName()
  - com.azure.storage.file.datalake.DataLakeFileAsyncClient.getFilePath()
  - com.azure.storage.file.datalake.DataLakeFileAsyncClient.getFileUrl()
  - com.azure.storage.file.datalake.DataLakeFileAsyncClient.query(java.lang.String)
  - com.azure.storage.file.datalake.DataLakeFileAsyncClient.queryWithResponse(com.azure.storage.file.datalake.options.FileQueryOptions)
  - com.azure.storage.file.datalake.DataLakeFileAsyncClient.read()
  - com.azure.storage.file.datalake.DataLakeFileAsyncClient.readToFile(java.lang.String)
  - com.azure.storage.file.datalake.DataLakeFileAsyncClient.readToFile(java.lang.String,boolean)
  - com.azure.storage.file.datalake.DataLakeFileAsyncClient.readToFileWithResponse(java.lang.String,com.azure.storage.file.datalake.models.FileRange,com.azure.storage.common.ParallelTransferOptions,com.azure.storage.file.datalake.models.DownloadRetryOptions,com.azure.storage.file.datalake.models.DataLakeRequestConditions,boolean,java.util.Set<java.nio.file.OpenOption>)
  - com.azure.storage.file.datalake.DataLakeFileAsyncClient.readWithResponse(com.azure.storage.file.datalake.models.FileRange,com.azure.storage.file.datalake.models.DownloadRetryOptions,com.azure.storage.file.datalake.models.DataLakeRequestConditions,boolean)
  - com.azure.storage.file.datalake.DataLakeFileAsyncClient.rename(java.lang.String,java.lang.String)
  - com.azure.storage.file.datalake.DataLakeFileAsyncClient.renameWithResponse(java.lang.String,java.lang.String,com.azure.storage.file.datalake.models.DataLakeRequestConditions,com.azure.storage.file.datalake.models.DataLakeRequestConditions)
  - com.azure.storage.file.datalake.DataLakeFileAsyncClient.scheduleDeletion(com.azure.storage.file.datalake.options.FileScheduleDeletionOptions)
  - com.azure.storage.file.datalake.DataLakeFileAsyncClient.scheduleDeletionWithResponse(com.azure.storage.file.datalake.options.FileScheduleDeletionOptions)
  - com.azure.storage.file.datalake.DataLakeFileAsyncClient.upload(reactor.core.publisher.Flux<java.nio.ByteBuffer>,com.azure.storage.common.ParallelTransferOptions)
  - com.azure.storage.file.datalake.DataLakeFileAsyncClient.upload(reactor.core.publisher.Flux<java.nio.ByteBuffer>,com.azure.storage.common.ParallelTransferOptions,boolean)
  - com.azure.storage.file.datalake.DataLakeFileAsyncClient.uploadFromFile(java.lang.String)
  - com.azure.storage.file.datalake.DataLakeFileAsyncClient.uploadFromFile(java.lang.String,boolean)
  - com.azure.storage.file.datalake.DataLakeFileAsyncClient.uploadFromFile(java.lang.String,com.azure.storage.common.ParallelTransferOptions,com.azure.storage.file.datalake.models.PathHttpHeaders,java.util.Map<java.lang.String,java.lang.String>,com.azure.storage.file.datalake.models.DataLakeRequestConditions)
  - com.azure.storage.file.datalake.DataLakeFileAsyncClient.uploadWithResponse(com.azure.storage.file.datalake.options.FileParallelUploadOptions)
  - com.azure.storage.file.datalake.DataLakeFileAsyncClient.uploadWithResponse(reactor.core.publisher.Flux<java.nio.ByteBuffer>,com.azure.storage.common.ParallelTransferOptions,com.azure.storage.file.datalake.models.PathHttpHeaders,java.util.Map<java.lang.String,java.lang.String>,com.azure.storage.file.datalake.models.DataLakeRequestConditions)
  langs:
  - java
  name: DataLakeFileAsyncClient
  nameWithType: DataLakeFileAsyncClient
  fullName: com.azure.storage.file.datalake.DataLakeFileAsyncClient
  type: Class
  package: com.azure.storage.file.datalake
  summary: >-
    This class provides a client that contains file operations for Azure Storage Data Lake. Operations provided by this client include creating a file, deleting a file, renaming a file, setting metadata and http headers, setting and retrieving access control, getting properties, reading a file, and appending and flushing data to write to a file.


    This client is instantiated through <xref uid="com.azure.storage.file.datalake.DataLakePathClientBuilder" data-throw-if-not-resolved="false">DataLakePathClientBuilder</xref> or retrieved via <xref uid="com.azure.storage.file.datalake.DataLakeFileSystemAsyncClient.getFileAsyncClient(java.lang.String)" data-throw-if-not-resolved="false">DataLakeFileSystemAsyncClient#getFileAsyncClient(String)</xref>.


    Please refer to the [Azure Docs][] for more information.



    [Azure Docs]: https://docs.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-introduction?toc=%2fazure%2fstorage%2fblobs%2ftoc.json
  syntax:
    content: public class DataLakeFileAsyncClient extends DataLakePathAsyncClient
  inheritance:
  - java.lang.Object
  - com.azure.storage.file.datalake.DataLakePathAsyncClient
  inheritedMembers:
  - com.azure.storage.file.datalake.DataLakePathAsyncClient.create()
  - com.azure.storage.file.datalake.DataLakePathAsyncClient.create(boolean)
  - com.azure.storage.file.datalake.DataLakePathAsyncClient.createWithResponse(java.lang.String,java.lang.String,com.azure.storage.file.datalake.models.PathHttpHeaders,java.util.Map<java.lang.String,java.lang.String>,com.azure.storage.file.datalake.models.DataLakeRequestConditions)
  - com.azure.storage.file.datalake.DataLakePathAsyncClient.exists()
  - com.azure.storage.file.datalake.DataLakePathAsyncClient.existsWithResponse()
  - com.azure.storage.file.datalake.DataLakePathAsyncClient.generateSas(com.azure.storage.file.datalake.sas.DataLakeServiceSasSignatureValues)
  - com.azure.storage.file.datalake.DataLakePathAsyncClient.generateUserDelegationSas(com.azure.storage.file.datalake.sas.DataLakeServiceSasSignatureValues,com.azure.storage.file.datalake.models.UserDelegationKey)
  - com.azure.storage.file.datalake.DataLakePathAsyncClient.getAccessControl()
  - com.azure.storage.file.datalake.DataLakePathAsyncClient.getAccessControlWithResponse(boolean,com.azure.storage.file.datalake.models.DataLakeRequestConditions)
  - com.azure.storage.file.datalake.DataLakePathAsyncClient.getAccountName()
  - com.azure.storage.file.datalake.DataLakePathAsyncClient.getFileSystemName()
  - com.azure.storage.file.datalake.DataLakePathAsyncClient.getHttpPipeline()
  - com.azure.storage.file.datalake.DataLakePathAsyncClient.getProperties()
  - com.azure.storage.file.datalake.DataLakePathAsyncClient.getPropertiesWithResponse(com.azure.storage.file.datalake.models.DataLakeRequestConditions)
  - com.azure.storage.file.datalake.DataLakePathAsyncClient.getServiceVersion()
  - com.azure.storage.file.datalake.DataLakePathAsyncClient.removeAccessControlRecursive(java.util.List<com.azure.storage.file.datalake.models.PathRemoveAccessControlEntry>)
  - com.azure.storage.file.datalake.DataLakePathAsyncClient.removeAccessControlRecursiveWithResponse(com.azure.storage.file.datalake.options.PathRemoveAccessControlRecursiveOptions)
  - com.azure.storage.file.datalake.DataLakePathAsyncClient.setAccessControlList(java.util.List<com.azure.storage.file.datalake.models.PathAccessControlEntry>,java.lang.String,java.lang.String)
  - com.azure.storage.file.datalake.DataLakePathAsyncClient.setAccessControlListWithResponse(java.util.List<com.azure.storage.file.datalake.models.PathAccessControlEntry>,java.lang.String,java.lang.String,com.azure.storage.file.datalake.models.DataLakeRequestConditions)
  - com.azure.storage.file.datalake.DataLakePathAsyncClient.setAccessControlRecursive(java.util.List<com.azure.storage.file.datalake.models.PathAccessControlEntry>)
  - com.azure.storage.file.datalake.DataLakePathAsyncClient.setAccessControlRecursiveWithResponse(com.azure.storage.file.datalake.options.PathSetAccessControlRecursiveOptions)
  - com.azure.storage.file.datalake.DataLakePathAsyncClient.setHttpHeaders(com.azure.storage.file.datalake.models.PathHttpHeaders)
  - com.azure.storage.file.datalake.DataLakePathAsyncClient.setHttpHeadersWithResponse(com.azure.storage.file.datalake.models.PathHttpHeaders,com.azure.storage.file.datalake.models.DataLakeRequestConditions)
  - com.azure.storage.file.datalake.DataLakePathAsyncClient.setMetadata(java.util.Map<java.lang.String,java.lang.String>)
  - com.azure.storage.file.datalake.DataLakePathAsyncClient.setMetadataWithResponse(java.util.Map<java.lang.String,java.lang.String>,com.azure.storage.file.datalake.models.DataLakeRequestConditions)
  - com.azure.storage.file.datalake.DataLakePathAsyncClient.setPermissions(com.azure.storage.file.datalake.models.PathPermissions,java.lang.String,java.lang.String)
  - com.azure.storage.file.datalake.DataLakePathAsyncClient.setPermissionsWithResponse(com.azure.storage.file.datalake.models.PathPermissions,java.lang.String,java.lang.String,com.azure.storage.file.datalake.models.DataLakeRequestConditions)
  - com.azure.storage.file.datalake.DataLakePathAsyncClient.updateAccessControlRecursive(java.util.List<com.azure.storage.file.datalake.models.PathAccessControlEntry>)
  - com.azure.storage.file.datalake.DataLakePathAsyncClient.updateAccessControlRecursiveWithResponse(com.azure.storage.file.datalake.options.PathUpdateAccessControlRecursiveOptions)
  - java.lang.Object.clone()
  - java.lang.Object.equals(java.lang.Object)
  - java.lang.Object.finalize()
  - java.lang.Object.getClass()
  - java.lang.Object.hashCode()
  - java.lang.Object.notify()
  - java.lang.Object.notifyAll()
  - java.lang.Object.toString()
  - java.lang.Object.wait()
  - java.lang.Object.wait(long)
  - java.lang.Object.wait(long,int)
- uid: com.azure.storage.file.datalake.DataLakeFileAsyncClient.DataLakeFileAsyncClient(com.azure.core.http.HttpPipeline,java.lang.String,com.azure.storage.file.datalake.DataLakeServiceVersion,java.lang.String,java.lang.String,java.lang.String,com.azure.storage.blob.specialized.BlockBlobAsyncClient)
  id: DataLakeFileAsyncClient(com.azure.core.http.HttpPipeline,java.lang.String,com.azure.storage.file.datalake.DataLakeServiceVersion,java.lang.String,java.lang.String,java.lang.String,com.azure.storage.blob.specialized.BlockBlobAsyncClient)
  artifact: com.azure:azure-storage-file-datalake:12.3.0-beta.1
  parent: com.azure.storage.file.datalake.DataLakeFileAsyncClient
  langs:
  - java
  name: DataLakeFileAsyncClient(HttpPipeline pipeline, String url, DataLakeServiceVersion serviceVersion, String accountName, String fileSystemName, String fileName, BlockBlobAsyncClient blockBlobAsyncClient)
  nameWithType: DataLakeFileAsyncClient.DataLakeFileAsyncClient(HttpPipeline pipeline, String url, DataLakeServiceVersion serviceVersion, String accountName, String fileSystemName, String fileName, BlockBlobAsyncClient blockBlobAsyncClient)
  fullName: com.azure.storage.file.datalake.DataLakeFileAsyncClient.DataLakeFileAsyncClient(HttpPipeline pipeline, String url, DataLakeServiceVersion serviceVersion, String accountName, String fileSystemName, String fileName, BlockBlobAsyncClient blockBlobAsyncClient)
  overload: com.azure.storage.file.datalake.DataLakeFileAsyncClient.DataLakeFileAsyncClient*
  type: Constructor
  package: com.azure.storage.file.datalake
  summary: Package-private constructor for use by <xref uid="com.azure.storage.file.datalake.DataLakePathClientBuilder" data-throw-if-not-resolved="false">DataLakePathClientBuilder</xref>.
  syntax:
    content: " DataLakeFileAsyncClient(HttpPipeline pipeline, String url, DataLakeServiceVersion serviceVersion, String accountName, String fileSystemName, String fileName, BlockBlobAsyncClient blockBlobAsyncClient)"
    parameters:
    - id: pipeline
      type: com.azure.core.http.HttpPipeline
      description: The pipeline used to send and receive service requests.
    - id: url
      type: java.lang.String
      description: The endpoint where to send service requests.
    - id: serviceVersion
      type: com.azure.storage.file.datalake.DataLakeServiceVersion
      description: The version of the service to receive requests.
    - id: accountName
      type: java.lang.String
      description: The storage account name.
    - id: fileSystemName
      type: java.lang.String
      description: The file system name.
    - id: fileName
      type: java.lang.String
      description: The file name.
    - id: blockBlobAsyncClient
      type: com.azure.storage.blob.specialized.BlockBlobAsyncClient
      description: The underlying <xref uid="" data-throw-if-not-resolved="false">BlobContainerAsyncClient</xref>
- uid: com.azure.storage.file.datalake.DataLakeFileAsyncClient.DataLakeFileAsyncClient(com.azure.storage.file.datalake.DataLakePathAsyncClient)
  id: DataLakeFileAsyncClient(com.azure.storage.file.datalake.DataLakePathAsyncClient)
  artifact: com.azure:azure-storage-file-datalake:12.3.0-beta.1
  parent: com.azure.storage.file.datalake.DataLakeFileAsyncClient
  langs:
  - java
  name: DataLakeFileAsyncClient(DataLakePathAsyncClient pathAsyncClient)
  nameWithType: DataLakeFileAsyncClient.DataLakeFileAsyncClient(DataLakePathAsyncClient pathAsyncClient)
  fullName: com.azure.storage.file.datalake.DataLakeFileAsyncClient.DataLakeFileAsyncClient(DataLakePathAsyncClient pathAsyncClient)
  overload: com.azure.storage.file.datalake.DataLakeFileAsyncClient.DataLakeFileAsyncClient*
  type: Constructor
  package: com.azure.storage.file.datalake
  syntax:
    content: " DataLakeFileAsyncClient(DataLakePathAsyncClient pathAsyncClient)"
    parameters:
    - id: pathAsyncClient
      type: com.azure.storage.file.datalake.DataLakePathAsyncClient
- uid: com.azure.storage.file.datalake.DataLakeFileAsyncClient.append(reactor.core.publisher.Flux<java.nio.ByteBuffer>,long,long)
  id: append(reactor.core.publisher.Flux<java.nio.ByteBuffer>,long,long)
  artifact: com.azure:azure-storage-file-datalake:12.3.0-beta.1
  parent: com.azure.storage.file.datalake.DataLakeFileAsyncClient
  langs:
  - java
  name: append(Flux<ByteBuffer> data, long fileOffset, long length)
  nameWithType: DataLakeFileAsyncClient.append(Flux<ByteBuffer> data, long fileOffset, long length)
  fullName: com.azure.storage.file.datalake.DataLakeFileAsyncClient.append(Flux<ByteBuffer> data, long fileOffset, long length)
  overload: com.azure.storage.file.datalake.DataLakeFileAsyncClient.append*
  type: Method
  package: com.azure.storage.file.datalake
  summary: >-
    Appends data to the specified resource to later be flushed (written) by a call to flush


    **Code Samples**


    ```java

    client.append(data, offset, length)
         .subscribe(
             response -> System.out.println("Append data completed"),
             error -> System.out.printf("Error when calling append data: %s", error));
    ```


    For more information, see the [Azure Docs][]



    [Azure Docs]: https://docs.microsoft.com/en-us/rest/api/storageservices/datalakestoragegen2/path/update
  syntax:
    content: public Mono<Void> append(Flux<ByteBuffer> data, long fileOffset, long length)
    parameters:
    - id: data
      type: reactor.core.publisher.Flux<java.nio.ByteBuffer>
      description: The data to write to the file.
    - id: fileOffset
      type: long
      description: The position where the data is to be appended.
    - id: length
      type: long
      description: >-
        The exact length of the data. It is important that this value match precisely the length of the
         data emitted by the <code>Flux</code>.
    return:
      type: reactor.core.publisher.Mono<java.lang.Void>
      description: A reactive response signalling completion.
- uid: com.azure.storage.file.datalake.DataLakeFileAsyncClient.appendWithResponse(reactor.core.publisher.Flux<java.nio.ByteBuffer>,long,long,byte[],java.lang.String)
  id: appendWithResponse(reactor.core.publisher.Flux<java.nio.ByteBuffer>,long,long,byte[],java.lang.String)
  artifact: com.azure:azure-storage-file-datalake:12.3.0-beta.1
  parent: com.azure.storage.file.datalake.DataLakeFileAsyncClient
  langs:
  - java
  name: appendWithResponse(Flux<ByteBuffer> data, long fileOffset, long length, byte[] contentMd5, String leaseId)
  nameWithType: DataLakeFileAsyncClient.appendWithResponse(Flux<ByteBuffer> data, long fileOffset, long length, byte[] contentMd5, String leaseId)
  fullName: com.azure.storage.file.datalake.DataLakeFileAsyncClient.appendWithResponse(Flux<ByteBuffer> data, long fileOffset, long length, byte[] contentMd5, String leaseId)
  overload: com.azure.storage.file.datalake.DataLakeFileAsyncClient.appendWithResponse*
  type: Method
  package: com.azure.storage.file.datalake
  summary: "Appends data to the specified resource to later be flushed (written) by a call to flush\n\n**Code Samples**\n\n```java\nFileRange range = new FileRange(1024, 2048L);\n DownloadRetryOptions options = new DownloadRetryOptions().setMaxRetryRequests(5);\n byte[] contentMd5 = new byte[0]; // Replace with valid md5\n \n client.appendWithResponse(data, offset, length, contentMd5, leaseId).subscribe(response ->\n     System.out.printf(\"Append data completed with status %d%n\", response.getStatusCode()));\n```\n\nFor more information, see the [Azure Docs][]\n\n\n[Azure Docs]: https://docs.microsoft.com/en-us/rest/api/storageservices/datalakestoragegen2/path/update"
  syntax:
    content: public Mono<Response<Void>> appendWithResponse(Flux<ByteBuffer> data, long fileOffset, long length, byte[] contentMd5, String leaseId)
    parameters:
    - id: data
      type: reactor.core.publisher.Flux<java.nio.ByteBuffer>
      description: The data to write to the file.
    - id: fileOffset
      type: long
      description: The position where the data is to be appended.
    - id: length
      type: long
      description: >-
        The exact length of the data. It is important that this value match precisely the length of the
         data emitted by the <code>Flux</code>.
    - id: contentMd5
      type: byte[]
      description: >-
        An MD5 hash of the content of the data. If specified, the service will calculate the MD5 of the
         received data and fail the request if it does not match the provided MD5.
    - id: leaseId
      type: java.lang.String
      description: >-
        By setting lease id, requests will fail if the provided lease does not match the active lease on
         the file.
    return:
      type: reactor.core.publisher.Mono<com.azure.core.http.rest.Response<java.lang.Void>>
      description: A reactive response signalling completion.
- uid: com.azure.storage.file.datalake.DataLakeFileAsyncClient.delete()
  id: delete()
  artifact: com.azure:azure-storage-file-datalake:12.3.0-beta.1
  parent: com.azure.storage.file.datalake.DataLakeFileAsyncClient
  langs:
  - java
  name: delete()
  nameWithType: DataLakeFileAsyncClient.delete()
  fullName: com.azure.storage.file.datalake.DataLakeFileAsyncClient.delete()
  overload: com.azure.storage.file.datalake.DataLakeFileAsyncClient.delete*
  type: Method
  package: com.azure.storage.file.datalake
  summary: >-
    Deletes a file.


    **Code Samples**


    ```java

    client.delete().subscribe(response ->
         System.out.println("Delete request completed"));
    ```


    For more information see the [Azure Docs][]



    [Azure Docs]: https://docs.microsoft.com/en-us/rest/api/storageservices/datalakestoragegen2/path/delete
  syntax:
    content: public Mono<Void> delete()
    return:
      type: reactor.core.publisher.Mono<java.lang.Void>
      description: A reactive response signalling completion.
- uid: com.azure.storage.file.datalake.DataLakeFileAsyncClient.deleteWithResponse(com.azure.storage.file.datalake.models.DataLakeRequestConditions)
  id: deleteWithResponse(com.azure.storage.file.datalake.models.DataLakeRequestConditions)
  artifact: com.azure:azure-storage-file-datalake:12.3.0-beta.1
  parent: com.azure.storage.file.datalake.DataLakeFileAsyncClient
  langs:
  - java
  name: deleteWithResponse(DataLakeRequestConditions requestConditions)
  nameWithType: DataLakeFileAsyncClient.deleteWithResponse(DataLakeRequestConditions requestConditions)
  fullName: com.azure.storage.file.datalake.DataLakeFileAsyncClient.deleteWithResponse(DataLakeRequestConditions requestConditions)
  overload: com.azure.storage.file.datalake.DataLakeFileAsyncClient.deleteWithResponse*
  type: Method
  package: com.azure.storage.file.datalake
  summary: "Deletes a file.\n\n**Code Samples**\n\n```java\nDataLakeRequestConditions requestConditions = new DataLakeRequestConditions()\n     .setLeaseId(leaseId);\n \n client.deleteWithResponse(requestConditions)\n     .subscribe(response -> System.out.println(\"Delete request completed\"));\n```\n\nFor more information see the [Azure Docs][]\n\n\n[Azure Docs]: https://docs.microsoft.com/en-us/rest/api/storageservices/datalakestoragegen2/path/delete"
  syntax:
    content: public Mono<Response<Void>> deleteWithResponse(DataLakeRequestConditions requestConditions)
    parameters:
    - id: requestConditions
      type: com.azure.storage.file.datalake.models.DataLakeRequestConditions
      description: <xref uid="com.azure.storage.file.datalake.models.DataLakeRequestConditions" data-throw-if-not-resolved="false">DataLakeRequestConditions</xref>
    return:
      type: reactor.core.publisher.Mono<com.azure.core.http.rest.Response<java.lang.Void>>
      description: A reactive response signalling completion.
- uid: com.azure.storage.file.datalake.DataLakeFileAsyncClient.flush(long)
  id: flush(long)
  artifact: com.azure:azure-storage-file-datalake:12.3.0-beta.1
  parent: com.azure.storage.file.datalake.DataLakeFileAsyncClient
  langs:
  - java
  name: flush(long position)
  nameWithType: DataLakeFileAsyncClient.flush(long position)
  fullName: com.azure.storage.file.datalake.DataLakeFileAsyncClient.flush(long position)
  overload: com.azure.storage.file.datalake.DataLakeFileAsyncClient.flush*
  type: Method
  package: com.azure.storage.file.datalake
  summary: >-
    Flushes (writes) data previously appended to the file through a call to append. The previously uploaded data must be contiguous.


    By default this method will not overwrite existing data.


    **Code Samples**


    ```java

    client.flush(position).subscribe(response ->
         System.out.println("Flush data completed"));
    ```


    For more information, see the [Azure Docs][]



    [Azure Docs]: https://docs.microsoft.com/en-us/rest/api/storageservices/datalakestoragegen2/path/update
  syntax:
    content: public Mono<PathInfo> flush(long position)
    parameters:
    - id: position
      type: long
      description: The length of the file after all data has been written.
    return:
      type: reactor.core.publisher.Mono<com.azure.storage.file.datalake.models.PathInfo>
      description: A reactive response containing the information of the created resource.
- uid: com.azure.storage.file.datalake.DataLakeFileAsyncClient.flush(long,boolean)
  id: flush(long,boolean)
  artifact: com.azure:azure-storage-file-datalake:12.3.0-beta.1
  parent: com.azure.storage.file.datalake.DataLakeFileAsyncClient
  langs:
  - java
  name: flush(long position, boolean overwrite)
  nameWithType: DataLakeFileAsyncClient.flush(long position, boolean overwrite)
  fullName: com.azure.storage.file.datalake.DataLakeFileAsyncClient.flush(long position, boolean overwrite)
  overload: com.azure.storage.file.datalake.DataLakeFileAsyncClient.flush*
  type: Method
  package: com.azure.storage.file.datalake
  summary: >-
    Flushes (writes) data previously appended to the file through a call to append. The previously uploaded data must be contiguous.


    **Code Samples**


    ```java

    boolean overwrite = true;
     client.flush(position, overwrite).subscribe(response ->
         System.out.println("Flush data completed"));
    ```


    For more information, see the [Azure Docs][]



    [Azure Docs]: https://docs.microsoft.com/en-us/rest/api/storageservices/datalakestoragegen2/path/update
  syntax:
    content: public Mono<PathInfo> flush(long position, boolean overwrite)
    parameters:
    - id: position
      type: long
      description: The length of the file after all data has been written.
    - id: overwrite
      type: boolean
      description: Whether or not to overwrite, should data exist on the file.
    return:
      type: reactor.core.publisher.Mono<com.azure.storage.file.datalake.models.PathInfo>
      description: A reactive response containing the information of the created resource.
- uid: com.azure.storage.file.datalake.DataLakeFileAsyncClient.flushWithResponse(long,boolean,boolean,com.azure.storage.file.datalake.models.PathHttpHeaders,com.azure.storage.file.datalake.models.DataLakeRequestConditions)
  id: flushWithResponse(long,boolean,boolean,com.azure.storage.file.datalake.models.PathHttpHeaders,com.azure.storage.file.datalake.models.DataLakeRequestConditions)
  artifact: com.azure:azure-storage-file-datalake:12.3.0-beta.1
  parent: com.azure.storage.file.datalake.DataLakeFileAsyncClient
  langs:
  - java
  name: flushWithResponse(long position, boolean retainUncommittedData, boolean close, PathHttpHeaders httpHeaders, DataLakeRequestConditions requestConditions)
  nameWithType: DataLakeFileAsyncClient.flushWithResponse(long position, boolean retainUncommittedData, boolean close, PathHttpHeaders httpHeaders, DataLakeRequestConditions requestConditions)
  fullName: com.azure.storage.file.datalake.DataLakeFileAsyncClient.flushWithResponse(long position, boolean retainUncommittedData, boolean close, PathHttpHeaders httpHeaders, DataLakeRequestConditions requestConditions)
  overload: com.azure.storage.file.datalake.DataLakeFileAsyncClient.flushWithResponse*
  type: Method
  package: com.azure.storage.file.datalake
  summary: "Flushes (writes) data previously appended to the file through a call to append. The previously uploaded data must be contiguous.\n\n**Code Samples**\n\n```java\nFileRange range = new FileRange(1024, 2048L);\n DownloadRetryOptions options = new DownloadRetryOptions().setMaxRetryRequests(5);\n byte[] contentMd5 = new byte[0]; // Replace with valid md5\n boolean retainUncommittedData = false;\n boolean close = false;\n PathHttpHeaders httpHeaders = new PathHttpHeaders()\n     .setContentLanguage(\"en-US\")\n     .setContentType(\"binary\");\n DataLakeRequestConditions requestConditions = new DataLakeRequestConditions()\n     .setLeaseId(leaseId);\n \n client.flushWithResponse(position, retainUncommittedData, close, httpHeaders,\n     requestConditions).subscribe(response ->\n     System.out.printf(\"Flush data completed with status %d%n\", response.getStatusCode()));\n```\n\nFor more information, see the [Azure Docs][]\n\n\n[Azure Docs]: https://docs.microsoft.com/en-us/rest/api/storageservices/datalakestoragegen2/path/update"
  syntax:
    content: public Mono<Response<PathInfo>> flushWithResponse(long position, boolean retainUncommittedData, boolean close, PathHttpHeaders httpHeaders, DataLakeRequestConditions requestConditions)
    parameters:
    - id: position
      type: long
      description: The length of the file after all data has been written.
    - id: retainUncommittedData
      type: boolean
      description: Whether or not uncommitted data is to be retained after the operation.
    - id: close
      type: boolean
      description: Whether or not a file changed event raised indicates completion (true) or modification (false).
    - id: httpHeaders
      type: com.azure.storage.file.datalake.models.PathHttpHeaders
      description: <xref uid="com.azure.storage.file.datalake.models.PathHttpHeaders" data-throw-if-not-resolved="false">httpHeaders</xref>
    - id: requestConditions
      type: com.azure.storage.file.datalake.models.DataLakeRequestConditions
      description: <xref uid="com.azure.storage.file.datalake.models.DataLakeRequestConditions" data-throw-if-not-resolved="false">requestConditions</xref>
    return:
      type: reactor.core.publisher.Mono<com.azure.core.http.rest.Response<com.azure.storage.file.datalake.models.PathInfo>>
      description: A reactive response containing the information of the created resource.
- uid: com.azure.storage.file.datalake.DataLakeFileAsyncClient.getFileName()
  id: getFileName()
  artifact: com.azure:azure-storage-file-datalake:12.3.0-beta.1
  parent: com.azure.storage.file.datalake.DataLakeFileAsyncClient
  langs:
  - java
  name: getFileName()
  nameWithType: DataLakeFileAsyncClient.getFileName()
  fullName: com.azure.storage.file.datalake.DataLakeFileAsyncClient.getFileName()
  overload: com.azure.storage.file.datalake.DataLakeFileAsyncClient.getFileName*
  type: Method
  package: com.azure.storage.file.datalake
  summary: Gets the name of this file, not including its full path.
  syntax:
    content: public String getFileName()
    return:
      type: java.lang.String
      description: The name of the file.
- uid: com.azure.storage.file.datalake.DataLakeFileAsyncClient.getFilePath()
  id: getFilePath()
  artifact: com.azure:azure-storage-file-datalake:12.3.0-beta.1
  parent: com.azure.storage.file.datalake.DataLakeFileAsyncClient
  langs:
  - java
  name: getFilePath()
  nameWithType: DataLakeFileAsyncClient.getFilePath()
  fullName: com.azure.storage.file.datalake.DataLakeFileAsyncClient.getFilePath()
  overload: com.azure.storage.file.datalake.DataLakeFileAsyncClient.getFilePath*
  type: Method
  package: com.azure.storage.file.datalake
  summary: Gets the path of this file, not including the name of the resource itself.
  syntax:
    content: public String getFilePath()
    return:
      type: java.lang.String
      description: The path of the file.
- uid: com.azure.storage.file.datalake.DataLakeFileAsyncClient.getFileUrl()
  id: getFileUrl()
  artifact: com.azure:azure-storage-file-datalake:12.3.0-beta.1
  parent: com.azure.storage.file.datalake.DataLakeFileAsyncClient
  langs:
  - java
  name: getFileUrl()
  nameWithType: DataLakeFileAsyncClient.getFileUrl()
  fullName: com.azure.storage.file.datalake.DataLakeFileAsyncClient.getFileUrl()
  overload: com.azure.storage.file.datalake.DataLakeFileAsyncClient.getFileUrl*
  type: Method
  package: com.azure.storage.file.datalake
  summary: Gets the URL of the file represented by this client on the Data Lake service.
  syntax:
    content: public String getFileUrl()
    return:
      type: java.lang.String
      description: the URL.
- uid: com.azure.storage.file.datalake.DataLakeFileAsyncClient.query(java.lang.String)
  id: query(java.lang.String)
  artifact: com.azure:azure-storage-file-datalake:12.3.0-beta.1
  parent: com.azure.storage.file.datalake.DataLakeFileAsyncClient
  langs:
  - java
  name: query(String expression)
  nameWithType: DataLakeFileAsyncClient.query(String expression)
  fullName: com.azure.storage.file.datalake.DataLakeFileAsyncClient.query(String expression)
  overload: com.azure.storage.file.datalake.DataLakeFileAsyncClient.query*
  type: Method
  package: com.azure.storage.file.datalake
  summary: >-
    Queries the entire file.


    For more information, see the [Azure Docs][]


    **Code Samples**


    ```java

    ByteArrayOutputStream queryData = new ByteArrayOutputStream();
     String expression = "SELECT * from BlobStorage";
     client.query(expression).subscribe(piece -> {
         try {
             queryData.write(piece.array());
         } catch (IOException ex) {
             throw new UncheckedIOException(ex);
         }
     });
    ```



    [Azure Docs]: https://docs.microsoft.com/en-us/rest/api/storageservices/query-blob-contents
  syntax:
    content: public Flux<ByteBuffer> query(String expression)
    parameters:
    - id: expression
      type: java.lang.String
      description: The query expression.
    return:
      type: reactor.core.publisher.Flux<java.nio.ByteBuffer>
      description: A reactive response containing the queried data.
- uid: com.azure.storage.file.datalake.DataLakeFileAsyncClient.queryWithResponse(com.azure.storage.file.datalake.options.FileQueryOptions)
  id: queryWithResponse(com.azure.storage.file.datalake.options.FileQueryOptions)
  artifact: com.azure:azure-storage-file-datalake:12.3.0-beta.1
  parent: com.azure.storage.file.datalake.DataLakeFileAsyncClient
  langs:
  - java
  name: queryWithResponse(FileQueryOptions queryOptions)
  nameWithType: DataLakeFileAsyncClient.queryWithResponse(FileQueryOptions queryOptions)
  fullName: com.azure.storage.file.datalake.DataLakeFileAsyncClient.queryWithResponse(FileQueryOptions queryOptions)
  overload: com.azure.storage.file.datalake.DataLakeFileAsyncClient.queryWithResponse*
  type: Method
  package: com.azure.storage.file.datalake
  summary: "Queries the entire file.\n\nFor more information, see the [Azure Docs][]\n\n**Code Samples**\n\n```java\nString expression = \"SELECT * from BlobStorage\";\n FileQueryJsonSerialization input = new FileQueryJsonSerialization()\n     .setRecordSeparator('\\n');\n FileQueryDelimitedSerialization output = new FileQueryDelimitedSerialization()\n     .setEscapeChar('\\0')\n     .setColumnSeparator(',')\n     .setRecordSeparator('\\n')\n     .setFieldQuote('\\'')\n     .setHeadersPresent(true);\n DataLakeRequestConditions requestConditions = new DataLakeRequestConditions().setLeaseId(leaseId);\n Consumer<FileQueryError> errorConsumer = System.out::println;\n Consumer<FileQueryProgress> progressConsumer = progress -> System.out.println(\"total file bytes read: \"\n     + progress.getBytesScanned());\n FileQueryOptions queryOptions = new FileQueryOptions(expression)\n     .setInputSerialization(input)\n     .setOutputSerialization(output)\n     .setRequestConditions(requestConditions)\n     .setErrorConsumer(errorConsumer)\n     .setProgressConsumer(progressConsumer);\n \n client.queryWithResponse(queryOptions)\n     .subscribe(response -> {\n         ByteArrayOutputStream queryData = new ByteArrayOutputStream();\n         response.getValue().subscribe(piece -> {\n             try {\n                 queryData.write(piece.array());\n             } catch (IOException ex) {\n                 throw new UncheckedIOException(ex);\n             }\n         });\n     });\n```\n\n\n[Azure Docs]: https://docs.microsoft.com/en-us/rest/api/storageservices/query-blob-contents"
  syntax:
    content: public Mono<FileQueryAsyncResponse> queryWithResponse(FileQueryOptions queryOptions)
    parameters:
    - id: queryOptions
      type: com.azure.storage.file.datalake.options.FileQueryOptions
      description: <xref uid="com.azure.storage.file.datalake.options.FileQueryOptions" data-throw-if-not-resolved="false">The query options</xref>
    return:
      type: reactor.core.publisher.Mono<com.azure.storage.file.datalake.models.FileQueryAsyncResponse>
      description: A reactive response containing the queried data.
- uid: com.azure.storage.file.datalake.DataLakeFileAsyncClient.read()
  id: read()
  artifact: com.azure:azure-storage-file-datalake:12.3.0-beta.1
  parent: com.azure.storage.file.datalake.DataLakeFileAsyncClient
  langs:
  - java
  name: read()
  nameWithType: DataLakeFileAsyncClient.read()
  fullName: com.azure.storage.file.datalake.DataLakeFileAsyncClient.read()
  overload: com.azure.storage.file.datalake.DataLakeFileAsyncClient.read*
  type: Method
  package: com.azure.storage.file.datalake
  summary: >-
    Reads the entire file.


    **Code Samples**


    ```java

    ByteArrayOutputStream downloadData = new ByteArrayOutputStream();
     client.read().subscribe(piece -> {
         try {
             downloadData.write(piece.array());
         } catch (IOException ex) {
             throw new UncheckedIOException(ex);
         }
     });
    ```


    For more information, see the [Azure Docs][]



    [Azure Docs]: https://docs.microsoft.com/en-us/rest/api/storageservices/get-blob
  syntax:
    content: public Flux<ByteBuffer> read()
    return:
      type: reactor.core.publisher.Flux<java.nio.ByteBuffer>
      description: A reactive response containing the file data.
- uid: com.azure.storage.file.datalake.DataLakeFileAsyncClient.readToFile(java.lang.String)
  id: readToFile(java.lang.String)
  artifact: com.azure:azure-storage-file-datalake:12.3.0-beta.1
  parent: com.azure.storage.file.datalake.DataLakeFileAsyncClient
  langs:
  - java
  name: readToFile(String filePath)
  nameWithType: DataLakeFileAsyncClient.readToFile(String filePath)
  fullName: com.azure.storage.file.datalake.DataLakeFileAsyncClient.readToFile(String filePath)
  overload: com.azure.storage.file.datalake.DataLakeFileAsyncClient.readToFile*
  type: Method
  package: com.azure.storage.file.datalake
  summary: >-
    Reads the entire file into a file specified by the path.


    The file will be created and must not exist, if the file already exists a <xref uid="" data-throw-if-not-resolved="false">FileAlreadyExistsException</xref> will be thrown.


    **Code Samples**


    ```java

    client.readToFile(file).subscribe(response -> System.out.println("Completed download to file"));

    ```


    For more information, see the [Azure Docs][]



    [Azure Docs]: https://docs.microsoft.com/en-us/rest/api/storageservices/get-blob
  syntax:
    content: public Mono<PathProperties> readToFile(String filePath)
    parameters:
    - id: filePath
      type: java.lang.String
      description: A <xref uid="java.lang.String" data-throw-if-not-resolved="false">String</xref> representing the filePath where the downloaded data will be written.
    return:
      type: reactor.core.publisher.Mono<com.azure.storage.file.datalake.models.PathProperties>
      description: A reactive response containing the file properties and metadata.
- uid: com.azure.storage.file.datalake.DataLakeFileAsyncClient.readToFile(java.lang.String,boolean)
  id: readToFile(java.lang.String,boolean)
  artifact: com.azure:azure-storage-file-datalake:12.3.0-beta.1
  parent: com.azure.storage.file.datalake.DataLakeFileAsyncClient
  langs:
  - java
  name: readToFile(String filePath, boolean overwrite)
  nameWithType: DataLakeFileAsyncClient.readToFile(String filePath, boolean overwrite)
  fullName: com.azure.storage.file.datalake.DataLakeFileAsyncClient.readToFile(String filePath, boolean overwrite)
  overload: com.azure.storage.file.datalake.DataLakeFileAsyncClient.readToFile*
  type: Method
  package: com.azure.storage.file.datalake
  summary: >-
    Reads the entire file into a file specified by the path.


    If overwrite is set to false, the file will be created and must not exist, if the file already exists a <xref uid="" data-throw-if-not-resolved="false">FileAlreadyExistsException</xref> will be thrown.


    **Code Samples**


    ```java

    boolean overwrite = false; // Default value
     client.readToFile(file, overwrite).subscribe(response -> System.out.println("Completed download to file"));
    ```


    For more information, see the [Azure Docs][]



    [Azure Docs]: https://docs.microsoft.com/en-us/rest/api/storageservices/get-blob
  syntax:
    content: public Mono<PathProperties> readToFile(String filePath, boolean overwrite)
    parameters:
    - id: filePath
      type: java.lang.String
      description: A <xref uid="java.lang.String" data-throw-if-not-resolved="false">String</xref> representing the filePath where the downloaded data will be written.
    - id: overwrite
      type: boolean
      description: Whether or not to overwrite the file, should the file exist.
    return:
      type: reactor.core.publisher.Mono<com.azure.storage.file.datalake.models.PathProperties>
      description: A reactive response containing the file properties and metadata.
- uid: com.azure.storage.file.datalake.DataLakeFileAsyncClient.readToFileWithResponse(java.lang.String,com.azure.storage.file.datalake.models.FileRange,com.azure.storage.common.ParallelTransferOptions,com.azure.storage.file.datalake.models.DownloadRetryOptions,com.azure.storage.file.datalake.models.DataLakeRequestConditions,boolean,java.util.Set<java.nio.file.OpenOption>)
  id: readToFileWithResponse(java.lang.String,com.azure.storage.file.datalake.models.FileRange,com.azure.storage.common.ParallelTransferOptions,com.azure.storage.file.datalake.models.DownloadRetryOptions,com.azure.storage.file.datalake.models.DataLakeRequestConditions,boolean,java.util.Set<java.nio.file.OpenOption>)
  artifact: com.azure:azure-storage-file-datalake:12.3.0-beta.1
  parent: com.azure.storage.file.datalake.DataLakeFileAsyncClient
  langs:
  - java
  name: readToFileWithResponse(String filePath, FileRange range, ParallelTransferOptions parallelTransferOptions, DownloadRetryOptions options, DataLakeRequestConditions requestConditions, boolean rangeGetContentMd5, Set<OpenOption> openOptions)
  nameWithType: DataLakeFileAsyncClient.readToFileWithResponse(String filePath, FileRange range, ParallelTransferOptions parallelTransferOptions, DownloadRetryOptions options, DataLakeRequestConditions requestConditions, boolean rangeGetContentMd5, Set<OpenOption> openOptions)
  fullName: com.azure.storage.file.datalake.DataLakeFileAsyncClient.readToFileWithResponse(String filePath, FileRange range, ParallelTransferOptions parallelTransferOptions, DownloadRetryOptions options, DataLakeRequestConditions requestConditions, boolean rangeGetContentMd5, Set<OpenOption> openOptions)
  overload: com.azure.storage.file.datalake.DataLakeFileAsyncClient.readToFileWithResponse*
  type: Method
  package: com.azure.storage.file.datalake
  summary: "Reads the entire file into a file specified by the path.\n\nBy default the file will be created and must not exist, if the file already exists a <xref uid=\"\" data-throw-if-not-resolved=\"false\">FileAlreadyExistsException</xref> will be thrown. To override this behavior, provide appropriate <xref uid=\"java.nio.file.OpenOption\" data-throw-if-not-resolved=\"false\">OpenOptions</xref>\n\n**Code Samples**\n\n```java\nFileRange fileRange = new FileRange(1024, 2048L);\n DownloadRetryOptions downloadRetryOptions = new DownloadRetryOptions().setMaxRetryRequests(5);\n Set<OpenOption> openOptions = new HashSet<>(Arrays.asList(StandardOpenOption.CREATE_NEW,\n     StandardOpenOption.WRITE, StandardOpenOption.READ)); // Default options\n \n client.readToFileWithResponse(file, fileRange, null, downloadRetryOptions, null, false, openOptions)\n     .subscribe(response -> System.out.println(\"Completed download to file\"));\n```\n\nFor more information, see the [Azure Docs][]\n\n\n[Azure Docs]: https://docs.microsoft.com/en-us/rest/api/storageservices/get-blob"
  syntax:
    content: public Mono<Response<PathProperties>> readToFileWithResponse(String filePath, FileRange range, ParallelTransferOptions parallelTransferOptions, DownloadRetryOptions options, DataLakeRequestConditions requestConditions, boolean rangeGetContentMd5, Set<OpenOption> openOptions)
    parameters:
    - id: filePath
      type: java.lang.String
      description: A <xref uid="java.lang.String" data-throw-if-not-resolved="false">String</xref> representing the filePath where the downloaded data will be written.
    - id: range
      type: com.azure.storage.file.datalake.models.FileRange
      description: <xref uid="com.azure.storage.file.datalake.models.FileRange" data-throw-if-not-resolved="false">FileRange</xref>
    - id: parallelTransferOptions
      type: com.azure.storage.common.ParallelTransferOptions
      description: >-
        <xref uid="com.azure.storage.common.ParallelTransferOptions" data-throw-if-not-resolved="false">ParallelTransferOptions</xref> to use to download to file. Number of parallel
         transfers parameter is ignored.
    - id: options
      type: com.azure.storage.file.datalake.models.DownloadRetryOptions
      description: <xref uid="com.azure.storage.file.datalake.models.DownloadRetryOptions" data-throw-if-not-resolved="false">DownloadRetryOptions</xref>
    - id: requestConditions
      type: com.azure.storage.file.datalake.models.DataLakeRequestConditions
      description: <xref uid="com.azure.storage.file.datalake.models.DataLakeRequestConditions" data-throw-if-not-resolved="false">DataLakeRequestConditions</xref>
    - id: rangeGetContentMd5
      type: boolean
      description: Whether the contentMD5 for the specified file range should be returned.
    - id: openOptions
      type: java.util.Set<java.nio.file.OpenOption>
      description: <xref uid="java.nio.file.OpenOption" data-throw-if-not-resolved="false">OpenOptions</xref> to use to configure how to open or create the file.
    return:
      type: reactor.core.publisher.Mono<com.azure.core.http.rest.Response<com.azure.storage.file.datalake.models.PathProperties>>
      description: A reactive response containing the file properties and metadata.
- uid: com.azure.storage.file.datalake.DataLakeFileAsyncClient.readWithResponse(com.azure.storage.file.datalake.models.FileRange,com.azure.storage.file.datalake.models.DownloadRetryOptions,com.azure.storage.file.datalake.models.DataLakeRequestConditions,boolean)
  id: readWithResponse(com.azure.storage.file.datalake.models.FileRange,com.azure.storage.file.datalake.models.DownloadRetryOptions,com.azure.storage.file.datalake.models.DataLakeRequestConditions,boolean)
  artifact: com.azure:azure-storage-file-datalake:12.3.0-beta.1
  parent: com.azure.storage.file.datalake.DataLakeFileAsyncClient
  langs:
  - java
  name: readWithResponse(FileRange range, DownloadRetryOptions options, DataLakeRequestConditions requestConditions, boolean getRangeContentMd5)
  nameWithType: DataLakeFileAsyncClient.readWithResponse(FileRange range, DownloadRetryOptions options, DataLakeRequestConditions requestConditions, boolean getRangeContentMd5)
  fullName: com.azure.storage.file.datalake.DataLakeFileAsyncClient.readWithResponse(FileRange range, DownloadRetryOptions options, DataLakeRequestConditions requestConditions, boolean getRangeContentMd5)
  overload: com.azure.storage.file.datalake.DataLakeFileAsyncClient.readWithResponse*
  type: Method
  package: com.azure.storage.file.datalake
  summary: "Reads a range of bytes from a file.\n\n**Code Samples**\n\n```java\nFileRange range = new FileRange(1024, 2048L);\n DownloadRetryOptions options = new DownloadRetryOptions().setMaxRetryRequests(5);\n \n client.readWithResponse(range, options, null, false).subscribe(response -> {\n     ByteArrayOutputStream readData = new ByteArrayOutputStream();\n     response.getValue().subscribe(piece -> {\n         try {\n             readData.write(piece.array());\n         } catch (IOException ex) {\n             throw new UncheckedIOException(ex);\n         }\n     });\n });\n```\n\nFor more information, see the [Azure Docs][]\n\n\n[Azure Docs]: https://docs.microsoft.com/en-us/rest/api/storageservices/get-blob"
  syntax:
    content: public Mono<FileReadAsyncResponse> readWithResponse(FileRange range, DownloadRetryOptions options, DataLakeRequestConditions requestConditions, boolean getRangeContentMd5)
    parameters:
    - id: range
      type: com.azure.storage.file.datalake.models.FileRange
      description: <xref uid="com.azure.storage.file.datalake.models.FileRange" data-throw-if-not-resolved="false">FileRange</xref>
    - id: options
      type: com.azure.storage.file.datalake.models.DownloadRetryOptions
      description: <xref uid="com.azure.storage.file.datalake.models.DownloadRetryOptions" data-throw-if-not-resolved="false">DownloadRetryOptions</xref>
    - id: requestConditions
      type: com.azure.storage.file.datalake.models.DataLakeRequestConditions
      description: <xref uid="com.azure.storage.file.datalake.models.DataLakeRequestConditions" data-throw-if-not-resolved="false">DataLakeRequestConditions</xref>
    - id: getRangeContentMd5
      type: boolean
      description: Whether the contentMD5 for the specified file range should be returned.
    return:
      type: reactor.core.publisher.Mono<com.azure.storage.file.datalake.models.FileReadAsyncResponse>
      description: A reactive response containing the file data.
- uid: com.azure.storage.file.datalake.DataLakeFileAsyncClient.rename(java.lang.String,java.lang.String)
  id: rename(java.lang.String,java.lang.String)
  artifact: com.azure:azure-storage-file-datalake:12.3.0-beta.1
  parent: com.azure.storage.file.datalake.DataLakeFileAsyncClient
  langs:
  - java
  name: rename(String destinationFileSystem, String destinationPath)
  nameWithType: DataLakeFileAsyncClient.rename(String destinationFileSystem, String destinationPath)
  fullName: com.azure.storage.file.datalake.DataLakeFileAsyncClient.rename(String destinationFileSystem, String destinationPath)
  overload: com.azure.storage.file.datalake.DataLakeFileAsyncClient.rename*
  type: Method
  package: com.azure.storage.file.datalake
  summary: >-
    Moves the file to another location within the file system. For more information see the [Azure Docs][].


    **Code Samples**


    ```java

    DataLakeFileAsyncClient renamedClient = client.rename(fileSystemName, destinationPath).block();
     System.out.println("Directory Client has been renamed");
    ```



    [Azure Docs]: https://docs.microsoft.com/en-us/rest/api/storageservices/datalakestoragegen2/path/create
  syntax:
    content: public Mono<DataLakeFileAsyncClient> rename(String destinationFileSystem, String destinationPath)
    parameters:
    - id: destinationFileSystem
      type: java.lang.String
      description: >-
        The file system of the destination within the account.
         <code>null</code> for the current file system.
    - id: destinationPath
      type: java.lang.String
      description: >-
        Relative path from the file system to rename the file to, excludes the file system name.
         For example if you want to move a file with fileSystem = "myfilesystem", path = "mydir/hello.txt" to another path
         in myfilesystem (ex: newdir/hi.txt) then set the destinationPath = "newdir/hi.txt"
    return:
      type: reactor.core.publisher.Mono<com.azure.storage.file.datalake.DataLakeFileAsyncClient>
      description: A <xref uid="reactor.core.publisher.Mono" data-throw-if-not-resolved="false">Mono</xref> containing a <xref uid="com.azure.storage.file.datalake.DataLakeFileAsyncClient" data-throw-if-not-resolved="false">DataLakeFileAsyncClient</xref> used to interact with the new file created.
- uid: com.azure.storage.file.datalake.DataLakeFileAsyncClient.renameWithResponse(java.lang.String,java.lang.String,com.azure.storage.file.datalake.models.DataLakeRequestConditions,com.azure.storage.file.datalake.models.DataLakeRequestConditions)
  id: renameWithResponse(java.lang.String,java.lang.String,com.azure.storage.file.datalake.models.DataLakeRequestConditions,com.azure.storage.file.datalake.models.DataLakeRequestConditions)
  artifact: com.azure:azure-storage-file-datalake:12.3.0-beta.1
  parent: com.azure.storage.file.datalake.DataLakeFileAsyncClient
  langs:
  - java
  name: renameWithResponse(String destinationFileSystem, String destinationPath, DataLakeRequestConditions sourceRequestConditions, DataLakeRequestConditions destinationRequestConditions)
  nameWithType: DataLakeFileAsyncClient.renameWithResponse(String destinationFileSystem, String destinationPath, DataLakeRequestConditions sourceRequestConditions, DataLakeRequestConditions destinationRequestConditions)
  fullName: com.azure.storage.file.datalake.DataLakeFileAsyncClient.renameWithResponse(String destinationFileSystem, String destinationPath, DataLakeRequestConditions sourceRequestConditions, DataLakeRequestConditions destinationRequestConditions)
  overload: com.azure.storage.file.datalake.DataLakeFileAsyncClient.renameWithResponse*
  type: Method
  package: com.azure.storage.file.datalake
  summary: "Moves the file to another location within the file system. For more information, see the [Azure Docs][].\n\n**Code Samples**\n\n```java\nDataLakeRequestConditions sourceRequestConditions = new DataLakeRequestConditions()\n     .setLeaseId(leaseId);\n DataLakeRequestConditions destinationRequestConditions = new DataLakeRequestConditions();\n \n DataLakeFileAsyncClient newRenamedClient = client.renameWithResponse(fileSystemName, destinationPath,\n     sourceRequestConditions, destinationRequestConditions).block().getValue();\n System.out.println(\"Directory Client has been renamed\");\n```\n\n\n[Azure Docs]: https://docs.microsoft.com/en-us/rest/api/storageservices/datalakestoragegen2/path/create"
  syntax:
    content: public Mono<Response<DataLakeFileAsyncClient>> renameWithResponse(String destinationFileSystem, String destinationPath, DataLakeRequestConditions sourceRequestConditions, DataLakeRequestConditions destinationRequestConditions)
    parameters:
    - id: destinationFileSystem
      type: java.lang.String
      description: >-
        The file system of the destination within the account.
         <code>null</code> for the current file system.
    - id: destinationPath
      type: java.lang.String
      description: >-
        Relative path from the file system to rename the file to, excludes the file system name.
         For example if you want to move a file with fileSystem = "myfilesystem", path = "mydir/hello.txt" to another path
         in myfilesystem (ex: newdir/hi.txt) then set the destinationPath = "newdir/hi.txt"
    - id: sourceRequestConditions
      type: com.azure.storage.file.datalake.models.DataLakeRequestConditions
      description: <xref uid="com.azure.storage.file.datalake.models.DataLakeRequestConditions" data-throw-if-not-resolved="false">DataLakeRequestConditions</xref> against the source.
    - id: destinationRequestConditions
      type: com.azure.storage.file.datalake.models.DataLakeRequestConditions
      description: <xref uid="com.azure.storage.file.datalake.models.DataLakeRequestConditions" data-throw-if-not-resolved="false">DataLakeRequestConditions</xref> against the destination.
    return:
      type: reactor.core.publisher.Mono<com.azure.core.http.rest.Response<com.azure.storage.file.datalake.DataLakeFileAsyncClient>>
      description: A <xref uid="reactor.core.publisher.Mono" data-throw-if-not-resolved="false">Mono</xref> containing a <xref uid="com.azure.core.http.rest.Response" data-throw-if-not-resolved="false">Response</xref> whose <xref uid="com.azure.core.http.rest.Response.getValue*" data-throw-if-not-resolved="false">value</xref> contains a <xref uid="com.azure.storage.file.datalake.DataLakeFileAsyncClient" data-throw-if-not-resolved="false">DataLakeFileAsyncClient</xref> used to interact with the file created.
- uid: com.azure.storage.file.datalake.DataLakeFileAsyncClient.scheduleDeletion(com.azure.storage.file.datalake.options.FileScheduleDeletionOptions)
  id: scheduleDeletion(com.azure.storage.file.datalake.options.FileScheduleDeletionOptions)
  artifact: com.azure:azure-storage-file-datalake:12.3.0-beta.1
  parent: com.azure.storage.file.datalake.DataLakeFileAsyncClient
  langs:
  - java
  name: scheduleDeletion(FileScheduleDeletionOptions options)
  nameWithType: DataLakeFileAsyncClient.scheduleDeletion(FileScheduleDeletionOptions options)
  fullName: com.azure.storage.file.datalake.DataLakeFileAsyncClient.scheduleDeletion(FileScheduleDeletionOptions options)
  overload: com.azure.storage.file.datalake.DataLakeFileAsyncClient.scheduleDeletion*
  type: Method
  package: com.azure.storage.file.datalake
  summary: "Schedules the file for deletion.\n\n**Code Samples**\n\n```java\nFileScheduleDeletionOptions options = new FileScheduleDeletionOptions(OffsetDateTime.now().plusDays(1));\n \n client.scheduleDeletion(options)\n     .subscribe(r -> System.out.println(\"File deletion has been scheduled\"));\n```"
  syntax:
    content: public Mono<Void> scheduleDeletion(FileScheduleDeletionOptions options)
    parameters:
    - id: options
      type: com.azure.storage.file.datalake.options.FileScheduleDeletionOptions
      description: Schedule deletion parameters.
    return:
      type: reactor.core.publisher.Mono<java.lang.Void>
      description: A reactive response signalling completion.
- uid: com.azure.storage.file.datalake.DataLakeFileAsyncClient.scheduleDeletionWithResponse(com.azure.storage.file.datalake.options.FileScheduleDeletionOptions)
  id: scheduleDeletionWithResponse(com.azure.storage.file.datalake.options.FileScheduleDeletionOptions)
  artifact: com.azure:azure-storage-file-datalake:12.3.0-beta.1
  parent: com.azure.storage.file.datalake.DataLakeFileAsyncClient
  langs:
  - java
  name: scheduleDeletionWithResponse(FileScheduleDeletionOptions options)
  nameWithType: DataLakeFileAsyncClient.scheduleDeletionWithResponse(FileScheduleDeletionOptions options)
  fullName: com.azure.storage.file.datalake.DataLakeFileAsyncClient.scheduleDeletionWithResponse(FileScheduleDeletionOptions options)
  overload: com.azure.storage.file.datalake.DataLakeFileAsyncClient.scheduleDeletionWithResponse*
  type: Method
  package: com.azure.storage.file.datalake
  summary: "Schedules the file for deletion.\n\n**Code Samples**\n\n```java\nFileScheduleDeletionOptions options = new FileScheduleDeletionOptions(OffsetDateTime.now().plusDays(1));\n \n client.scheduleDeletionWithResponse(options)\n     .subscribe(r -> System.out.println(\"File deletion has been scheduled\"));\n```"
  syntax:
    content: public Mono<Response<Void>> scheduleDeletionWithResponse(FileScheduleDeletionOptions options)
    parameters:
    - id: options
      type: com.azure.storage.file.datalake.options.FileScheduleDeletionOptions
      description: Schedule deletion parameters.
    return:
      type: reactor.core.publisher.Mono<com.azure.core.http.rest.Response<java.lang.Void>>
      description: A reactive response signalling completion.
- uid: com.azure.storage.file.datalake.DataLakeFileAsyncClient.upload(reactor.core.publisher.Flux<java.nio.ByteBuffer>,com.azure.storage.common.ParallelTransferOptions)
  id: upload(reactor.core.publisher.Flux<java.nio.ByteBuffer>,com.azure.storage.common.ParallelTransferOptions)
  artifact: com.azure:azure-storage-file-datalake:12.3.0-beta.1
  parent: com.azure.storage.file.datalake.DataLakeFileAsyncClient
  langs:
  - java
  name: upload(Flux<ByteBuffer> data, ParallelTransferOptions parallelTransferOptions)
  nameWithType: DataLakeFileAsyncClient.upload(Flux<ByteBuffer> data, ParallelTransferOptions parallelTransferOptions)
  fullName: com.azure.storage.file.datalake.DataLakeFileAsyncClient.upload(Flux<ByteBuffer> data, ParallelTransferOptions parallelTransferOptions)
  overload: com.azure.storage.file.datalake.DataLakeFileAsyncClient.upload*
  type: Method
  package: com.azure.storage.file.datalake
  summary: >-
    Creates a new file and uploads content.


    **Code Samples**


    ```java

    client.uploadFromFile(filePath)
         .doOnError(throwable -> System.err.printf("Failed to upload from file %s%n", throwable.getMessage()))
         .subscribe(completion -> System.out.println("Upload from file succeeded"));
    ```
  syntax:
    content: public Mono<PathInfo> upload(Flux<ByteBuffer> data, ParallelTransferOptions parallelTransferOptions)
    parameters:
    - id: data
      type: reactor.core.publisher.Flux<java.nio.ByteBuffer>
      description: >-
        The data to write to the file. Unlike other upload methods, this method does not require that the
         <code>Flux</code> be replayable. In other words, it does not have to support multiple subscribers and is not expected
         to produce the same values across subscriptions.
    - id: parallelTransferOptions
      type: com.azure.storage.common.ParallelTransferOptions
      description: <xref uid="com.azure.storage.common.ParallelTransferOptions" data-throw-if-not-resolved="false">ParallelTransferOptions</xref> used to configure buffered uploading.
    return:
      type: reactor.core.publisher.Mono<com.azure.storage.file.datalake.models.PathInfo>
      description: A reactive response containing the information of the uploaded file.
- uid: com.azure.storage.file.datalake.DataLakeFileAsyncClient.upload(reactor.core.publisher.Flux<java.nio.ByteBuffer>,com.azure.storage.common.ParallelTransferOptions,boolean)
  id: upload(reactor.core.publisher.Flux<java.nio.ByteBuffer>,com.azure.storage.common.ParallelTransferOptions,boolean)
  artifact: com.azure:azure-storage-file-datalake:12.3.0-beta.1
  parent: com.azure.storage.file.datalake.DataLakeFileAsyncClient
  langs:
  - java
  name: upload(Flux<ByteBuffer> data, ParallelTransferOptions parallelTransferOptions, boolean overwrite)
  nameWithType: DataLakeFileAsyncClient.upload(Flux<ByteBuffer> data, ParallelTransferOptions parallelTransferOptions, boolean overwrite)
  fullName: com.azure.storage.file.datalake.DataLakeFileAsyncClient.upload(Flux<ByteBuffer> data, ParallelTransferOptions parallelTransferOptions, boolean overwrite)
  overload: com.azure.storage.file.datalake.DataLakeFileAsyncClient.upload*
  type: Method
  package: com.azure.storage.file.datalake
  summary: >-
    Creates a new file and uploads content.


    **Code Samples**


    ```java

    boolean overwrite = false; // Default behavior
     client.uploadFromFile(filePath, overwrite)
         .doOnError(throwable -> System.err.printf("Failed to upload from file %s%n", throwable.getMessage()))
         .subscribe(completion -> System.out.println("Upload from file succeeded"));
    ```
  syntax:
    content: public Mono<PathInfo> upload(Flux<ByteBuffer> data, ParallelTransferOptions parallelTransferOptions, boolean overwrite)
    parameters:
    - id: data
      type: reactor.core.publisher.Flux<java.nio.ByteBuffer>
      description: >-
        The data to write to the file. Unlike other upload methods, this method does not require that the
         <code>Flux</code> be replayable. In other words, it does not have to support multiple subscribers and is not expected
         to produce the same values across subscriptions.
    - id: parallelTransferOptions
      type: com.azure.storage.common.ParallelTransferOptions
      description: <xref uid="com.azure.storage.common.ParallelTransferOptions" data-throw-if-not-resolved="false">ParallelTransferOptions</xref> used to configure buffered uploading.
    - id: overwrite
      type: boolean
      description: Whether or not to overwrite, should the file already exist.
    return:
      type: reactor.core.publisher.Mono<com.azure.storage.file.datalake.models.PathInfo>
      description: A reactive response containing the information of the uploaded file.
- uid: com.azure.storage.file.datalake.DataLakeFileAsyncClient.uploadFromFile(java.lang.String)
  id: uploadFromFile(java.lang.String)
  artifact: com.azure:azure-storage-file-datalake:12.3.0-beta.1
  parent: com.azure.storage.file.datalake.DataLakeFileAsyncClient
  langs:
  - java
  name: uploadFromFile(String filePath)
  nameWithType: DataLakeFileAsyncClient.uploadFromFile(String filePath)
  fullName: com.azure.storage.file.datalake.DataLakeFileAsyncClient.uploadFromFile(String filePath)
  overload: com.azure.storage.file.datalake.DataLakeFileAsyncClient.uploadFromFile*
  type: Method
  package: com.azure.storage.file.datalake
  summary: >-
    Creates a new file, with the content of the specified file. By default this method will not overwrite an existing file.


    **Code Samples**


    ```java

    client.uploadFromFile(filePath)
         .doOnError(throwable -> System.err.printf("Failed to upload from file %s%n", throwable.getMessage()))
         .subscribe(completion -> System.out.println("Upload from file succeeded"));
    ```
  syntax:
    content: public Mono<Void> uploadFromFile(String filePath)
    parameters:
    - id: filePath
      type: java.lang.String
      description: Path to the upload file
    return:
      type: reactor.core.publisher.Mono<java.lang.Void>
      description: An empty response
- uid: com.azure.storage.file.datalake.DataLakeFileAsyncClient.uploadFromFile(java.lang.String,boolean)
  id: uploadFromFile(java.lang.String,boolean)
  artifact: com.azure:azure-storage-file-datalake:12.3.0-beta.1
  parent: com.azure.storage.file.datalake.DataLakeFileAsyncClient
  langs:
  - java
  name: uploadFromFile(String filePath, boolean overwrite)
  nameWithType: DataLakeFileAsyncClient.uploadFromFile(String filePath, boolean overwrite)
  fullName: com.azure.storage.file.datalake.DataLakeFileAsyncClient.uploadFromFile(String filePath, boolean overwrite)
  overload: com.azure.storage.file.datalake.DataLakeFileAsyncClient.uploadFromFile*
  type: Method
  package: com.azure.storage.file.datalake
  summary: >-
    Creates a new file, with the content of the specified file.


    **Code Samples**


    ```java

    boolean overwrite = false; // Default behavior
     client.uploadFromFile(filePath, overwrite)
         .doOnError(throwable -> System.err.printf("Failed to upload from file %s%n", throwable.getMessage()))
         .subscribe(completion -> System.out.println("Upload from file succeeded"));
    ```
  syntax:
    content: public Mono<Void> uploadFromFile(String filePath, boolean overwrite)
    parameters:
    - id: filePath
      type: java.lang.String
      description: Path to the upload file
    - id: overwrite
      type: boolean
      description: Whether or not to overwrite, should the file already exist.
    return:
      type: reactor.core.publisher.Mono<java.lang.Void>
      description: An empty response
- uid: com.azure.storage.file.datalake.DataLakeFileAsyncClient.uploadFromFile(java.lang.String,com.azure.storage.common.ParallelTransferOptions,com.azure.storage.file.datalake.models.PathHttpHeaders,java.util.Map<java.lang.String,java.lang.String>,com.azure.storage.file.datalake.models.DataLakeRequestConditions)
  id: uploadFromFile(java.lang.String,com.azure.storage.common.ParallelTransferOptions,com.azure.storage.file.datalake.models.PathHttpHeaders,java.util.Map<java.lang.String,java.lang.String>,com.azure.storage.file.datalake.models.DataLakeRequestConditions)
  artifact: com.azure:azure-storage-file-datalake:12.3.0-beta.1
  parent: com.azure.storage.file.datalake.DataLakeFileAsyncClient
  langs:
  - java
  name: uploadFromFile(String filePath, ParallelTransferOptions parallelTransferOptions, PathHttpHeaders headers, Map<String,String> metadata, DataLakeRequestConditions requestConditions)
  nameWithType: DataLakeFileAsyncClient.uploadFromFile(String filePath, ParallelTransferOptions parallelTransferOptions, PathHttpHeaders headers, Map<String,String> metadata, DataLakeRequestConditions requestConditions)
  fullName: com.azure.storage.file.datalake.DataLakeFileAsyncClient.uploadFromFile(String filePath, ParallelTransferOptions parallelTransferOptions, PathHttpHeaders headers, Map<String,String> metadata, DataLakeRequestConditions requestConditions)
  overload: com.azure.storage.file.datalake.DataLakeFileAsyncClient.uploadFromFile*
  type: Method
  package: com.azure.storage.file.datalake
  summary: "Creates a new file, with the content of the specified file.\n\nTo avoid overwriting, pass \"\\*\" to <xref uid=\"com.azure.storage.file.datalake.models.DataLakeRequestConditions.setIfNoneMatch(java.lang.String)\" data-throw-if-not-resolved=\"false\">DataLakeRequestConditions#setIfNoneMatch(String)</xref>.\n\n**Code Samples**\n\n```java\nPathHttpHeaders headers = new PathHttpHeaders()\n     .setContentMd5(\"data\".getBytes(StandardCharsets.UTF_8))\n     .setContentLanguage(\"en-US\")\n     .setContentType(\"binary\");\n \n Map<String, String> metadata = Collections.singletonMap(\"metadata\", \"value\");\n DataLakeRequestConditions requestConditions = new DataLakeRequestConditions()\n     .setLeaseId(leaseId)\n     .setIfUnmodifiedSince(OffsetDateTime.now().minusDays(3));\n Long blockSize = 100L * 1024L * 1024L; // 100 MB;\n ParallelTransferOptions parallelTransferOptions = new ParallelTransferOptions().setBlockSizeLong(blockSize);\n \n client.uploadFromFile(filePath, parallelTransferOptions, headers, metadata, requestConditions)\n     .doOnError(throwable -> System.err.printf(\"Failed to upload from file %s%n\", throwable.getMessage()))\n     .subscribe(completion -> System.out.println(\"Upload from file succeeded\"));\n```"
  syntax:
    content: public Mono<Void> uploadFromFile(String filePath, ParallelTransferOptions parallelTransferOptions, PathHttpHeaders headers, Map<String,String> metadata, DataLakeRequestConditions requestConditions)
    parameters:
    - id: filePath
      type: java.lang.String
      description: Path to the upload file
    - id: parallelTransferOptions
      type: com.azure.storage.common.ParallelTransferOptions
      description: >-
        <xref uid="com.azure.storage.common.ParallelTransferOptions" data-throw-if-not-resolved="false">ParallelTransferOptions</xref> to use to upload from file. Number of parallel
         transfers parameter is ignored.
    - id: headers
      type: com.azure.storage.file.datalake.models.PathHttpHeaders
      description: <xref uid="com.azure.storage.file.datalake.models.PathHttpHeaders" data-throw-if-not-resolved="false">PathHttpHeaders</xref>
    - id: metadata
      type: java.util.Map<java.lang.String,java.lang.String>
      description: Metadata to associate with the resource.
    - id: requestConditions
      type: com.azure.storage.file.datalake.models.DataLakeRequestConditions
      description: <xref uid="com.azure.storage.file.datalake.models.DataLakeRequestConditions" data-throw-if-not-resolved="false">DataLakeRequestConditions</xref>
    return:
      type: reactor.core.publisher.Mono<java.lang.Void>
      description: An empty response
- uid: com.azure.storage.file.datalake.DataLakeFileAsyncClient.uploadWithResponse(com.azure.storage.file.datalake.options.FileParallelUploadOptions)
  id: uploadWithResponse(com.azure.storage.file.datalake.options.FileParallelUploadOptions)
  artifact: com.azure:azure-storage-file-datalake:12.3.0-beta.1
  parent: com.azure.storage.file.datalake.DataLakeFileAsyncClient
  langs:
  - java
  name: uploadWithResponse(FileParallelUploadOptions options)
  nameWithType: DataLakeFileAsyncClient.uploadWithResponse(FileParallelUploadOptions options)
  fullName: com.azure.storage.file.datalake.DataLakeFileAsyncClient.uploadWithResponse(FileParallelUploadOptions options)
  overload: com.azure.storage.file.datalake.DataLakeFileAsyncClient.uploadWithResponse*
  type: Method
  package: com.azure.storage.file.datalake
  summary: "Creates a new file.\n\nTo avoid overwriting, pass \"\\*\" to <xref uid=\"com.azure.storage.file.datalake.models.DataLakeRequestConditions.setIfNoneMatch(java.lang.String)\" data-throw-if-not-resolved=\"false\">DataLakeRequestConditions#setIfNoneMatch(String)</xref>.\n\n**Code Samples**\n\n```java\nPathHttpHeaders headers = new PathHttpHeaders()\n     .setContentMd5(\"data\".getBytes(StandardCharsets.UTF_8))\n     .setContentLanguage(\"en-US\")\n     .setContentType(\"binary\");\n \n Map<String, String> metadata = Collections.singletonMap(\"metadata\", \"value\");\n DataLakeRequestConditions requestConditions = new DataLakeRequestConditions()\n     .setLeaseId(leaseId)\n     .setIfUnmodifiedSince(OffsetDateTime.now().minusDays(3));\n Long blockSize = 100L * 1024L * 1024L; // 100 MB;\n ParallelTransferOptions parallelTransferOptions = new ParallelTransferOptions().setBlockSizeLong(blockSize);\n \n client.uploadWithResponse(new FileParallelUploadOptions(data)\n     .setParallelTransferOptions(parallelTransferOptions).setHeaders(headers)\n     .setMetadata(metadata).setRequestConditions(requestConditions)\n     .setPermissions(\"permissions\").setUmask(\"umask\"))\n     .subscribe(response -> System.out.println(\"Uploaded file %n\"));\n```\n\n**Using Progress Reporting**\n\n```java\nPathHttpHeaders httpHeaders = new PathHttpHeaders()\n     .setContentMd5(\"data\".getBytes(StandardCharsets.UTF_8))\n     .setContentLanguage(\"en-US\")\n     .setContentType(\"binary\");\n \n Map<String, String> metadataMap = Collections.singletonMap(\"metadata\", \"value\");\n DataLakeRequestConditions conditions = new DataLakeRequestConditions()\n     .setLeaseId(leaseId)\n     .setIfUnmodifiedSince(OffsetDateTime.now().minusDays(3));\n ParallelTransferOptions pto = new ParallelTransferOptions()\n     .setBlockSizeLong(blockSize)\n     .setProgressReceiver(bytesTransferred -> System.out.printf(\"Upload progress: %s bytes sent\", bytesTransferred));\n \n client.uploadWithResponse(new FileParallelUploadOptions(data)\n     .setParallelTransferOptions(parallelTransferOptions).setHeaders(headers)\n     .setMetadata(metadata).setRequestConditions(requestConditions)\n     .setPermissions(\"permissions\").setUmask(\"umask\"))\n     .subscribe(response -> System.out.println(\"Uploaded file %n\"));\n```"
  syntax:
    content: public Mono<Response<PathInfo>> uploadWithResponse(FileParallelUploadOptions options)
    parameters:
    - id: options
      type: com.azure.storage.file.datalake.options.FileParallelUploadOptions
      description: <xref uid="com.azure.storage.file.datalake.options.FileParallelUploadOptions" data-throw-if-not-resolved="false">FileParallelUploadOptions</xref>
    return:
      type: reactor.core.publisher.Mono<com.azure.core.http.rest.Response<com.azure.storage.file.datalake.models.PathInfo>>
      description: A reactive response containing the information of the uploaded file.
- uid: com.azure.storage.file.datalake.DataLakeFileAsyncClient.uploadWithResponse(reactor.core.publisher.Flux<java.nio.ByteBuffer>,com.azure.storage.common.ParallelTransferOptions,com.azure.storage.file.datalake.models.PathHttpHeaders,java.util.Map<java.lang.String,java.lang.String>,com.azure.storage.file.datalake.models.DataLakeRequestConditions)
  id: uploadWithResponse(reactor.core.publisher.Flux<java.nio.ByteBuffer>,com.azure.storage.common.ParallelTransferOptions,com.azure.storage.file.datalake.models.PathHttpHeaders,java.util.Map<java.lang.String,java.lang.String>,com.azure.storage.file.datalake.models.DataLakeRequestConditions)
  artifact: com.azure:azure-storage-file-datalake:12.3.0-beta.1
  parent: com.azure.storage.file.datalake.DataLakeFileAsyncClient
  langs:
  - java
  name: uploadWithResponse(Flux<ByteBuffer> data, ParallelTransferOptions parallelTransferOptions, PathHttpHeaders headers, Map<String,String> metadata, DataLakeRequestConditions requestConditions)
  nameWithType: DataLakeFileAsyncClient.uploadWithResponse(Flux<ByteBuffer> data, ParallelTransferOptions parallelTransferOptions, PathHttpHeaders headers, Map<String,String> metadata, DataLakeRequestConditions requestConditions)
  fullName: com.azure.storage.file.datalake.DataLakeFileAsyncClient.uploadWithResponse(Flux<ByteBuffer> data, ParallelTransferOptions parallelTransferOptions, PathHttpHeaders headers, Map<String,String> metadata, DataLakeRequestConditions requestConditions)
  overload: com.azure.storage.file.datalake.DataLakeFileAsyncClient.uploadWithResponse*
  type: Method
  package: com.azure.storage.file.datalake
  summary: "Creates a new file.\n\nTo avoid overwriting, pass \"\\*\" to <xref uid=\"com.azure.storage.file.datalake.models.DataLakeRequestConditions.setIfNoneMatch(java.lang.String)\" data-throw-if-not-resolved=\"false\">DataLakeRequestConditions#setIfNoneMatch(String)</xref>.\n\n**Code Samples**\n\n```java\nPathHttpHeaders headers = new PathHttpHeaders()\n     .setContentMd5(\"data\".getBytes(StandardCharsets.UTF_8))\n     .setContentLanguage(\"en-US\")\n     .setContentType(\"binary\");\n \n Map<String, String> metadata = Collections.singletonMap(\"metadata\", \"value\");\n DataLakeRequestConditions requestConditions = new DataLakeRequestConditions()\n     .setLeaseId(leaseId)\n     .setIfUnmodifiedSince(OffsetDateTime.now().minusDays(3));\n Long blockSize = 100L * 1024L * 1024L; // 100 MB;\n ParallelTransferOptions parallelTransferOptions = new ParallelTransferOptions().setBlockSizeLong(blockSize);\n \n client.uploadWithResponse(data, parallelTransferOptions, headers, metadata, requestConditions)\n     .subscribe(response -> System.out.println(\"Uploaded file %n\"));\n```\n\n**Using Progress Reporting**\n\n```java\nPathHttpHeaders httpHeaders = new PathHttpHeaders()\n     .setContentMd5(\"data\".getBytes(StandardCharsets.UTF_8))\n     .setContentLanguage(\"en-US\")\n     .setContentType(\"binary\");\n \n Map<String, String> metadataMap = Collections.singletonMap(\"metadata\", \"value\");\n DataLakeRequestConditions conditions = new DataLakeRequestConditions()\n     .setLeaseId(leaseId)\n     .setIfUnmodifiedSince(OffsetDateTime.now().minusDays(3));\n ParallelTransferOptions pto = new ParallelTransferOptions()\n     .setBlockSizeLong(blockSize)\n     .setProgressReceiver(bytesTransferred -> System.out.printf(\"Upload progress: %s bytes sent\", bytesTransferred));\n \n client.uploadWithResponse(data, pto, httpHeaders, metadataMap, conditions)\n     .subscribe(response -> System.out.println(\"Uploaded file %n\"));\n```"
  syntax:
    content: public Mono<Response<PathInfo>> uploadWithResponse(Flux<ByteBuffer> data, ParallelTransferOptions parallelTransferOptions, PathHttpHeaders headers, Map<String,String> metadata, DataLakeRequestConditions requestConditions)
    parameters:
    - id: data
      type: reactor.core.publisher.Flux<java.nio.ByteBuffer>
      description: >-
        The data to write to the file. Unlike other upload methods, this method does not require that the
         <code>Flux</code> be replayable. In other words, it does not have to support multiple subscribers and is not expected
         to produce the same values across subscriptions.
    - id: parallelTransferOptions
      type: com.azure.storage.common.ParallelTransferOptions
      description: <xref uid="com.azure.storage.common.ParallelTransferOptions" data-throw-if-not-resolved="false">ParallelTransferOptions</xref> used to configure buffered uploading.
    - id: headers
      type: com.azure.storage.file.datalake.models.PathHttpHeaders
      description: <xref uid="com.azure.storage.file.datalake.models.PathHttpHeaders" data-throw-if-not-resolved="false">PathHttpHeaders</xref>
    - id: metadata
      type: java.util.Map<java.lang.String,java.lang.String>
      description: Metadata to associate with the resource.
    - id: requestConditions
      type: com.azure.storage.file.datalake.models.DataLakeRequestConditions
      description: <xref uid="com.azure.storage.file.datalake.models.DataLakeRequestConditions" data-throw-if-not-resolved="false">DataLakeRequestConditions</xref>
    return:
      type: reactor.core.publisher.Mono<com.azure.core.http.rest.Response<com.azure.storage.file.datalake.models.PathInfo>>
      description: A reactive response containing the information of the uploaded file.
references:
- uid: com.azure.core.http.HttpPipeline
  spec.java:
  - uid: com.azure.core.http.HttpPipeline
    name: HttpPipeline
    fullName: com.azure.core.http.HttpPipeline
- uid: java.lang.String
  spec.java:
  - uid: java.lang.String
    name: String
    fullName: java.lang.String
- uid: com.azure.storage.file.datalake.DataLakeServiceVersion
  name: DataLakeServiceVersion
  nameWithType: DataLakeServiceVersion
  fullName: com.azure.storage.file.datalake.DataLakeServiceVersion
- uid: com.azure.storage.blob.specialized.BlockBlobAsyncClient
  spec.java:
  - uid: com.azure.storage.blob.specialized.BlockBlobAsyncClient
    name: BlockBlobAsyncClient
    fullName: com.azure.storage.blob.specialized.BlockBlobAsyncClient
- uid: com.azure.storage.file.datalake.DataLakeFileAsyncClient.DataLakeFileAsyncClient*
  name: DataLakeFileAsyncClient
  nameWithType: DataLakeFileAsyncClient.DataLakeFileAsyncClient
  fullName: com.azure.storage.file.datalake.DataLakeFileAsyncClient.DataLakeFileAsyncClient
  package: com.azure.storage.file.datalake
- uid: com.azure.storage.file.datalake.DataLakePathAsyncClient
  name: DataLakePathAsyncClient
  nameWithType: DataLakePathAsyncClient
  fullName: com.azure.storage.file.datalake.DataLakePathAsyncClient
- uid: com.azure.storage.file.datalake.DataLakeFileAsyncClient.getFileUrl*
  name: getFileUrl
  nameWithType: DataLakeFileAsyncClient.getFileUrl
  fullName: com.azure.storage.file.datalake.DataLakeFileAsyncClient.getFileUrl
  package: com.azure.storage.file.datalake
- uid: com.azure.storage.file.datalake.DataLakeFileAsyncClient.getFilePath*
  name: getFilePath
  nameWithType: DataLakeFileAsyncClient.getFilePath
  fullName: com.azure.storage.file.datalake.DataLakeFileAsyncClient.getFilePath
  package: com.azure.storage.file.datalake
- uid: com.azure.storage.file.datalake.DataLakeFileAsyncClient.getFileName*
  name: getFileName
  nameWithType: DataLakeFileAsyncClient.getFileName
  fullName: com.azure.storage.file.datalake.DataLakeFileAsyncClient.getFileName
  package: com.azure.storage.file.datalake
- uid: reactor.core.publisher.Mono<java.lang.Void>
  spec.java:
  - uid: reactor.core.publisher.Mono
    name: Mono
    fullName: reactor.core.publisher.Mono
  - name: <
    fullName: <
  - uid: java.lang.Void
    name: Void
    fullName: java.lang.Void
  - name: '>'
    fullName: '>'
- uid: com.azure.storage.file.datalake.DataLakeFileAsyncClient.delete*
  name: delete
  nameWithType: DataLakeFileAsyncClient.delete
  fullName: com.azure.storage.file.datalake.DataLakeFileAsyncClient.delete
  package: com.azure.storage.file.datalake
- uid: com.azure.storage.file.datalake.models.DataLakeRequestConditions
  name: DataLakeRequestConditions
  nameWithType: DataLakeRequestConditions
  fullName: com.azure.storage.file.datalake.models.DataLakeRequestConditions
- uid: reactor.core.publisher.Mono<com.azure.core.http.rest.Response<java.lang.Void>>
  spec.java:
  - uid: reactor.core.publisher.Mono
    name: Mono
    fullName: reactor.core.publisher.Mono
  - name: <
    fullName: <
  - uid: com.azure.core.http.rest.Response
    name: Response
    fullName: com.azure.core.http.rest.Response
  - name: <
    fullName: <
  - uid: java.lang.Void
    name: Void
    fullName: java.lang.Void
  - name: '>'
    fullName: '>'
  - name: '>'
    fullName: '>'
- uid: com.azure.storage.file.datalake.DataLakeFileAsyncClient.deleteWithResponse*
  name: deleteWithResponse
  nameWithType: DataLakeFileAsyncClient.deleteWithResponse
  fullName: com.azure.storage.file.datalake.DataLakeFileAsyncClient.deleteWithResponse
  package: com.azure.storage.file.datalake
- uid: reactor.core.publisher.Flux<java.nio.ByteBuffer>
  spec.java:
  - uid: reactor.core.publisher.Flux
    name: Flux
    fullName: reactor.core.publisher.Flux
  - name: <
    fullName: <
  - uid: java.nio.ByteBuffer
    name: ByteBuffer
    fullName: java.nio.ByteBuffer
  - name: '>'
    fullName: '>'
- uid: com.azure.storage.common.ParallelTransferOptions
  spec.java:
  - uid: com.azure.storage.common.ParallelTransferOptions
    name: ParallelTransferOptions
    fullName: com.azure.storage.common.ParallelTransferOptions
- uid: reactor.core.publisher.Mono<com.azure.storage.file.datalake.models.PathInfo>
  spec.java:
  - uid: reactor.core.publisher.Mono
    name: Mono
    fullName: reactor.core.publisher.Mono
  - name: <
    fullName: <
  - uid: com.azure.storage.file.datalake.models.PathInfo
    name: PathInfo
    fullName: com.azure.storage.file.datalake.models.PathInfo
  - name: '>'
    fullName: '>'
- uid: com.azure.storage.file.datalake.DataLakeFileAsyncClient.upload*
  name: upload
  nameWithType: DataLakeFileAsyncClient.upload
  fullName: com.azure.storage.file.datalake.DataLakeFileAsyncClient.upload
  package: com.azure.storage.file.datalake
- uid: boolean
  spec.java:
  - uid: boolean
    name: boolean
    fullName: boolean
- uid: com.azure.storage.file.datalake.models.PathHttpHeaders
  name: PathHttpHeaders
  nameWithType: PathHttpHeaders
  fullName: com.azure.storage.file.datalake.models.PathHttpHeaders
- uid: java.util.Map<java.lang.String,java.lang.String>
  spec.java:
  - uid: java.util.Map
    name: Map
    fullName: java.util.Map
  - name: <
    fullName: <
  - uid: java.lang.String
    name: String
    fullName: java.lang.String
  - name: ','
    fullName: ','
  - uid: java.lang.String
    name: String
    fullName: java.lang.String
  - name: '>'
    fullName: '>'
- uid: reactor.core.publisher.Mono<com.azure.core.http.rest.Response<com.azure.storage.file.datalake.models.PathInfo>>
  spec.java:
  - uid: reactor.core.publisher.Mono
    name: Mono
    fullName: reactor.core.publisher.Mono
  - name: <
    fullName: <
  - uid: com.azure.core.http.rest.Response
    name: Response
    fullName: com.azure.core.http.rest.Response
  - name: <
    fullName: <
  - uid: com.azure.storage.file.datalake.models.PathInfo
    name: PathInfo
    fullName: com.azure.storage.file.datalake.models.PathInfo
  - name: '>'
    fullName: '>'
  - name: '>'
    fullName: '>'
- uid: com.azure.storage.file.datalake.DataLakeFileAsyncClient.uploadWithResponse*
  name: uploadWithResponse
  nameWithType: DataLakeFileAsyncClient.uploadWithResponse
  fullName: com.azure.storage.file.datalake.DataLakeFileAsyncClient.uploadWithResponse
  package: com.azure.storage.file.datalake
- uid: com.azure.storage.file.datalake.options.FileParallelUploadOptions
  name: FileParallelUploadOptions
  nameWithType: FileParallelUploadOptions
  fullName: com.azure.storage.file.datalake.options.FileParallelUploadOptions
- uid: com.azure.storage.file.datalake.DataLakeFileAsyncClient.uploadFromFile*
  name: uploadFromFile
  nameWithType: DataLakeFileAsyncClient.uploadFromFile
  fullName: com.azure.storage.file.datalake.DataLakeFileAsyncClient.uploadFromFile
  package: com.azure.storage.file.datalake
- uid: long
  spec.java:
  - uid: long
    name: long
    fullName: long
- uid: com.azure.storage.file.datalake.DataLakeFileAsyncClient.append*
  name: append
  nameWithType: DataLakeFileAsyncClient.append
  fullName: com.azure.storage.file.datalake.DataLakeFileAsyncClient.append
  package: com.azure.storage.file.datalake
- uid: byte[]
  spec.java:
  - uid: byte
    name: byte
    fullName: byte
  - name: '[]'
    fullName: '[]'
- uid: com.azure.storage.file.datalake.DataLakeFileAsyncClient.appendWithResponse*
  name: appendWithResponse
  nameWithType: DataLakeFileAsyncClient.appendWithResponse
  fullName: com.azure.storage.file.datalake.DataLakeFileAsyncClient.appendWithResponse
  package: com.azure.storage.file.datalake
- uid: com.azure.storage.file.datalake.DataLakeFileAsyncClient.flush*
  name: flush
  nameWithType: DataLakeFileAsyncClient.flush
  fullName: com.azure.storage.file.datalake.DataLakeFileAsyncClient.flush
  package: com.azure.storage.file.datalake
- uid: com.azure.storage.file.datalake.DataLakeFileAsyncClient.flushWithResponse*
  name: flushWithResponse
  nameWithType: DataLakeFileAsyncClient.flushWithResponse
  fullName: com.azure.storage.file.datalake.DataLakeFileAsyncClient.flushWithResponse
  package: com.azure.storage.file.datalake
- uid: com.azure.storage.file.datalake.DataLakeFileAsyncClient.read*
  name: read
  nameWithType: DataLakeFileAsyncClient.read
  fullName: com.azure.storage.file.datalake.DataLakeFileAsyncClient.read
  package: com.azure.storage.file.datalake
- uid: com.azure.storage.file.datalake.models.FileRange
  name: FileRange
  nameWithType: FileRange
  fullName: com.azure.storage.file.datalake.models.FileRange
- uid: com.azure.storage.file.datalake.models.DownloadRetryOptions
  name: DownloadRetryOptions
  nameWithType: DownloadRetryOptions
  fullName: com.azure.storage.file.datalake.models.DownloadRetryOptions
- uid: reactor.core.publisher.Mono<com.azure.storage.file.datalake.models.FileReadAsyncResponse>
  spec.java:
  - uid: reactor.core.publisher.Mono
    name: Mono
    fullName: reactor.core.publisher.Mono
  - name: <
    fullName: <
  - uid: com.azure.storage.file.datalake.models.FileReadAsyncResponse
    name: FileReadAsyncResponse
    fullName: com.azure.storage.file.datalake.models.FileReadAsyncResponse
  - name: '>'
    fullName: '>'
- uid: com.azure.storage.file.datalake.DataLakeFileAsyncClient.readWithResponse*
  name: readWithResponse
  nameWithType: DataLakeFileAsyncClient.readWithResponse
  fullName: com.azure.storage.file.datalake.DataLakeFileAsyncClient.readWithResponse
  package: com.azure.storage.file.datalake
- uid: reactor.core.publisher.Mono<com.azure.storage.file.datalake.models.PathProperties>
  spec.java:
  - uid: reactor.core.publisher.Mono
    name: Mono
    fullName: reactor.core.publisher.Mono
  - name: <
    fullName: <
  - uid: com.azure.storage.file.datalake.models.PathProperties
    name: PathProperties
    fullName: com.azure.storage.file.datalake.models.PathProperties
  - name: '>'
    fullName: '>'
- uid: com.azure.storage.file.datalake.DataLakeFileAsyncClient.readToFile*
  name: readToFile
  nameWithType: DataLakeFileAsyncClient.readToFile
  fullName: com.azure.storage.file.datalake.DataLakeFileAsyncClient.readToFile
  package: com.azure.storage.file.datalake
- uid: java.util.Set<java.nio.file.OpenOption>
  spec.java:
  - uid: java.util.Set
    name: Set
    fullName: java.util.Set
  - name: <
    fullName: <
  - uid: java.nio.file.OpenOption
    name: OpenOption
    fullName: java.nio.file.OpenOption
  - name: '>'
    fullName: '>'
- uid: reactor.core.publisher.Mono<com.azure.core.http.rest.Response<com.azure.storage.file.datalake.models.PathProperties>>
  spec.java:
  - uid: reactor.core.publisher.Mono
    name: Mono
    fullName: reactor.core.publisher.Mono
  - name: <
    fullName: <
  - uid: com.azure.core.http.rest.Response
    name: Response
    fullName: com.azure.core.http.rest.Response
  - name: <
    fullName: <
  - uid: com.azure.storage.file.datalake.models.PathProperties
    name: PathProperties
    fullName: com.azure.storage.file.datalake.models.PathProperties
  - name: '>'
    fullName: '>'
  - name: '>'
    fullName: '>'
- uid: com.azure.storage.file.datalake.DataLakeFileAsyncClient.readToFileWithResponse*
  name: readToFileWithResponse
  nameWithType: DataLakeFileAsyncClient.readToFileWithResponse
  fullName: com.azure.storage.file.datalake.DataLakeFileAsyncClient.readToFileWithResponse
  package: com.azure.storage.file.datalake
- uid: reactor.core.publisher.Mono<com.azure.storage.file.datalake.DataLakeFileAsyncClient>
  spec.java:
  - uid: reactor.core.publisher.Mono
    name: Mono
    fullName: reactor.core.publisher.Mono
  - name: <
    fullName: <
  - uid: com.azure.storage.file.datalake.DataLakeFileAsyncClient
    name: DataLakeFileAsyncClient
    fullName: com.azure.storage.file.datalake.DataLakeFileAsyncClient
  - name: '>'
    fullName: '>'
- uid: com.azure.storage.file.datalake.DataLakeFileAsyncClient.rename*
  name: rename
  nameWithType: DataLakeFileAsyncClient.rename
  fullName: com.azure.storage.file.datalake.DataLakeFileAsyncClient.rename
  package: com.azure.storage.file.datalake
- uid: reactor.core.publisher.Mono<com.azure.core.http.rest.Response<com.azure.storage.file.datalake.DataLakeFileAsyncClient>>
  spec.java:
  - uid: reactor.core.publisher.Mono
    name: Mono
    fullName: reactor.core.publisher.Mono
  - name: <
    fullName: <
  - uid: com.azure.core.http.rest.Response
    name: Response
    fullName: com.azure.core.http.rest.Response
  - name: <
    fullName: <
  - uid: com.azure.storage.file.datalake.DataLakeFileAsyncClient
    name: DataLakeFileAsyncClient
    fullName: com.azure.storage.file.datalake.DataLakeFileAsyncClient
  - name: '>'
    fullName: '>'
  - name: '>'
    fullName: '>'
- uid: com.azure.storage.file.datalake.DataLakeFileAsyncClient.renameWithResponse*
  name: renameWithResponse
  nameWithType: DataLakeFileAsyncClient.renameWithResponse
  fullName: com.azure.storage.file.datalake.DataLakeFileAsyncClient.renameWithResponse
  package: com.azure.storage.file.datalake
- uid: com.azure.storage.file.datalake.DataLakeFileAsyncClient.query*
  name: query
  nameWithType: DataLakeFileAsyncClient.query
  fullName: com.azure.storage.file.datalake.DataLakeFileAsyncClient.query
  package: com.azure.storage.file.datalake
- uid: com.azure.storage.file.datalake.options.FileQueryOptions
  name: FileQueryOptions
  nameWithType: FileQueryOptions
  fullName: com.azure.storage.file.datalake.options.FileQueryOptions
- uid: reactor.core.publisher.Mono<com.azure.storage.file.datalake.models.FileQueryAsyncResponse>
  spec.java:
  - uid: reactor.core.publisher.Mono
    name: Mono
    fullName: reactor.core.publisher.Mono
  - name: <
    fullName: <
  - uid: com.azure.storage.file.datalake.models.FileQueryAsyncResponse
    name: FileQueryAsyncResponse
    fullName: com.azure.storage.file.datalake.models.FileQueryAsyncResponse
  - name: '>'
    fullName: '>'
- uid: com.azure.storage.file.datalake.DataLakeFileAsyncClient.queryWithResponse*
  name: queryWithResponse
  nameWithType: DataLakeFileAsyncClient.queryWithResponse
  fullName: com.azure.storage.file.datalake.DataLakeFileAsyncClient.queryWithResponse
  package: com.azure.storage.file.datalake
- uid: com.azure.storage.file.datalake.options.FileScheduleDeletionOptions
  name: FileScheduleDeletionOptions
  nameWithType: FileScheduleDeletionOptions
  fullName: com.azure.storage.file.datalake.options.FileScheduleDeletionOptions
- uid: com.azure.storage.file.datalake.DataLakeFileAsyncClient.scheduleDeletion*
  name: scheduleDeletion
  nameWithType: DataLakeFileAsyncClient.scheduleDeletion
  fullName: com.azure.storage.file.datalake.DataLakeFileAsyncClient.scheduleDeletion
  package: com.azure.storage.file.datalake
- uid: com.azure.storage.file.datalake.DataLakeFileAsyncClient.scheduleDeletionWithResponse*
  name: scheduleDeletionWithResponse
  nameWithType: DataLakeFileAsyncClient.scheduleDeletionWithResponse
  fullName: com.azure.storage.file.datalake.DataLakeFileAsyncClient.scheduleDeletionWithResponse
  package: com.azure.storage.file.datalake
- uid: com.azure.storage.file.datalake.DataLakePathAsyncClient.setPermissionsWithResponse(com.azure.storage.file.datalake.models.PathPermissions,java.lang.String,java.lang.String,com.azure.storage.file.datalake.models.DataLakeRequestConditions)
  name: DataLakePathAsyncClient.setPermissionsWithResponse(PathPermissions,String,String,DataLakeRequestConditions)
  nameWithType: DataLakePathAsyncClient.setPermissionsWithResponse(PathPermissions,String,String,DataLakeRequestConditions)
  fullName: com.azure.storage.file.datalake.DataLakePathAsyncClient.setPermissionsWithResponse(com.azure.storage.file.datalake.models.PathPermissions,java.lang.String,java.lang.String,com.azure.storage.file.datalake.models.DataLakeRequestConditions)
- uid: com.azure.storage.file.datalake.DataLakePathAsyncClient.updateAccessControlRecursiveWithResponse(com.azure.storage.file.datalake.options.PathUpdateAccessControlRecursiveOptions)
  name: DataLakePathAsyncClient.updateAccessControlRecursiveWithResponse(PathUpdateAccessControlRecursiveOptions)
  nameWithType: DataLakePathAsyncClient.updateAccessControlRecursiveWithResponse(PathUpdateAccessControlRecursiveOptions)
  fullName: com.azure.storage.file.datalake.DataLakePathAsyncClient.updateAccessControlRecursiveWithResponse(com.azure.storage.file.datalake.options.PathUpdateAccessControlRecursiveOptions)
- uid: java.lang.Object.wait()
  name: Object.wait()
  nameWithType: Object.wait()
  fullName: java.lang.Object.wait()
- uid: java.lang.Object.finalize()
  name: Object.finalize()
  nameWithType: Object.finalize()
  fullName: java.lang.Object.finalize()
- uid: com.azure.storage.file.datalake.DataLakePathAsyncClient.setPermissions(com.azure.storage.file.datalake.models.PathPermissions,java.lang.String,java.lang.String)
  name: DataLakePathAsyncClient.setPermissions(PathPermissions,String,String)
  nameWithType: DataLakePathAsyncClient.setPermissions(PathPermissions,String,String)
  fullName: com.azure.storage.file.datalake.DataLakePathAsyncClient.setPermissions(com.azure.storage.file.datalake.models.PathPermissions,java.lang.String,java.lang.String)
- uid: java.lang.Object.clone()
  name: Object.clone()
  nameWithType: Object.clone()
  fullName: java.lang.Object.clone()
- uid: com.azure.storage.file.datalake.DataLakePathAsyncClient.existsWithResponse()
  name: DataLakePathAsyncClient.existsWithResponse()
  nameWithType: DataLakePathAsyncClient.existsWithResponse()
  fullName: com.azure.storage.file.datalake.DataLakePathAsyncClient.existsWithResponse()
- uid: com.azure.storage.file.datalake.DataLakePathAsyncClient.generateUserDelegationSas(com.azure.storage.file.datalake.sas.DataLakeServiceSasSignatureValues,com.azure.storage.file.datalake.models.UserDelegationKey)
  name: DataLakePathAsyncClient.generateUserDelegationSas(DataLakeServiceSasSignatureValues,UserDelegationKey)
  nameWithType: DataLakePathAsyncClient.generateUserDelegationSas(DataLakeServiceSasSignatureValues,UserDelegationKey)
  fullName: com.azure.storage.file.datalake.DataLakePathAsyncClient.generateUserDelegationSas(com.azure.storage.file.datalake.sas.DataLakeServiceSasSignatureValues,com.azure.storage.file.datalake.models.UserDelegationKey)
- uid: com.azure.storage.file.datalake.DataLakePathAsyncClient.getServiceVersion()
  name: DataLakePathAsyncClient.getServiceVersion()
  nameWithType: DataLakePathAsyncClient.getServiceVersion()
  fullName: com.azure.storage.file.datalake.DataLakePathAsyncClient.getServiceVersion()
- uid: java.lang.Object.wait(long)
  name: Object.wait(long)
  nameWithType: Object.wait(long)
  fullName: java.lang.Object.wait(long)
- uid: com.azure.storage.file.datalake.DataLakePathAsyncClient.exists()
  name: DataLakePathAsyncClient.exists()
  nameWithType: DataLakePathAsyncClient.exists()
  fullName: com.azure.storage.file.datalake.DataLakePathAsyncClient.exists()
- uid: java.lang.Object.getClass()
  name: Object.getClass()
  nameWithType: Object.getClass()
  fullName: java.lang.Object.getClass()
- uid: com.azure.storage.file.datalake.DataLakePathAsyncClient.setHttpHeaders(com.azure.storage.file.datalake.models.PathHttpHeaders)
  name: DataLakePathAsyncClient.setHttpHeaders(PathHttpHeaders)
  nameWithType: DataLakePathAsyncClient.setHttpHeaders(PathHttpHeaders)
  fullName: com.azure.storage.file.datalake.DataLakePathAsyncClient.setHttpHeaders(com.azure.storage.file.datalake.models.PathHttpHeaders)
- uid: com.azure.storage.file.datalake.DataLakePathAsyncClient.getFileSystemName()
  name: DataLakePathAsyncClient.getFileSystemName()
  nameWithType: DataLakePathAsyncClient.getFileSystemName()
  fullName: com.azure.storage.file.datalake.DataLakePathAsyncClient.getFileSystemName()
- uid: com.azure.storage.file.datalake.DataLakePathAsyncClient.removeAccessControlRecursive(java.util.List<com.azure.storage.file.datalake.models.PathRemoveAccessControlEntry>)
  name: DataLakePathAsyncClient.removeAccessControlRecursive(List<PathRemoveAccessControlEntry>)
  nameWithType: DataLakePathAsyncClient.removeAccessControlRecursive(List<PathRemoveAccessControlEntry>)
  fullName: com.azure.storage.file.datalake.DataLakePathAsyncClient.removeAccessControlRecursive(java.util.List<com.azure.storage.file.datalake.models.PathRemoveAccessControlEntry>)
- uid: java.lang.Object.hashCode()
  name: Object.hashCode()
  nameWithType: Object.hashCode()
  fullName: java.lang.Object.hashCode()
- uid: com.azure.storage.file.datalake.DataLakePathAsyncClient.setAccessControlRecursiveWithResponse(com.azure.storage.file.datalake.options.PathSetAccessControlRecursiveOptions)
  name: DataLakePathAsyncClient.setAccessControlRecursiveWithResponse(PathSetAccessControlRecursiveOptions)
  nameWithType: DataLakePathAsyncClient.setAccessControlRecursiveWithResponse(PathSetAccessControlRecursiveOptions)
  fullName: com.azure.storage.file.datalake.DataLakePathAsyncClient.setAccessControlRecursiveWithResponse(com.azure.storage.file.datalake.options.PathSetAccessControlRecursiveOptions)
- uid: java.lang.Object.wait(long,int)
  name: Object.wait(long,int)
  nameWithType: Object.wait(long,int)
  fullName: java.lang.Object.wait(long,int)
- uid: com.azure.storage.file.datalake.DataLakePathAsyncClient.setAccessControlRecursive(java.util.List<com.azure.storage.file.datalake.models.PathAccessControlEntry>)
  name: DataLakePathAsyncClient.setAccessControlRecursive(List<PathAccessControlEntry>)
  nameWithType: DataLakePathAsyncClient.setAccessControlRecursive(List<PathAccessControlEntry>)
  fullName: com.azure.storage.file.datalake.DataLakePathAsyncClient.setAccessControlRecursive(java.util.List<com.azure.storage.file.datalake.models.PathAccessControlEntry>)
- uid: com.azure.storage.file.datalake.DataLakePathAsyncClient.removeAccessControlRecursiveWithResponse(com.azure.storage.file.datalake.options.PathRemoveAccessControlRecursiveOptions)
  name: DataLakePathAsyncClient.removeAccessControlRecursiveWithResponse(PathRemoveAccessControlRecursiveOptions)
  nameWithType: DataLakePathAsyncClient.removeAccessControlRecursiveWithResponse(PathRemoveAccessControlRecursiveOptions)
  fullName: com.azure.storage.file.datalake.DataLakePathAsyncClient.removeAccessControlRecursiveWithResponse(com.azure.storage.file.datalake.options.PathRemoveAccessControlRecursiveOptions)
- uid: com.azure.storage.file.datalake.DataLakePathAsyncClient.updateAccessControlRecursive(java.util.List<com.azure.storage.file.datalake.models.PathAccessControlEntry>)
  name: DataLakePathAsyncClient.updateAccessControlRecursive(List<PathAccessControlEntry>)
  nameWithType: DataLakePathAsyncClient.updateAccessControlRecursive(List<PathAccessControlEntry>)
  fullName: com.azure.storage.file.datalake.DataLakePathAsyncClient.updateAccessControlRecursive(java.util.List<com.azure.storage.file.datalake.models.PathAccessControlEntry>)
- uid: com.azure.storage.file.datalake.DataLakePathAsyncClient.generateSas(com.azure.storage.file.datalake.sas.DataLakeServiceSasSignatureValues)
  name: DataLakePathAsyncClient.generateSas(DataLakeServiceSasSignatureValues)
  nameWithType: DataLakePathAsyncClient.generateSas(DataLakeServiceSasSignatureValues)
  fullName: com.azure.storage.file.datalake.DataLakePathAsyncClient.generateSas(com.azure.storage.file.datalake.sas.DataLakeServiceSasSignatureValues)
- uid: com.azure.storage.file.datalake.DataLakePathAsyncClient.setAccessControlListWithResponse(java.util.List<com.azure.storage.file.datalake.models.PathAccessControlEntry>,java.lang.String,java.lang.String,com.azure.storage.file.datalake.models.DataLakeRequestConditions)
  name: DataLakePathAsyncClient.setAccessControlListWithResponse(List<PathAccessControlEntry>,String,String,DataLakeRequestConditions)
  nameWithType: DataLakePathAsyncClient.setAccessControlListWithResponse(List<PathAccessControlEntry>,String,String,DataLakeRequestConditions)
  fullName: com.azure.storage.file.datalake.DataLakePathAsyncClient.setAccessControlListWithResponse(java.util.List<com.azure.storage.file.datalake.models.PathAccessControlEntry>,java.lang.String,java.lang.String,com.azure.storage.file.datalake.models.DataLakeRequestConditions)
- uid: java.lang.Object.notify()
  name: Object.notify()
  nameWithType: Object.notify()
  fullName: java.lang.Object.notify()
- uid: java.lang.Object.notifyAll()
  name: Object.notifyAll()
  nameWithType: Object.notifyAll()
  fullName: java.lang.Object.notifyAll()
- uid: com.azure.storage.file.datalake.DataLakePathAsyncClient.create(boolean)
  name: DataLakePathAsyncClient.create(boolean)
  nameWithType: DataLakePathAsyncClient.create(boolean)
  fullName: com.azure.storage.file.datalake.DataLakePathAsyncClient.create(boolean)
- uid: com.azure.storage.file.datalake.DataLakePathAsyncClient.getAccessControl()
  name: DataLakePathAsyncClient.getAccessControl()
  nameWithType: DataLakePathAsyncClient.getAccessControl()
  fullName: com.azure.storage.file.datalake.DataLakePathAsyncClient.getAccessControl()
- uid: com.azure.storage.file.datalake.DataLakePathAsyncClient.getAccountName()
  name: DataLakePathAsyncClient.getAccountName()
  nameWithType: DataLakePathAsyncClient.getAccountName()
  fullName: com.azure.storage.file.datalake.DataLakePathAsyncClient.getAccountName()
- uid: java.lang.Object.equals(java.lang.Object)
  name: Object.equals(Object)
  nameWithType: Object.equals(Object)
  fullName: java.lang.Object.equals(java.lang.Object)
- uid: com.azure.storage.file.datalake.DataLakePathAsyncClient.getAccessControlWithResponse(boolean,com.azure.storage.file.datalake.models.DataLakeRequestConditions)
  name: DataLakePathAsyncClient.getAccessControlWithResponse(boolean,DataLakeRequestConditions)
  nameWithType: DataLakePathAsyncClient.getAccessControlWithResponse(boolean,DataLakeRequestConditions)
  fullName: com.azure.storage.file.datalake.DataLakePathAsyncClient.getAccessControlWithResponse(boolean,com.azure.storage.file.datalake.models.DataLakeRequestConditions)
- uid: com.azure.storage.file.datalake.DataLakePathAsyncClient.setHttpHeadersWithResponse(com.azure.storage.file.datalake.models.PathHttpHeaders,com.azure.storage.file.datalake.models.DataLakeRequestConditions)
  name: DataLakePathAsyncClient.setHttpHeadersWithResponse(PathHttpHeaders,DataLakeRequestConditions)
  nameWithType: DataLakePathAsyncClient.setHttpHeadersWithResponse(PathHttpHeaders,DataLakeRequestConditions)
  fullName: com.azure.storage.file.datalake.DataLakePathAsyncClient.setHttpHeadersWithResponse(com.azure.storage.file.datalake.models.PathHttpHeaders,com.azure.storage.file.datalake.models.DataLakeRequestConditions)
- uid: java.lang.Object.toString()
  name: Object.toString()
  nameWithType: Object.toString()
  fullName: java.lang.Object.toString()
- uid: com.azure.storage.file.datalake.DataLakePathAsyncClient.createWithResponse(java.lang.String,java.lang.String,com.azure.storage.file.datalake.models.PathHttpHeaders,java.util.Map<java.lang.String,java.lang.String>,com.azure.storage.file.datalake.models.DataLakeRequestConditions)
  name: DataLakePathAsyncClient.createWithResponse(String,String,PathHttpHeaders,Map<String,String>,DataLakeRequestConditions)
  nameWithType: DataLakePathAsyncClient.createWithResponse(String,String,PathHttpHeaders,Map<String,String>,DataLakeRequestConditions)
  fullName: com.azure.storage.file.datalake.DataLakePathAsyncClient.createWithResponse(java.lang.String,java.lang.String,com.azure.storage.file.datalake.models.PathHttpHeaders,java.util.Map<java.lang.String,java.lang.String>,com.azure.storage.file.datalake.models.DataLakeRequestConditions)
- uid: com.azure.storage.file.datalake.DataLakePathAsyncClient.getPropertiesWithResponse(com.azure.storage.file.datalake.models.DataLakeRequestConditions)
  name: DataLakePathAsyncClient.getPropertiesWithResponse(DataLakeRequestConditions)
  nameWithType: DataLakePathAsyncClient.getPropertiesWithResponse(DataLakeRequestConditions)
  fullName: com.azure.storage.file.datalake.DataLakePathAsyncClient.getPropertiesWithResponse(com.azure.storage.file.datalake.models.DataLakeRequestConditions)
- uid: com.azure.storage.file.datalake.DataLakePathAsyncClient.setMetadataWithResponse(java.util.Map<java.lang.String,java.lang.String>,com.azure.storage.file.datalake.models.DataLakeRequestConditions)
  name: DataLakePathAsyncClient.setMetadataWithResponse(Map<String,String>,DataLakeRequestConditions)
  nameWithType: DataLakePathAsyncClient.setMetadataWithResponse(Map<String,String>,DataLakeRequestConditions)
  fullName: com.azure.storage.file.datalake.DataLakePathAsyncClient.setMetadataWithResponse(java.util.Map<java.lang.String,java.lang.String>,com.azure.storage.file.datalake.models.DataLakeRequestConditions)
- uid: com.azure.storage.file.datalake.DataLakePathAsyncClient.setMetadata(java.util.Map<java.lang.String,java.lang.String>)
  name: DataLakePathAsyncClient.setMetadata(Map<String,String>)
  nameWithType: DataLakePathAsyncClient.setMetadata(Map<String,String>)
  fullName: com.azure.storage.file.datalake.DataLakePathAsyncClient.setMetadata(java.util.Map<java.lang.String,java.lang.String>)
- uid: com.azure.storage.file.datalake.DataLakePathAsyncClient.getProperties()
  name: DataLakePathAsyncClient.getProperties()
  nameWithType: DataLakePathAsyncClient.getProperties()
  fullName: com.azure.storage.file.datalake.DataLakePathAsyncClient.getProperties()
- uid: com.azure.storage.file.datalake.DataLakePathAsyncClient.getHttpPipeline()
  name: DataLakePathAsyncClient.getHttpPipeline()
  nameWithType: DataLakePathAsyncClient.getHttpPipeline()
  fullName: com.azure.storage.file.datalake.DataLakePathAsyncClient.getHttpPipeline()
- uid: com.azure.storage.file.datalake.DataLakePathAsyncClient.setAccessControlList(java.util.List<com.azure.storage.file.datalake.models.PathAccessControlEntry>,java.lang.String,java.lang.String)
  name: DataLakePathAsyncClient.setAccessControlList(List<PathAccessControlEntry>,String,String)
  nameWithType: DataLakePathAsyncClient.setAccessControlList(List<PathAccessControlEntry>,String,String)
  fullName: com.azure.storage.file.datalake.DataLakePathAsyncClient.setAccessControlList(java.util.List<com.azure.storage.file.datalake.models.PathAccessControlEntry>,java.lang.String,java.lang.String)
- uid: com.azure.storage.file.datalake.DataLakePathAsyncClient.create()
  name: DataLakePathAsyncClient.create()
  nameWithType: DataLakePathAsyncClient.create()
  fullName: com.azure.storage.file.datalake.DataLakePathAsyncClient.create()
- uid: java.lang.Void
  name: Void
  nameWithType: Void
  fullName: java.lang.Void
- uid: reactor.core.publisher.Mono
  name: Mono
  nameWithType: Mono
  fullName: reactor.core.publisher.Mono
- uid: com.azure.core.http.rest.Response
  name: Response
  nameWithType: Response
  fullName: com.azure.core.http.rest.Response
- uid: reactor.core.publisher.Flux
  name: Flux
  nameWithType: Flux
  fullName: reactor.core.publisher.Flux
- uid: java.nio.ByteBuffer
  name: ByteBuffer
  nameWithType: ByteBuffer
  fullName: java.nio.ByteBuffer
- uid: com.azure.storage.file.datalake.models.PathInfo
  name: PathInfo
  nameWithType: PathInfo
  fullName: com.azure.storage.file.datalake.models.PathInfo
- uid: java.util.Map
  name: Map
  nameWithType: Map
  fullName: java.util.Map
- uid: java.lang.String,java.lang.String
  name: String,String
  nameWithType: String,String
  fullName: java.lang.String,java.lang.String
- uid: com.azure.storage.file.datalake.models.FileReadAsyncResponse
  name: FileReadAsyncResponse
  nameWithType: FileReadAsyncResponse
  fullName: com.azure.storage.file.datalake.models.FileReadAsyncResponse
- uid: com.azure.storage.file.datalake.models.PathProperties
  name: PathProperties
  nameWithType: PathProperties
  fullName: com.azure.storage.file.datalake.models.PathProperties
- uid: java.nio.file.OpenOption
  name: OpenOption
  nameWithType: OpenOption
  fullName: java.nio.file.OpenOption
- uid: java.util.Set
  name: Set
  nameWithType: Set
  fullName: java.util.Set
- uid: com.azure.storage.file.datalake.models.FileQueryAsyncResponse
  name: FileQueryAsyncResponse
  nameWithType: FileQueryAsyncResponse
  fullName: com.azure.storage.file.datalake.models.FileQueryAsyncResponse
- uid: com.azure.storage.file.datalake.DataLakePathAsyncClient.removeAccessControlRecursive(java.util.List
  name: DataLakePathAsyncClient.removeAccessControlRecursive(List
  nameWithType: DataLakePathAsyncClient.removeAccessControlRecursive(List
  fullName: com.azure.storage.file.datalake.DataLakePathAsyncClient.removeAccessControlRecursive(java.util.List
- uid: com.azure.storage.file.datalake.models.PathRemoveAccessControlEntry>)
  name: PathRemoveAccessControlEntry>)
  nameWithType: PathRemoveAccessControlEntry>)
  fullName: com.azure.storage.file.datalake.models.PathRemoveAccessControlEntry>)
- uid: com.azure.storage.file.datalake.DataLakePathAsyncClient.setAccessControlRecursive(java.util.List
  name: DataLakePathAsyncClient.setAccessControlRecursive(List
  nameWithType: DataLakePathAsyncClient.setAccessControlRecursive(List
  fullName: com.azure.storage.file.datalake.DataLakePathAsyncClient.setAccessControlRecursive(java.util.List
- uid: com.azure.storage.file.datalake.models.PathAccessControlEntry>)
  name: PathAccessControlEntry>)
  nameWithType: PathAccessControlEntry>)
  fullName: com.azure.storage.file.datalake.models.PathAccessControlEntry>)
- uid: com.azure.storage.file.datalake.DataLakePathAsyncClient.updateAccessControlRecursive(java.util.List
  name: DataLakePathAsyncClient.updateAccessControlRecursive(List
  nameWithType: DataLakePathAsyncClient.updateAccessControlRecursive(List
  fullName: com.azure.storage.file.datalake.DataLakePathAsyncClient.updateAccessControlRecursive(java.util.List
- uid: com.azure.storage.file.datalake.models.PathAccessControlEntry>,java.lang.String,java.lang.String,com.azure.storage.file.datalake.models.DataLakeRequestConditions)
  name: PathAccessControlEntry>,String,String,DataLakeRequestConditions)
  nameWithType: PathAccessControlEntry>,String,String,DataLakeRequestConditions)
  fullName: com.azure.storage.file.datalake.models.PathAccessControlEntry>,java.lang.String,java.lang.String,com.azure.storage.file.datalake.models.DataLakeRequestConditions)
- uid: com.azure.storage.file.datalake.DataLakePathAsyncClient.setAccessControlListWithResponse(java.util.List
  name: DataLakePathAsyncClient.setAccessControlListWithResponse(List
  nameWithType: DataLakePathAsyncClient.setAccessControlListWithResponse(List
  fullName: com.azure.storage.file.datalake.DataLakePathAsyncClient.setAccessControlListWithResponse(java.util.List
- uid: java.lang.String,java.lang.String>,com.azure.storage.file.datalake.models.DataLakeRequestConditions)
  name: String,String>,DataLakeRequestConditions)
  nameWithType: String,String>,DataLakeRequestConditions)
  fullName: java.lang.String,java.lang.String>,com.azure.storage.file.datalake.models.DataLakeRequestConditions)
- uid: com.azure.storage.file.datalake.DataLakePathAsyncClient.createWithResponse(java.lang.String,java.lang.String,com.azure.storage.file.datalake.models.PathHttpHeaders,java.util.Map
  name: DataLakePathAsyncClient.createWithResponse(String,String,PathHttpHeaders,Map
  nameWithType: DataLakePathAsyncClient.createWithResponse(String,String,PathHttpHeaders,Map
  fullName: com.azure.storage.file.datalake.DataLakePathAsyncClient.createWithResponse(java.lang.String,java.lang.String,com.azure.storage.file.datalake.models.PathHttpHeaders,java.util.Map
- uid: com.azure.storage.file.datalake.DataLakePathAsyncClient.setMetadataWithResponse(java.util.Map
  name: DataLakePathAsyncClient.setMetadataWithResponse(Map
  nameWithType: DataLakePathAsyncClient.setMetadataWithResponse(Map
  fullName: com.azure.storage.file.datalake.DataLakePathAsyncClient.setMetadataWithResponse(java.util.Map
- uid: java.lang.String,java.lang.String>)
  name: String,String>)
  nameWithType: String,String>)
  fullName: java.lang.String,java.lang.String>)
- uid: com.azure.storage.file.datalake.DataLakePathAsyncClient.setMetadata(java.util.Map
  name: DataLakePathAsyncClient.setMetadata(Map
  nameWithType: DataLakePathAsyncClient.setMetadata(Map
  fullName: com.azure.storage.file.datalake.DataLakePathAsyncClient.setMetadata(java.util.Map
- uid: com.azure.storage.file.datalake.DataLakePathAsyncClient.setAccessControlList(java.util.List
  name: DataLakePathAsyncClient.setAccessControlList(List
  nameWithType: DataLakePathAsyncClient.setAccessControlList(List
  fullName: com.azure.storage.file.datalake.DataLakePathAsyncClient.setAccessControlList(java.util.List
- uid: com.azure.storage.file.datalake.models.PathAccessControlEntry>,java.lang.String,java.lang.String)
  name: PathAccessControlEntry>,String,String)
  nameWithType: PathAccessControlEntry>,String,String)
  fullName: com.azure.storage.file.datalake.models.PathAccessControlEntry>,java.lang.String,java.lang.String)
