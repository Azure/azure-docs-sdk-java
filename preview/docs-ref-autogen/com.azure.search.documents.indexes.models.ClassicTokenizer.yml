### YamlMime:JavaType
uid: "com.azure.search.documents.indexes.models.ClassicTokenizer"
fullName: "com.azure.search.documents.indexes.models.ClassicTokenizer"
name: "ClassicTokenizer"
nameWithType: "ClassicTokenizer"
summary: "Grammar-based tokenizer that is suitable for processing most European-language documents."
inheritances:
- "<xref href=\"java.lang.Object?displayProperty=fullName\" data-throw-if-not-resolved=\"False\" />"
- "<xref href=\"com.azure.search.documents.indexes.models.LexicalTokenizer?displayProperty=fullName\" data-throw-if-not-resolved=\"False\" />"
inheritedMembers:
- "com.azure.search.documents.indexes.models.LexicalTokenizer.getName()"
- "java.lang.Object.clone()"
- "java.lang.Object.equals(java.lang.Object)"
- "java.lang.Object.finalize()"
- "java.lang.Object.getClass()"
- "java.lang.Object.hashCode()"
- "java.lang.Object.notify()"
- "java.lang.Object.notifyAll()"
- "java.lang.Object.toString()"
- "java.lang.Object.wait()"
- "java.lang.Object.wait(long)"
- "java.lang.Object.wait(long,int)"
syntax: "public final class ClassicTokenizer extends LexicalTokenizer"
constructors:
- uid: "com.azure.search.documents.indexes.models.ClassicTokenizer.ClassicTokenizer(java.lang.String)"
  fullName: "com.azure.search.documents.indexes.models.ClassicTokenizer.ClassicTokenizer(String name)"
  name: "ClassicTokenizer(String name)"
  nameWithType: "ClassicTokenizer.ClassicTokenizer(String name)"
  summary: "Constructor of <xref uid=\"com.azure.search.documents.indexes.models.ClassicTokenizer\" data-throw-if-not-resolved=\"false\" data-raw-source=\"ClassicTokenizer\"></xref>."
  parameters:
  - description: "The name of the token filter. It must only contain letters, digits,\n spaces, dashes or underscores, can only start and end with alphanumeric\n characters, and is limited to 128 characters."
    name: "name"
    type: "<xref href=\"java.lang.String?alt=java.lang.String&text=String\" data-throw-if-not-resolved=\"False\" />"
  syntax: "public ClassicTokenizer(String name)"
  desc: "Constructor of <xref uid=\"com.azure.search.documents.indexes.models.ClassicTokenizer\" data-throw-if-not-resolved=\"false\" data-raw-source=\"ClassicTokenizer\"></xref>."
methods:
- uid: "com.azure.search.documents.indexes.models.ClassicTokenizer.getMaxTokenLength()"
  fullName: "com.azure.search.documents.indexes.models.ClassicTokenizer.getMaxTokenLength()"
  name: "getMaxTokenLength()"
  nameWithType: "ClassicTokenizer.getMaxTokenLength()"
  summary: "Get the max<wbr>Token<wbr>Length property: The maximum token length."
  syntax: "public Integer getMaxTokenLength()"
  desc: "Get the maxTokenLength property: The maximum token length. Default is 255. Tokens longer than the maximum length are split. The maximum token length that can be used is 300 characters."
  returns:
    description: "the maxTokenLength value."
    type: "<xref href=\"java.lang.Integer?alt=java.lang.Integer&text=Integer\" data-throw-if-not-resolved=\"False\" />"
- uid: "com.azure.search.documents.indexes.models.ClassicTokenizer.setMaxTokenLength(java.lang.Integer)"
  fullName: "com.azure.search.documents.indexes.models.ClassicTokenizer.setMaxTokenLength(Integer maxTokenLength)"
  name: "setMaxTokenLength(Integer maxTokenLength)"
  nameWithType: "ClassicTokenizer.setMaxTokenLength(Integer maxTokenLength)"
  summary: "Set the max<wbr>Token<wbr>Length property: The maximum token length."
  parameters:
  - description: "the maxTokenLength value to set."
    name: "maxTokenLength"
    type: "<xref href=\"java.lang.Integer?alt=java.lang.Integer&text=Integer\" data-throw-if-not-resolved=\"False\" />"
  syntax: "public ClassicTokenizer setMaxTokenLength(Integer maxTokenLength)"
  desc: "Set the maxTokenLength property: The maximum token length. Default is 255. Tokens longer than the maximum length are split. The maximum token length that can be used is 300 characters."
  returns:
    description: "the ClassicTokenizer object itself."
    type: "<xref href=\"com.azure.search.documents.indexes.models.ClassicTokenizer?alt=com.azure.search.documents.indexes.models.ClassicTokenizer&text=ClassicTokenizer\" data-throw-if-not-resolved=\"False\" />"
type: "class"
desc: "Grammar-based tokenizer that is suitable for processing most European-language documents. This tokenizer is implemented using Apache Lucene."
metadata: {}
package: "com.azure.search.documents.indexes.models"
artifact: com.azure:azure-search-documents:11.5.0-beta.12
