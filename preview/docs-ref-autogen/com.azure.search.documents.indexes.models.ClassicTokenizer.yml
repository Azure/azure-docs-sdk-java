### YamlMime:JavaType
uid: "com.azure.search.documents.indexes.models.ClassicTokenizer"
fullName: "com.azure.search.documents.indexes.models.ClassicTokenizer"
name: "ClassicTokenizer"
nameWithType: "ClassicTokenizer"
summary: "Grammar-based tokenizer that is suitable for processing most European-language documents."
inheritances:
- "<xref href=\"java.lang.Object?displayProperty=fullName\" data-throw-if-not-resolved=\"False\" />"
- "<xref href=\"com.azure.search.documents.indexes.models.LexicalTokenizer?displayProperty=fullName\" data-throw-if-not-resolved=\"False\" />"
inheritedClassMethods:
- classRef: "<xref href=\"com.azure.search.documents.indexes.models.LexicalTokenizer?alt=com.azure.search.documents.indexes.models.LexicalTokenizer&text=LexicalTokenizer\" data-throw-if-not-resolved=\"False\" />"
  methodsRef:
  - "<xref href=\"com.azure.search.documents.indexes.models.LexicalTokenizer.fromJson(com.azure.json.JsonReader)?alt=com.azure.search.documents.indexes.models.LexicalTokenizer.fromJson&text=fromJson\" data-throw-if-not-resolved=\"False\" />"
  - "<xref href=\"com.azure.search.documents.indexes.models.LexicalTokenizer.getName()?alt=com.azure.search.documents.indexes.models.LexicalTokenizer.getName&text=getName\" data-throw-if-not-resolved=\"False\" />"
  - "<xref href=\"com.azure.search.documents.indexes.models.LexicalTokenizer.getOdataType()?alt=com.azure.search.documents.indexes.models.LexicalTokenizer.getOdataType&text=getOdataType\" data-throw-if-not-resolved=\"False\" />"
  - "<xref href=\"com.azure.search.documents.indexes.models.LexicalTokenizer.toJson(com.azure.json.JsonWriter)?alt=com.azure.search.documents.indexes.models.LexicalTokenizer.toJson&text=toJson\" data-throw-if-not-resolved=\"False\" />"
- classRef: "java.lang.<a href=\"https://docs.oracle.com/javase/8/docs/api/java/lang/Object.html\">Object</a>"
  methodsRef:
  - "<a href=\"https://docs.oracle.com/javase/8/docs/api/java/lang/Object.html#clone--\">clone</a>"
  - "<a href=\"https://docs.oracle.com/javase/8/docs/api/java/lang/Object.html#equals-java.lang.Object-\">equals</a>"
  - "<a href=\"https://docs.oracle.com/javase/8/docs/api/java/lang/Object.html#finalize--\">finalize</a>"
  - "<a href=\"https://docs.oracle.com/javase/8/docs/api/java/lang/Object.html#getClass--\">getClass</a>"
  - "<a href=\"https://docs.oracle.com/javase/8/docs/api/java/lang/Object.html#hashCode--\">hashCode</a>"
  - "<a href=\"https://docs.oracle.com/javase/8/docs/api/java/lang/Object.html#notify--\">notify</a>"
  - "<a href=\"https://docs.oracle.com/javase/8/docs/api/java/lang/Object.html#notifyAll--\">notifyAll</a>"
  - "<a href=\"https://docs.oracle.com/javase/8/docs/api/java/lang/Object.html#toString--\">toString</a>"
  - "<a href=\"https://docs.oracle.com/javase/8/docs/api/java/lang/Object.html#wait--\">wait</a>"
  - "<a href=\"https://docs.oracle.com/javase/8/docs/api/java/lang/Object.html#wait-long-\">wait</a>"
  - "<a href=\"https://docs.oracle.com/javase/8/docs/api/java/lang/Object.html#wait-long-int-\">wait</a>"
syntax: "public final class **ClassicTokenizer**</br> extends <xref href=\"com.azure.search.documents.indexes.models.LexicalTokenizer?alt=com.azure.search.documents.indexes.models.LexicalTokenizer&text=LexicalTokenizer\" data-throw-if-not-resolved=\"False\" />"
constructors:
- uid: "com.azure.search.documents.indexes.models.ClassicTokenizer.ClassicTokenizer(java.lang.String)"
  fullName: "com.azure.search.documents.indexes.models.ClassicTokenizer.ClassicTokenizer(String name)"
  name: "ClassicTokenizer(String name)"
  nameWithType: "ClassicTokenizer.ClassicTokenizer(String name)"
  summary: "Creates an instance of Classic<wbr>Tokenizer class."
  parameters:
  - description: "the name value to set."
    name: "name"
    type: "<a href=\"https://docs.oracle.com/javase/8/docs/api/java/lang/String.html\">String</a>"
  syntax: "public ClassicTokenizer(String name)"
  desc: "Creates an instance of ClassicTokenizer class."
methods:
- uid: "com.azure.search.documents.indexes.models.ClassicTokenizer.fromJson(com.azure.json.JsonReader)"
  fullName: "com.azure.search.documents.indexes.models.ClassicTokenizer.fromJson(JsonReader jsonReader)"
  name: "fromJson(JsonReader jsonReader)"
  nameWithType: "ClassicTokenizer.fromJson(JsonReader jsonReader)"
  summary: "Reads an instance of Classic<wbr>Tokenizer from the Json<wbr>Reader."
  modifiers:
  - "static"
  parameters:
  - description: "The JsonReader being read."
    name: "jsonReader"
    type: "<xref href=\"com.azure.json.JsonReader?alt=com.azure.json.JsonReader&text=JsonReader\" data-throw-if-not-resolved=\"False\" />"
  syntax: "public static ClassicTokenizer fromJson(JsonReader jsonReader)"
  exceptions:
  - description: "If the deserialized JSON object was missing any required properties."
    type: "<a href=\"https://docs.oracle.com/javase/8/docs/api/java/io/IOException.html\">IOException</a>"
  desc: "Reads an instance of ClassicTokenizer from the JsonReader."
  returns:
    description: "An instance of ClassicTokenizer if the JsonReader was pointing to an instance of it, or null if it was\n pointing to JSON null."
    type: "<xref href=\"com.azure.search.documents.indexes.models.ClassicTokenizer?alt=com.azure.search.documents.indexes.models.ClassicTokenizer&text=ClassicTokenizer\" data-throw-if-not-resolved=\"False\" />"
- uid: "com.azure.search.documents.indexes.models.ClassicTokenizer.getMaxTokenLength()"
  fullName: "com.azure.search.documents.indexes.models.ClassicTokenizer.getMaxTokenLength()"
  name: "getMaxTokenLength()"
  nameWithType: "ClassicTokenizer.getMaxTokenLength()"
  summary: "Get the max<wbr>Token<wbr>Length property: The maximum token length."
  syntax: "public Integer getMaxTokenLength()"
  desc: "Get the maxTokenLength property: The maximum token length. Default is 255. Tokens longer than the maximum length are split. The maximum token length that can be used is 300 characters."
  returns:
    description: "the maxTokenLength value."
    type: "<a href=\"https://docs.oracle.com/javase/8/docs/api/java/lang/Integer.html\">Integer</a>"
- uid: "com.azure.search.documents.indexes.models.ClassicTokenizer.getOdataType()"
  fullName: "com.azure.search.documents.indexes.models.ClassicTokenizer.getOdataType()"
  name: "getOdataType()"
  nameWithType: "ClassicTokenizer.getOdataType()"
  summary: "Get the odata<wbr>Type property: A URI fragment specifying the type of tokenizer."
  overridden: "com.azure.search.documents.indexes.models.LexicalTokenizer.getOdataType()"
  syntax: "public String getOdataType()"
  desc: "Get the odataType property: A URI fragment specifying the type of tokenizer."
  returns:
    description: "the odataType value."
    type: "<a href=\"https://docs.oracle.com/javase/8/docs/api/java/lang/String.html\">String</a>"
- uid: "com.azure.search.documents.indexes.models.ClassicTokenizer.setMaxTokenLength(java.lang.Integer)"
  fullName: "com.azure.search.documents.indexes.models.ClassicTokenizer.setMaxTokenLength(Integer maxTokenLength)"
  name: "setMaxTokenLength(Integer maxTokenLength)"
  nameWithType: "ClassicTokenizer.setMaxTokenLength(Integer maxTokenLength)"
  summary: "Set the max<wbr>Token<wbr>Length property: The maximum token length."
  parameters:
  - description: "the maxTokenLength value to set."
    name: "maxTokenLength"
    type: "<a href=\"https://docs.oracle.com/javase/8/docs/api/java/lang/Integer.html\">Integer</a>"
  syntax: "public ClassicTokenizer setMaxTokenLength(Integer maxTokenLength)"
  desc: "Set the maxTokenLength property: The maximum token length. Default is 255. Tokens longer than the maximum length are split. The maximum token length that can be used is 300 characters."
  returns:
    description: "the ClassicTokenizer object itself."
    type: "<xref href=\"com.azure.search.documents.indexes.models.ClassicTokenizer?alt=com.azure.search.documents.indexes.models.ClassicTokenizer&text=ClassicTokenizer\" data-throw-if-not-resolved=\"False\" />"
- uid: "com.azure.search.documents.indexes.models.ClassicTokenizer.toJson(com.azure.json.JsonWriter)"
  fullName: "com.azure.search.documents.indexes.models.ClassicTokenizer.toJson(JsonWriter jsonWriter)"
  name: "toJson(JsonWriter jsonWriter)"
  nameWithType: "ClassicTokenizer.toJson(JsonWriter jsonWriter)"
  overridden: "com.azure.search.documents.indexes.models.LexicalTokenizer.toJson(com.azure.json.JsonWriter)"
  parameters:
  - name: "jsonWriter"
    type: "<xref href=\"com.azure.json.JsonWriter?alt=com.azure.json.JsonWriter&text=JsonWriter\" data-throw-if-not-resolved=\"False\" />"
  syntax: "public JsonWriter toJson(JsonWriter jsonWriter)"
  exceptions:
  - type: "<a href=\"https://docs.oracle.com/javase/8/docs/api/java/io/IOException.html\">IOException</a>"
  returns:
    type: "<xref href=\"com.azure.json.JsonWriter?alt=com.azure.json.JsonWriter&text=JsonWriter\" data-throw-if-not-resolved=\"False\" />"
type: "class"
desc: "Grammar-based tokenizer that is suitable for processing most European-language documents. This tokenizer is implemented using Apache Lucene."
metadata: {}
package: "com.azure.search.documents.indexes.models"
artifact: com.azure:azure-search-documents:11.8.0-beta.7
