### YamlMime:JavaMember
uid: "com.azure.search.documents.indexes.implementation.models.LuceneStandardTokenizerV2.setMaxTokenLength*"
fullName: "com.azure.search.documents.indexes.implementation.models.LuceneStandardTokenizerV2.setMaxTokenLength"
name: "setMaxTokenLength"
nameWithType: "LuceneStandardTokenizerV2.setMaxTokenLength"
members:
- uid: "com.azure.search.documents.indexes.implementation.models.LuceneStandardTokenizerV2.setMaxTokenLength(java.lang.Integer)"
  fullName: "com.azure.search.documents.indexes.implementation.models.LuceneStandardTokenizerV2.setMaxTokenLength(Integer maxTokenLength)"
  name: "setMaxTokenLength(Integer maxTokenLength)"
  nameWithType: "LuceneStandardTokenizerV2.setMaxTokenLength(Integer maxTokenLength)"
  summary: "Set the maxTokenLength property: The maximum token length. Default is 255. Tokens longer than the maximum length are split. The maximum token length that can be used is 300 characters."
  parameters:
  - description: "the maxTokenLength value to set."
    name: "maxTokenLength"
    type: "<xref href=\"java.lang.Integer?alt=java.lang.Integer&text=Integer\" data-throw-if-not-resolved=\"False\" />"
  syntax: "public LuceneStandardTokenizerV2 setMaxTokenLength(Integer maxTokenLength)"
  returns:
    description: "the LuceneStandardTokenizerV2 object itself."
    type: "<xref href=\"com.azure.search.documents.indexes.implementation.models.LuceneStandardTokenizerV2?alt=com.azure.search.documents.indexes.implementation.models.LuceneStandardTokenizerV2&text=LuceneStandardTokenizerV2\" data-throw-if-not-resolved=\"False\" />"
type: "method"
metadata: {}
package: "com.azure.search.documents.indexes.implementation.models"
artifact: com.azure:azure-search-documents:11.2.0-beta.3
