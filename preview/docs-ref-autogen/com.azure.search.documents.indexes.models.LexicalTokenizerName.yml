### YamlMime:JavaType
uid: "com.azure.search.documents.indexes.models.LexicalTokenizerName"
fullName: "com.azure.search.documents.indexes.models.LexicalTokenizerName"
name: "LexicalTokenizerName"
nameWithType: "LexicalTokenizerName"
summary: "Defines the names of all tokenizers supported by the search engine."
inheritances:
- "<xref href=\"java.lang.Object?displayProperty=fullName\" data-throw-if-not-resolved=\"False\" />"
- "<xref href=\"com.azure.core.util.ExpandableStringEnum?displayProperty=fullName\" data-throw-if-not-resolved=\"False\" />"
inheritedClassMethods:
- classRef: "<xref href=\"com.azure.core.util.ExpandableStringEnum?alt=com.azure.core.util.ExpandableStringEnum&text=ExpandableStringEnum\" data-throw-if-not-resolved=\"False\" />"
  methodsRef:
  - "<xref href=\"com.azure.core.util.ExpandableStringEnum.<T>fromString(java.lang.String,java.lang.Class<T>)?alt=com.azure.core.util.ExpandableStringEnum.<T>fromString&text=<T>fromString\" data-throw-if-not-resolved=\"False\" />"
  - "<xref href=\"com.azure.core.util.ExpandableStringEnum.<T>values(java.lang.Class<T>)?alt=com.azure.core.util.ExpandableStringEnum.<T>values&text=<T>values\" data-throw-if-not-resolved=\"False\" />"
  - "<xref href=\"com.azure.core.util.ExpandableStringEnum.equals(java.lang.Object)?alt=com.azure.core.util.ExpandableStringEnum.equals&text=equals\" data-throw-if-not-resolved=\"False\" />"
  - "<xref href=\"com.azure.core.util.ExpandableStringEnum.getValue()?alt=com.azure.core.util.ExpandableStringEnum.getValue&text=getValue\" data-throw-if-not-resolved=\"False\" />"
  - "<xref href=\"com.azure.core.util.ExpandableStringEnum.hashCode()?alt=com.azure.core.util.ExpandableStringEnum.hashCode&text=hashCode\" data-throw-if-not-resolved=\"False\" />"
  - "<xref href=\"com.azure.core.util.ExpandableStringEnum.toString()?alt=com.azure.core.util.ExpandableStringEnum.toString&text=toString\" data-throw-if-not-resolved=\"False\" />"
- classRef: "java.lang.<a href=\"https://docs.oracle.com/javase/8/docs/api/java/lang/Object.html\">Object</a>"
  methodsRef:
  - "<a href=\"https://docs.oracle.com/javase/8/docs/api/java/lang/Object.html#clone--\">clone</a>"
  - "<a href=\"https://docs.oracle.com/javase/8/docs/api/java/lang/Object.html#finalize--\">finalize</a>"
  - "<a href=\"https://docs.oracle.com/javase/8/docs/api/java/lang/Object.html#getClass--\">getClass</a>"
  - "<a href=\"https://docs.oracle.com/javase/8/docs/api/java/lang/Object.html#notify--\">notify</a>"
  - "<a href=\"https://docs.oracle.com/javase/8/docs/api/java/lang/Object.html#notifyAll--\">notifyAll</a>"
  - "<a href=\"https://docs.oracle.com/javase/8/docs/api/java/lang/Object.html#wait--\">wait</a>"
  - "<a href=\"https://docs.oracle.com/javase/8/docs/api/java/lang/Object.html#wait-long-\">wait</a>"
  - "<a href=\"https://docs.oracle.com/javase/8/docs/api/java/lang/Object.html#wait-long-int-\">wait</a>"
syntax: "public final class **LexicalTokenizerName**</br> extends <xref href=\"com.azure.core.util.ExpandableStringEnum?alt=com.azure.core.util.ExpandableStringEnum&text=ExpandableStringEnum\" data-throw-if-not-resolved=\"False\" />&lt;<xref href=\"com.azure.search.documents.indexes.models.LexicalTokenizerName?alt=com.azure.search.documents.indexes.models.LexicalTokenizerName&text=LexicalTokenizerName\" data-throw-if-not-resolved=\"False\" />&gt;"
constructors:
- uid: "com.azure.search.documents.indexes.models.LexicalTokenizerName.LexicalTokenizerName()"
  fullName: "com.azure.search.documents.indexes.models.LexicalTokenizerName.LexicalTokenizerName()"
  name: "LexicalTokenizerName()"
  nameWithType: "LexicalTokenizerName.LexicalTokenizerName()"
  summary: "Creates a new instance of Lexical<wbr>Tokenizer<wbr>Name value."
  deprecatedTag: "Use the <xref uid=\"com.azure.search.documents.indexes.models.LexicalTokenizerName.fromString(java.lang.String)\" data-throw-if-not-resolved=\"false\" data-raw-source=\"#fromString(String)\"></xref> factory method."
  syntax: "<a href=\"https://docs.oracle.com/javase/8/docs/api/java/lang/Deprecated.html\">@Deprecated</a></br>public LexicalTokenizerName()"
  desc: "Creates a new instance of LexicalTokenizerName value."
  hasDeprecatedTag: true
fields:
- uid: "com.azure.search.documents.indexes.models.LexicalTokenizerName.CLASSIC"
  fullName: "com.azure.search.documents.indexes.models.LexicalTokenizerName.CLASSIC"
  name: "CLASSIC"
  nameWithType: "LexicalTokenizerName.CLASSIC"
  summary: "Grammar-based tokenizer that is suitable for processing most European-language documents."
  modifiers:
  - "static"
  - "final"
  field:
    type: "<xref href=\"com.azure.search.documents.indexes.models.LexicalTokenizerName?alt=com.azure.search.documents.indexes.models.LexicalTokenizerName&text=LexicalTokenizerName\" data-throw-if-not-resolved=\"False\" />"
  syntax: "public static final LexicalTokenizerName CLASSIC"
  desc: "Grammar-based tokenizer that is suitable for processing most European-language documents. See http://lucene.apache.org/core/4\\_10\\_3/analyzers-common/org/apache/lucene/analysis/standard/ClassicTokenizer.html."
- uid: "com.azure.search.documents.indexes.models.LexicalTokenizerName.EDGE_NGRAM"
  fullName: "com.azure.search.documents.indexes.models.LexicalTokenizerName.EDGE_NGRAM"
  name: "EDGE_NGRAM"
  nameWithType: "LexicalTokenizerName.EDGE_NGRAM"
  summary: "Tokenizes the input from an edge into n-grams of the given size(s)."
  modifiers:
  - "static"
  - "final"
  field:
    type: "<xref href=\"com.azure.search.documents.indexes.models.LexicalTokenizerName?alt=com.azure.search.documents.indexes.models.LexicalTokenizerName&text=LexicalTokenizerName\" data-throw-if-not-resolved=\"False\" />"
  syntax: "public static final LexicalTokenizerName EDGE_NGRAM"
  desc: "Tokenizes the input from an edge into n-grams of the given size(s). See https://lucene.apache.org/core/4\\_10\\_3/analyzers-common/org/apache/lucene/analysis/ngram/EdgeNGramTokenizer.html."
- uid: "com.azure.search.documents.indexes.models.LexicalTokenizerName.KEYWORD"
  fullName: "com.azure.search.documents.indexes.models.LexicalTokenizerName.KEYWORD"
  name: "KEYWORD"
  nameWithType: "LexicalTokenizerName.KEYWORD"
  summary: "Emits the entire input as a single token."
  modifiers:
  - "static"
  - "final"
  field:
    type: "<xref href=\"com.azure.search.documents.indexes.models.LexicalTokenizerName?alt=com.azure.search.documents.indexes.models.LexicalTokenizerName&text=LexicalTokenizerName\" data-throw-if-not-resolved=\"False\" />"
  syntax: "public static final LexicalTokenizerName KEYWORD"
  desc: "Emits the entire input as a single token. See http://lucene.apache.org/core/4\\_10\\_3/analyzers-common/org/apache/lucene/analysis/core/KeywordTokenizer.html."
- uid: "com.azure.search.documents.indexes.models.LexicalTokenizerName.LETTER"
  fullName: "com.azure.search.documents.indexes.models.LexicalTokenizerName.LETTER"
  name: "LETTER"
  nameWithType: "LexicalTokenizerName.LETTER"
  summary: "Divides text at non-letters."
  modifiers:
  - "static"
  - "final"
  field:
    type: "<xref href=\"com.azure.search.documents.indexes.models.LexicalTokenizerName?alt=com.azure.search.documents.indexes.models.LexicalTokenizerName&text=LexicalTokenizerName\" data-throw-if-not-resolved=\"False\" />"
  syntax: "public static final LexicalTokenizerName LETTER"
  desc: "Divides text at non-letters. See http://lucene.apache.org/core/4\\_10\\_3/analyzers-common/org/apache/lucene/analysis/core/LetterTokenizer.html."
- uid: "com.azure.search.documents.indexes.models.LexicalTokenizerName.LOWERCASE"
  fullName: "com.azure.search.documents.indexes.models.LexicalTokenizerName.LOWERCASE"
  name: "LOWERCASE"
  nameWithType: "LexicalTokenizerName.LOWERCASE"
  summary: "Divides text at non-letters and converts them to lower case."
  modifiers:
  - "static"
  - "final"
  field:
    type: "<xref href=\"com.azure.search.documents.indexes.models.LexicalTokenizerName?alt=com.azure.search.documents.indexes.models.LexicalTokenizerName&text=LexicalTokenizerName\" data-throw-if-not-resolved=\"False\" />"
  syntax: "public static final LexicalTokenizerName LOWERCASE"
  desc: "Divides text at non-letters and converts them to lower case. See http://lucene.apache.org/core/4\\_10\\_3/analyzers-common/org/apache/lucene/analysis/core/LowerCaseTokenizer.html."
- uid: "com.azure.search.documents.indexes.models.LexicalTokenizerName.MICROSOFT_LANGUAGE_STEMMING_TOKENIZER"
  fullName: "com.azure.search.documents.indexes.models.LexicalTokenizerName.MICROSOFT_LANGUAGE_STEMMING_TOKENIZER"
  name: "MICROSOFT_LANGUAGE_STEMMING_TOKENIZER"
  nameWithType: "LexicalTokenizerName.MICROSOFT_LANGUAGE_STEMMING_TOKENIZER"
  summary: "Divides text using language-specific rules and reduces words to their base forms."
  modifiers:
  - "static"
  - "final"
  field:
    type: "<xref href=\"com.azure.search.documents.indexes.models.LexicalTokenizerName?alt=com.azure.search.documents.indexes.models.LexicalTokenizerName&text=LexicalTokenizerName\" data-throw-if-not-resolved=\"False\" />"
  syntax: "public static final LexicalTokenizerName MICROSOFT_LANGUAGE_STEMMING_TOKENIZER"
  desc: "Divides text using language-specific rules and reduces words to their base forms."
- uid: "com.azure.search.documents.indexes.models.LexicalTokenizerName.MICROSOFT_LANGUAGE_TOKENIZER"
  fullName: "com.azure.search.documents.indexes.models.LexicalTokenizerName.MICROSOFT_LANGUAGE_TOKENIZER"
  name: "MICROSOFT_LANGUAGE_TOKENIZER"
  nameWithType: "LexicalTokenizerName.MICROSOFT_LANGUAGE_TOKENIZER"
  summary: "Divides text using language-specific rules."
  modifiers:
  - "static"
  - "final"
  field:
    type: "<xref href=\"com.azure.search.documents.indexes.models.LexicalTokenizerName?alt=com.azure.search.documents.indexes.models.LexicalTokenizerName&text=LexicalTokenizerName\" data-throw-if-not-resolved=\"False\" />"
  syntax: "public static final LexicalTokenizerName MICROSOFT_LANGUAGE_TOKENIZER"
  desc: "Divides text using language-specific rules."
- uid: "com.azure.search.documents.indexes.models.LexicalTokenizerName.NGRAM"
  fullName: "com.azure.search.documents.indexes.models.LexicalTokenizerName.NGRAM"
  name: "NGRAM"
  nameWithType: "LexicalTokenizerName.NGRAM"
  summary: "Tokenizes the input into n-grams of the given size(s)."
  modifiers:
  - "static"
  - "final"
  field:
    type: "<xref href=\"com.azure.search.documents.indexes.models.LexicalTokenizerName?alt=com.azure.search.documents.indexes.models.LexicalTokenizerName&text=LexicalTokenizerName\" data-throw-if-not-resolved=\"False\" />"
  syntax: "public static final LexicalTokenizerName NGRAM"
  desc: "Tokenizes the input into n-grams of the given size(s). See http://lucene.apache.org/core/4\\_10\\_3/analyzers-common/org/apache/lucene/analysis/ngram/NGramTokenizer.html."
- uid: "com.azure.search.documents.indexes.models.LexicalTokenizerName.PATH_HIERARCHY"
  fullName: "com.azure.search.documents.indexes.models.LexicalTokenizerName.PATH_HIERARCHY"
  name: "PATH_HIERARCHY"
  nameWithType: "LexicalTokenizerName.PATH_HIERARCHY"
  summary: "Tokenizer for path-like hierarchies."
  modifiers:
  - "static"
  - "final"
  field:
    type: "<xref href=\"com.azure.search.documents.indexes.models.LexicalTokenizerName?alt=com.azure.search.documents.indexes.models.LexicalTokenizerName&text=LexicalTokenizerName\" data-throw-if-not-resolved=\"False\" />"
  syntax: "public static final LexicalTokenizerName PATH_HIERARCHY"
  desc: "Tokenizer for path-like hierarchies. See http://lucene.apache.org/core/4\\_10\\_3/analyzers-common/org/apache/lucene/analysis/path/PathHierarchyTokenizer.html."
- uid: "com.azure.search.documents.indexes.models.LexicalTokenizerName.PATTERN"
  fullName: "com.azure.search.documents.indexes.models.LexicalTokenizerName.PATTERN"
  name: "PATTERN"
  nameWithType: "LexicalTokenizerName.PATTERN"
  summary: "Tokenizer that uses regex pattern matching to construct distinct tokens."
  modifiers:
  - "static"
  - "final"
  field:
    type: "<xref href=\"com.azure.search.documents.indexes.models.LexicalTokenizerName?alt=com.azure.search.documents.indexes.models.LexicalTokenizerName&text=LexicalTokenizerName\" data-throw-if-not-resolved=\"False\" />"
  syntax: "public static final LexicalTokenizerName PATTERN"
  desc: "Tokenizer that uses regex pattern matching to construct distinct tokens. See http://lucene.apache.org/core/4\\_10\\_3/analyzers-common/org/apache/lucene/analysis/pattern/PatternTokenizer.html."
- uid: "com.azure.search.documents.indexes.models.LexicalTokenizerName.STANDARD"
  fullName: "com.azure.search.documents.indexes.models.LexicalTokenizerName.STANDARD"
  name: "STANDARD"
  nameWithType: "LexicalTokenizerName.STANDARD"
  summary: "Standard Lucene analyzer; Composed of the standard tokenizer, lowercase filter and stop filter."
  modifiers:
  - "static"
  - "final"
  field:
    type: "<xref href=\"com.azure.search.documents.indexes.models.LexicalTokenizerName?alt=com.azure.search.documents.indexes.models.LexicalTokenizerName&text=LexicalTokenizerName\" data-throw-if-not-resolved=\"False\" />"
  syntax: "public static final LexicalTokenizerName STANDARD"
  desc: "Standard Lucene analyzer; Composed of the standard tokenizer, lowercase filter and stop filter. See http://lucene.apache.org/core/4\\_10\\_3/analyzers-common/org/apache/lucene/analysis/standard/StandardTokenizer.html."
- uid: "com.azure.search.documents.indexes.models.LexicalTokenizerName.UAX_URL_EMAIL"
  fullName: "com.azure.search.documents.indexes.models.LexicalTokenizerName.UAX_URL_EMAIL"
  name: "UAX_URL_EMAIL"
  nameWithType: "LexicalTokenizerName.UAX_URL_EMAIL"
  summary: "Tokenizes urls and emails as one token."
  modifiers:
  - "static"
  - "final"
  field:
    type: "<xref href=\"com.azure.search.documents.indexes.models.LexicalTokenizerName?alt=com.azure.search.documents.indexes.models.LexicalTokenizerName&text=LexicalTokenizerName\" data-throw-if-not-resolved=\"False\" />"
  syntax: "public static final LexicalTokenizerName UAX_URL_EMAIL"
  desc: "Tokenizes urls and emails as one token. See http://lucene.apache.org/core/4\\_10\\_3/analyzers-common/org/apache/lucene/analysis/standard/UAX29URLEmailTokenizer.html."
- uid: "com.azure.search.documents.indexes.models.LexicalTokenizerName.WHITESPACE"
  fullName: "com.azure.search.documents.indexes.models.LexicalTokenizerName.WHITESPACE"
  name: "WHITESPACE"
  nameWithType: "LexicalTokenizerName.WHITESPACE"
  summary: "Divides text at whitespace."
  modifiers:
  - "static"
  - "final"
  field:
    type: "<xref href=\"com.azure.search.documents.indexes.models.LexicalTokenizerName?alt=com.azure.search.documents.indexes.models.LexicalTokenizerName&text=LexicalTokenizerName\" data-throw-if-not-resolved=\"False\" />"
  syntax: "public static final LexicalTokenizerName WHITESPACE"
  desc: "Divides text at whitespace. See http://lucene.apache.org/core/4\\_10\\_3/analyzers-common/org/apache/lucene/analysis/core/WhitespaceTokenizer.html."
methods:
- uid: "com.azure.search.documents.indexes.models.LexicalTokenizerName.fromString(java.lang.String)"
  fullName: "com.azure.search.documents.indexes.models.LexicalTokenizerName.fromString(String name)"
  name: "fromString(String name)"
  nameWithType: "LexicalTokenizerName.fromString(String name)"
  summary: "Creates or finds a Lexical<wbr>Tokenizer<wbr>Name from its string representation."
  modifiers:
  - "static"
  parameters:
  - description: "a name to look for."
    name: "name"
    type: "<a href=\"https://docs.oracle.com/javase/8/docs/api/java/lang/String.html\">String</a>"
  syntax: "public static LexicalTokenizerName fromString(String name)"
  desc: "Creates or finds a LexicalTokenizerName from its string representation."
  returns:
    description: "the corresponding LexicalTokenizerName."
    type: "<xref href=\"com.azure.search.documents.indexes.models.LexicalTokenizerName?alt=com.azure.search.documents.indexes.models.LexicalTokenizerName&text=LexicalTokenizerName\" data-throw-if-not-resolved=\"False\" />"
- uid: "com.azure.search.documents.indexes.models.LexicalTokenizerName.values()"
  fullName: "com.azure.search.documents.indexes.models.LexicalTokenizerName.values()"
  name: "values()"
  nameWithType: "LexicalTokenizerName.values()"
  summary: "Gets known Lexical<wbr>Tokenizer<wbr>Name values."
  modifiers:
  - "static"
  syntax: "public static Collection<LexicalTokenizerName> values()"
  desc: "Gets known LexicalTokenizerName values."
  returns:
    description: "known LexicalTokenizerName values."
    type: "<a href=\"https://docs.oracle.com/javase/8/docs/api/java/util/Collection.html\">Collection</a>&lt;<xref href=\"com.azure.search.documents.indexes.models.LexicalTokenizerName?alt=com.azure.search.documents.indexes.models.LexicalTokenizerName&text=LexicalTokenizerName\" data-throw-if-not-resolved=\"False\" />&gt;"
type: "class"
desc: "Defines the names of all tokenizers supported by the search engine."
metadata: {}
package: "com.azure.search.documents.indexes.models"
artifact: com.azure:azure-search-documents:11.8.0-beta.1
