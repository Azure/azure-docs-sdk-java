### YamlMime:JavaType
uid: "com.azure.search.documents.indexes.models.KeywordTokenizer"
fullName: "com.azure.search.documents.indexes.models.KeywordTokenizer"
name: "KeywordTokenizer"
nameWithType: "KeywordTokenizer"
summary: "Emits the entire input as a single token."
inheritances:
- "<xref href=\"java.lang.Object?displayProperty=fullName\" data-throw-if-not-resolved=\"False\" />"
- "<xref href=\"com.azure.search.documents.indexes.models.LexicalTokenizer?displayProperty=fullName\" data-throw-if-not-resolved=\"False\" />"
inheritedMembers:
- "com.azure.search.documents.indexes.models.LexicalTokenizer.getName()"
- "java.lang.Object.clone()"
- "java.lang.Object.equals(java.lang.Object)"
- "java.lang.Object.finalize()"
- "java.lang.Object.getClass()"
- "java.lang.Object.hashCode()"
- "java.lang.Object.notify()"
- "java.lang.Object.notifyAll()"
- "java.lang.Object.toString()"
- "java.lang.Object.wait()"
- "java.lang.Object.wait(long)"
- "java.lang.Object.wait(long,int)"
syntax: "public final class KeywordTokenizer extends LexicalTokenizer"
constructors:
- uid: "com.azure.search.documents.indexes.models.KeywordTokenizer.KeywordTokenizer(java.lang.String)"
  fullName: "com.azure.search.documents.indexes.models.KeywordTokenizer.KeywordTokenizer(String name)"
  name: "KeywordTokenizer(String name)"
  nameWithType: "KeywordTokenizer.KeywordTokenizer(String name)"
  summary: "Constructor of <xref uid=\"com.azure.search.documents.indexes.models.KeywordTokenizer\" data-throw-if-not-resolved=\"false\" data-raw-source=\"KeywordTokenizer\"></xref>."
  parameters:
  - description: "The name of the tokenizer. It must only contain letters, digits, spaces,\n dashes or underscores, can only start and end with alphanumeric\n characters, and is limited to 128 characters."
    name: "name"
    type: "<xref href=\"java.lang.String?alt=java.lang.String&text=String\" data-throw-if-not-resolved=\"False\" />"
  syntax: "public KeywordTokenizer(String name)"
  desc: "Constructor of <xref uid=\"com.azure.search.documents.indexes.models.KeywordTokenizer\" data-throw-if-not-resolved=\"false\" data-raw-source=\"KeywordTokenizer\"></xref>."
methods:
- uid: "com.azure.search.documents.indexes.models.KeywordTokenizer.getMaxTokenLength()"
  fullName: "com.azure.search.documents.indexes.models.KeywordTokenizer.getMaxTokenLength()"
  name: "getMaxTokenLength()"
  nameWithType: "KeywordTokenizer.getMaxTokenLength()"
  summary: "Get the max<wbr>Token<wbr>Length property: The maximum token length."
  syntax: "public Integer getMaxTokenLength()"
  desc: "Get the maxTokenLength property: The maximum token length. Default is 256. Tokens longer than the maximum length are split. The maximum token length that can be used is 300 characters."
  returns:
    description: "the maxTokenLength value."
    type: "<xref href=\"java.lang.Integer?alt=java.lang.Integer&text=Integer\" data-throw-if-not-resolved=\"False\" />"
- uid: "com.azure.search.documents.indexes.models.KeywordTokenizer.setMaxTokenLength(java.lang.Integer)"
  fullName: "com.azure.search.documents.indexes.models.KeywordTokenizer.setMaxTokenLength(Integer maxTokenLength)"
  name: "setMaxTokenLength(Integer maxTokenLength)"
  nameWithType: "KeywordTokenizer.setMaxTokenLength(Integer maxTokenLength)"
  summary: "Set the max<wbr>Token<wbr>Length property: The maximum token length."
  parameters:
  - description: "the maxTokenLength value to set."
    name: "maxTokenLength"
    type: "<xref href=\"java.lang.Integer?alt=java.lang.Integer&text=Integer\" data-throw-if-not-resolved=\"False\" />"
  syntax: "public KeywordTokenizer setMaxTokenLength(Integer maxTokenLength)"
  desc: "Set the maxTokenLength property: The maximum token length. Default is 256. Tokens longer than the maximum length are split. The maximum token length that can be used is 300 characters."
  returns:
    description: "the KeywordTokenizerV2 object itself."
    type: "<xref href=\"com.azure.search.documents.indexes.models.KeywordTokenizer?alt=com.azure.search.documents.indexes.models.KeywordTokenizer&text=KeywordTokenizer\" data-throw-if-not-resolved=\"False\" />"
type: "class"
desc: "Emits the entire input as a single token. This tokenizer is implemented using Apache Lucene."
metadata: {}
package: "com.azure.search.documents.indexes.models"
artifact: com.azure:azure-search-documents:11.5.0-beta.12
