### YamlMime:ManagedReference
items:
- uid: com.azure.storage.file.datalake.DataLakeFileClient
  id: DataLakeFileClient
  artifact: com.azure:azure-storage-file-datalake:12.3.0
  parent: com.azure.storage.file.datalake
  children:
  - com.azure.storage.file.datalake.DataLakeFileClient.append(java.io.InputStream,long,long)
  - com.azure.storage.file.datalake.DataLakeFileClient.appendWithResponse(java.io.InputStream,long,long,byte[],java.lang.String,java.time.Duration,com.azure.core.util.Context)
  - com.azure.storage.file.datalake.DataLakeFileClient.delete()
  - com.azure.storage.file.datalake.DataLakeFileClient.deleteWithResponse(com.azure.storage.file.datalake.models.DataLakeRequestConditions,java.time.Duration,com.azure.core.util.Context)
  - com.azure.storage.file.datalake.DataLakeFileClient.flush(long)
  - com.azure.storage.file.datalake.DataLakeFileClient.flush(long,boolean)
  - com.azure.storage.file.datalake.DataLakeFileClient.flushWithResponse(long,boolean,boolean,com.azure.storage.file.datalake.models.PathHttpHeaders,com.azure.storage.file.datalake.models.DataLakeRequestConditions,java.time.Duration,com.azure.core.util.Context)
  - com.azure.storage.file.datalake.DataLakeFileClient.getFileName()
  - com.azure.storage.file.datalake.DataLakeFileClient.getFilePath()
  - com.azure.storage.file.datalake.DataLakeFileClient.getFileUrl()
  - com.azure.storage.file.datalake.DataLakeFileClient.openQueryInputStream(java.lang.String)
  - com.azure.storage.file.datalake.DataLakeFileClient.openQueryInputStreamWithResponse(com.azure.storage.file.datalake.options.FileQueryOptions)
  - com.azure.storage.file.datalake.DataLakeFileClient.query(java.io.OutputStream,java.lang.String)
  - com.azure.storage.file.datalake.DataLakeFileClient.queryWithResponse(com.azure.storage.file.datalake.options.FileQueryOptions,java.time.Duration,com.azure.core.util.Context)
  - com.azure.storage.file.datalake.DataLakeFileClient.read(java.io.OutputStream)
  - com.azure.storage.file.datalake.DataLakeFileClient.readToFile(java.lang.String)
  - com.azure.storage.file.datalake.DataLakeFileClient.readToFile(java.lang.String,boolean)
  - com.azure.storage.file.datalake.DataLakeFileClient.readToFileWithResponse(java.lang.String,com.azure.storage.file.datalake.models.FileRange,com.azure.storage.common.ParallelTransferOptions,com.azure.storage.file.datalake.models.DownloadRetryOptions,com.azure.storage.file.datalake.models.DataLakeRequestConditions,boolean,java.util.Set<java.nio.file.OpenOption>,java.time.Duration,com.azure.core.util.Context)
  - com.azure.storage.file.datalake.DataLakeFileClient.readWithResponse(java.io.OutputStream,com.azure.storage.file.datalake.models.FileRange,com.azure.storage.file.datalake.models.DownloadRetryOptions,com.azure.storage.file.datalake.models.DataLakeRequestConditions,boolean,java.time.Duration,com.azure.core.util.Context)
  - com.azure.storage.file.datalake.DataLakeFileClient.rename(java.lang.String,java.lang.String)
  - com.azure.storage.file.datalake.DataLakeFileClient.renameWithResponse(java.lang.String,java.lang.String,com.azure.storage.file.datalake.models.DataLakeRequestConditions,com.azure.storage.file.datalake.models.DataLakeRequestConditions,java.time.Duration,com.azure.core.util.Context)
  - com.azure.storage.file.datalake.DataLakeFileClient.scheduleDeletion(com.azure.storage.file.datalake.options.FileScheduleDeletionOptions)
  - com.azure.storage.file.datalake.DataLakeFileClient.scheduleDeletionWithResponse(com.azure.storage.file.datalake.options.FileScheduleDeletionOptions,java.time.Duration,com.azure.core.util.Context)
  - com.azure.storage.file.datalake.DataLakeFileClient.upload(java.io.InputStream,long)
  - com.azure.storage.file.datalake.DataLakeFileClient.upload(java.io.InputStream,long,boolean)
  - com.azure.storage.file.datalake.DataLakeFileClient.uploadFromFile(java.lang.String)
  - com.azure.storage.file.datalake.DataLakeFileClient.uploadFromFile(java.lang.String,boolean)
  - com.azure.storage.file.datalake.DataLakeFileClient.uploadFromFile(java.lang.String,com.azure.storage.common.ParallelTransferOptions,com.azure.storage.file.datalake.models.PathHttpHeaders,java.util.Map<java.lang.String,java.lang.String>,com.azure.storage.file.datalake.models.DataLakeRequestConditions,java.time.Duration)
  - com.azure.storage.file.datalake.DataLakeFileClient.uploadWithResponse(com.azure.storage.file.datalake.options.FileParallelUploadOptions,java.time.Duration,com.azure.core.util.Context)
  langs:
  - java
  name: DataLakeFileClient
  nameWithType: DataLakeFileClient
  fullName: com.azure.storage.file.datalake.DataLakeFileClient
  type: Class
  package: com.azure.storage.file.datalake
  summary: >-
    This class provides a client that contains file operations for Azure Storage Data Lake. Operations provided by this client include creating a file, deleting a file, renaming a file, setting metadata and http headers, setting and retrieving access control, getting properties, reading a file, and appending and flushing data to write to a file.


    This client is instantiated through <xref uid="com.azure.storage.file.datalake.DataLakePathClientBuilder" data-throw-if-not-resolved="false">DataLakePathClientBuilder</xref> or retrieved via <xref uid="com.azure.storage.file.datalake.DataLakeFileSystemClient.getFileClient(java.lang.String)" data-throw-if-not-resolved="false">getFileClient</xref>.


    Please refer to the [Azure Docs][] for more information.



    [Azure Docs]: https://docs.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-introduction?toc=%2fazure%2fstorage%2fblobs%2ftoc.json
  syntax:
    content: public class DataLakeFileClient extends DataLakePathClient
  inheritance:
  - java.lang.Object
  - com.azure.storage.file.datalake.DataLakePathClient
  inheritedMembers:
  - com.azure.storage.file.datalake.DataLakePathClient.create()
  - com.azure.storage.file.datalake.DataLakePathClient.create(boolean)
  - com.azure.storage.file.datalake.DataLakePathClient.createWithResponse(java.lang.String,java.lang.String,com.azure.storage.file.datalake.models.PathHttpHeaders,java.util.Map<java.lang.String,java.lang.String>,com.azure.storage.file.datalake.models.DataLakeRequestConditions,java.time.Duration,com.azure.core.util.Context)
  - com.azure.storage.file.datalake.DataLakePathClient.exists()
  - com.azure.storage.file.datalake.DataLakePathClient.existsWithResponse(java.time.Duration,com.azure.core.util.Context)
  - com.azure.storage.file.datalake.DataLakePathClient.generateSas(com.azure.storage.file.datalake.sas.DataLakeServiceSasSignatureValues)
  - com.azure.storage.file.datalake.DataLakePathClient.generateUserDelegationSas(com.azure.storage.file.datalake.sas.DataLakeServiceSasSignatureValues,com.azure.storage.file.datalake.models.UserDelegationKey)
  - com.azure.storage.file.datalake.DataLakePathClient.getAccessControl()
  - com.azure.storage.file.datalake.DataLakePathClient.getAccessControlWithResponse(boolean,com.azure.storage.file.datalake.models.DataLakeRequestConditions,java.time.Duration,com.azure.core.util.Context)
  - com.azure.storage.file.datalake.DataLakePathClient.getAccountName()
  - com.azure.storage.file.datalake.DataLakePathClient.getFileSystemName()
  - com.azure.storage.file.datalake.DataLakePathClient.getHttpPipeline()
  - com.azure.storage.file.datalake.DataLakePathClient.getProperties()
  - com.azure.storage.file.datalake.DataLakePathClient.getPropertiesWithResponse(com.azure.storage.file.datalake.models.DataLakeRequestConditions,java.time.Duration,com.azure.core.util.Context)
  - com.azure.storage.file.datalake.DataLakePathClient.getServiceVersion()
  - com.azure.storage.file.datalake.DataLakePathClient.removeAccessControlRecursive(java.util.List<com.azure.storage.file.datalake.models.PathRemoveAccessControlEntry>)
  - com.azure.storage.file.datalake.DataLakePathClient.removeAccessControlRecursiveWithResponse(com.azure.storage.file.datalake.options.PathRemoveAccessControlRecursiveOptions,java.time.Duration,com.azure.core.util.Context)
  - com.azure.storage.file.datalake.DataLakePathClient.setAccessControlList(java.util.List<com.azure.storage.file.datalake.models.PathAccessControlEntry>,java.lang.String,java.lang.String)
  - com.azure.storage.file.datalake.DataLakePathClient.setAccessControlListWithResponse(java.util.List<com.azure.storage.file.datalake.models.PathAccessControlEntry>,java.lang.String,java.lang.String,com.azure.storage.file.datalake.models.DataLakeRequestConditions,java.time.Duration,com.azure.core.util.Context)
  - com.azure.storage.file.datalake.DataLakePathClient.setAccessControlRecursive(java.util.List<com.azure.storage.file.datalake.models.PathAccessControlEntry>)
  - com.azure.storage.file.datalake.DataLakePathClient.setAccessControlRecursiveWithResponse(com.azure.storage.file.datalake.options.PathSetAccessControlRecursiveOptions,java.time.Duration,com.azure.core.util.Context)
  - com.azure.storage.file.datalake.DataLakePathClient.setHttpHeaders(com.azure.storage.file.datalake.models.PathHttpHeaders)
  - com.azure.storage.file.datalake.DataLakePathClient.setHttpHeadersWithResponse(com.azure.storage.file.datalake.models.PathHttpHeaders,com.azure.storage.file.datalake.models.DataLakeRequestConditions,java.time.Duration,com.azure.core.util.Context)
  - com.azure.storage.file.datalake.DataLakePathClient.setMetadata(java.util.Map<java.lang.String,java.lang.String>)
  - com.azure.storage.file.datalake.DataLakePathClient.setMetadataWithResponse(java.util.Map<java.lang.String,java.lang.String>,com.azure.storage.file.datalake.models.DataLakeRequestConditions,java.time.Duration,com.azure.core.util.Context)
  - com.azure.storage.file.datalake.DataLakePathClient.setPermissions(com.azure.storage.file.datalake.models.PathPermissions,java.lang.String,java.lang.String)
  - com.azure.storage.file.datalake.DataLakePathClient.setPermissionsWithResponse(com.azure.storage.file.datalake.models.PathPermissions,java.lang.String,java.lang.String,com.azure.storage.file.datalake.models.DataLakeRequestConditions,java.time.Duration,com.azure.core.util.Context)
  - com.azure.storage.file.datalake.DataLakePathClient.updateAccessControlRecursive(java.util.List<com.azure.storage.file.datalake.models.PathAccessControlEntry>)
  - com.azure.storage.file.datalake.DataLakePathClient.updateAccessControlRecursiveWithResponse(com.azure.storage.file.datalake.options.PathUpdateAccessControlRecursiveOptions,java.time.Duration,com.azure.core.util.Context)
  - java.lang.Object.clone()
  - java.lang.Object.equals(java.lang.Object)
  - java.lang.Object.finalize()
  - java.lang.Object.getClass()
  - java.lang.Object.hashCode()
  - java.lang.Object.notify()
  - java.lang.Object.notifyAll()
  - java.lang.Object.toString()
  - java.lang.Object.wait()
  - java.lang.Object.wait(long)
  - java.lang.Object.wait(long,int)
- uid: com.azure.storage.file.datalake.DataLakeFileClient.DataLakeFileClient(com.azure.storage.file.datalake.DataLakeFileAsyncClient,com.azure.storage.blob.specialized.BlockBlobClient)
  id: DataLakeFileClient(com.azure.storage.file.datalake.DataLakeFileAsyncClient,com.azure.storage.blob.specialized.BlockBlobClient)
  artifact: com.azure:azure-storage-file-datalake:12.3.0
  parent: com.azure.storage.file.datalake.DataLakeFileClient
  langs:
  - java
  name: DataLakeFileClient(DataLakeFileAsyncClient pathAsyncClient, BlockBlobClient blockBlobClient)
  nameWithType: DataLakeFileClient.DataLakeFileClient(DataLakeFileAsyncClient pathAsyncClient, BlockBlobClient blockBlobClient)
  fullName: com.azure.storage.file.datalake.DataLakeFileClient.DataLakeFileClient(DataLakeFileAsyncClient pathAsyncClient, BlockBlobClient blockBlobClient)
  overload: com.azure.storage.file.datalake.DataLakeFileClient.DataLakeFileClient*
  type: Constructor
  package: com.azure.storage.file.datalake
  syntax:
    content: " DataLakeFileClient(DataLakeFileAsyncClient pathAsyncClient, BlockBlobClient blockBlobClient)"
    parameters:
    - id: pathAsyncClient
      type: com.azure.storage.file.datalake.DataLakeFileAsyncClient
    - id: blockBlobClient
      type: com.azure.storage.blob.specialized.BlockBlobClient
- uid: com.azure.storage.file.datalake.DataLakeFileClient.DataLakeFileClient(com.azure.storage.file.datalake.DataLakePathClient)
  id: DataLakeFileClient(com.azure.storage.file.datalake.DataLakePathClient)
  artifact: com.azure:azure-storage-file-datalake:12.3.0
  parent: com.azure.storage.file.datalake.DataLakeFileClient
  langs:
  - java
  name: DataLakeFileClient(DataLakePathClient dataLakePathClient)
  nameWithType: DataLakeFileClient.DataLakeFileClient(DataLakePathClient dataLakePathClient)
  fullName: com.azure.storage.file.datalake.DataLakeFileClient.DataLakeFileClient(DataLakePathClient dataLakePathClient)
  overload: com.azure.storage.file.datalake.DataLakeFileClient.DataLakeFileClient*
  type: Constructor
  package: com.azure.storage.file.datalake
  syntax:
    content: private DataLakeFileClient(DataLakePathClient dataLakePathClient)
    parameters:
    - id: dataLakePathClient
      type: com.azure.storage.file.datalake.DataLakePathClient
- uid: com.azure.storage.file.datalake.DataLakeFileClient.append(java.io.InputStream,long,long)
  id: append(java.io.InputStream,long,long)
  artifact: com.azure:azure-storage-file-datalake:12.3.0
  parent: com.azure.storage.file.datalake.DataLakeFileClient
  langs:
  - java
  name: append(InputStream data, long fileOffset, long length)
  nameWithType: DataLakeFileClient.append(InputStream data, long fileOffset, long length)
  fullName: com.azure.storage.file.datalake.DataLakeFileClient.append(InputStream data, long fileOffset, long length)
  overload: com.azure.storage.file.datalake.DataLakeFileClient.append*
  type: Method
  package: com.azure.storage.file.datalake
  summary: >-
    Appends data to the specified resource to later be flushed (written) by a call to flush


    **Code Samples**


    ```java

    client.append(data, offset, length);
     System.out.println("Append data completed");
    ```


    For more information, see the [Azure Docs][]



    [Azure Docs]: https://docs.microsoft.com/en-us/rest/api/storageservices/datalakestoragegen2/path/update
  syntax:
    content: public void append(InputStream data, long fileOffset, long length)
    parameters:
    - id: data
      type: java.io.InputStream
      description: The data to write to the file.
    - id: fileOffset
      type: long
      description: The position where the data is to be appended.
    - id: length
      type: long
      description: The exact length of the data.
- uid: com.azure.storage.file.datalake.DataLakeFileClient.appendWithResponse(java.io.InputStream,long,long,byte[],java.lang.String,java.time.Duration,com.azure.core.util.Context)
  id: appendWithResponse(java.io.InputStream,long,long,byte[],java.lang.String,java.time.Duration,com.azure.core.util.Context)
  artifact: com.azure:azure-storage-file-datalake:12.3.0
  parent: com.azure.storage.file.datalake.DataLakeFileClient
  langs:
  - java
  name: appendWithResponse(InputStream data, long fileOffset, long length, byte[] contentMd5, String leaseId, Duration timeout, Context context)
  nameWithType: DataLakeFileClient.appendWithResponse(InputStream data, long fileOffset, long length, byte[] contentMd5, String leaseId, Duration timeout, Context context)
  fullName: com.azure.storage.file.datalake.DataLakeFileClient.appendWithResponse(InputStream data, long fileOffset, long length, byte[] contentMd5, String leaseId, Duration timeout, Context context)
  overload: com.azure.storage.file.datalake.DataLakeFileClient.appendWithResponse*
  type: Method
  package: com.azure.storage.file.datalake
  summary: "Appends data to the specified resource to later be flushed (written) by a call to flush\n\n**Code Samples**\n\n```java\nFileRange range = new FileRange(1024, 2048L);\n DownloadRetryOptions options = new DownloadRetryOptions().setMaxRetryRequests(5);\n byte[] contentMd5 = new byte[0]; // Replace with valid md5\n \n Response<Void> response = client.appendWithResponse(data, offset, length, contentMd5, leaseId, timeout,\n     new Context(key1, value1));\n System.out.printf(\"Append data completed with status %d%n\", response.getStatusCode());\n```\n\nFor more information, see the [Azure Docs][]\n\n\n[Azure Docs]: https://docs.microsoft.com/en-us/rest/api/storageservices/datalakestoragegen2/path/update"
  syntax:
    content: public Response<Void> appendWithResponse(InputStream data, long fileOffset, long length, byte[] contentMd5, String leaseId, Duration timeout, Context context)
    parameters:
    - id: data
      type: java.io.InputStream
      description: The data to write to the file.
    - id: fileOffset
      type: long
      description: The position where the data is to be appended.
    - id: length
      type: long
      description: The exact length of the data.
    - id: contentMd5
      type: byte[]
      description: >-
        An MD5 hash of the content of the data. If specified, the service will calculate the MD5 of the
         received data and fail the request if it does not match the provided MD5.
    - id: leaseId
      type: java.lang.String
      description: >-
        By setting lease id, requests will fail if the provided lease does not match the active lease on
         the file.
    - id: timeout
      type: java.time.Duration
      description: An optional timeout value beyond which a <xref uid="" data-throw-if-not-resolved="false">RuntimeException</xref> will be raised.
    - id: context
      type: com.azure.core.util.Context
      description: Additional context that is passed through the Http pipeline during the service call.
    return:
      type: com.azure.core.http.rest.Response<java.lang.Void>
      description: A response signalling completion.
- uid: com.azure.storage.file.datalake.DataLakeFileClient.delete()
  id: delete()
  artifact: com.azure:azure-storage-file-datalake:12.3.0
  parent: com.azure.storage.file.datalake.DataLakeFileClient
  langs:
  - java
  name: delete()
  nameWithType: DataLakeFileClient.delete()
  fullName: com.azure.storage.file.datalake.DataLakeFileClient.delete()
  overload: com.azure.storage.file.datalake.DataLakeFileClient.delete*
  type: Method
  package: com.azure.storage.file.datalake
  summary: >-
    Deletes a file.


    **Code Samples**


    ```java

    client.delete();
     System.out.println("Delete request completed");
    ```


    For more information see the [Azure Docs][]



    [Azure Docs]: https://docs.microsoft.com/en-us/rest/api/storageservices/datalakestoragegen2/path/delete
  syntax:
    content: public void delete()
- uid: com.azure.storage.file.datalake.DataLakeFileClient.deleteWithResponse(com.azure.storage.file.datalake.models.DataLakeRequestConditions,java.time.Duration,com.azure.core.util.Context)
  id: deleteWithResponse(com.azure.storage.file.datalake.models.DataLakeRequestConditions,java.time.Duration,com.azure.core.util.Context)
  artifact: com.azure:azure-storage-file-datalake:12.3.0
  parent: com.azure.storage.file.datalake.DataLakeFileClient
  langs:
  - java
  name: deleteWithResponse(DataLakeRequestConditions requestConditions, Duration timeout, Context context)
  nameWithType: DataLakeFileClient.deleteWithResponse(DataLakeRequestConditions requestConditions, Duration timeout, Context context)
  fullName: com.azure.storage.file.datalake.DataLakeFileClient.deleteWithResponse(DataLakeRequestConditions requestConditions, Duration timeout, Context context)
  overload: com.azure.storage.file.datalake.DataLakeFileClient.deleteWithResponse*
  type: Method
  package: com.azure.storage.file.datalake
  summary: "Deletes a file.\n\n**Code Samples**\n\n```java\nDataLakeRequestConditions requestConditions = new DataLakeRequestConditions()\n     .setLeaseId(leaseId);\n \n client.deleteWithResponse(requestConditions, timeout, new Context(key1, value1));\n System.out.println(\"Delete request completed\");\n```\n\nFor more information see the [Azure Docs][]\n\n\n[Azure Docs]: https://docs.microsoft.com/en-us/rest/api/storageservices/datalakestoragegen2/path/delete"
  syntax:
    content: public Response<Void> deleteWithResponse(DataLakeRequestConditions requestConditions, Duration timeout, Context context)
    parameters:
    - id: requestConditions
      type: com.azure.storage.file.datalake.models.DataLakeRequestConditions
      description: <xref uid="com.azure.storage.file.datalake.models.DataLakeRequestConditions" data-throw-if-not-resolved="false">DataLakeRequestConditions</xref>
    - id: timeout
      type: java.time.Duration
      description: An optional timeout value beyond which a <xref uid="" data-throw-if-not-resolved="false">RuntimeException</xref> will be raised.
    - id: context
      type: com.azure.core.util.Context
      description: Additional context that is passed through the Http pipeline during the service call.
    return:
      type: com.azure.core.http.rest.Response<java.lang.Void>
      description: A response containing status code and HTTP headers.
- uid: com.azure.storage.file.datalake.DataLakeFileClient.flush(long)
  id: flush(long)
  artifact: com.azure:azure-storage-file-datalake:12.3.0
  parent: com.azure.storage.file.datalake.DataLakeFileClient
  langs:
  - java
  name: flush(long position)
  nameWithType: DataLakeFileClient.flush(long position)
  fullName: com.azure.storage.file.datalake.DataLakeFileClient.flush(long position)
  overload: com.azure.storage.file.datalake.DataLakeFileClient.flush*
  type: Method
  package: com.azure.storage.file.datalake
  summary: >-
    Flushes (writes) data previously appended to the file through a call to append. The previously uploaded data must be contiguous.


    By default this method will not overwrite existing data.


    **Code Samples**


    ```java

    client.flush(position);
     System.out.println("Flush data completed");
    ```


    For more information, see the [Azure Docs][]



    [Azure Docs]: https://docs.microsoft.com/en-us/rest/api/storageservices/datalakestoragegen2/path/update
  syntax:
    content: public PathInfo flush(long position)
    parameters:
    - id: position
      type: long
      description: The length of the file after all data has been written.
    return:
      type: com.azure.storage.file.datalake.models.PathInfo
      description: Information about the created resource.
- uid: com.azure.storage.file.datalake.DataLakeFileClient.flush(long,boolean)
  id: flush(long,boolean)
  artifact: com.azure:azure-storage-file-datalake:12.3.0
  parent: com.azure.storage.file.datalake.DataLakeFileClient
  langs:
  - java
  name: flush(long position, boolean overwrite)
  nameWithType: DataLakeFileClient.flush(long position, boolean overwrite)
  fullName: com.azure.storage.file.datalake.DataLakeFileClient.flush(long position, boolean overwrite)
  overload: com.azure.storage.file.datalake.DataLakeFileClient.flush*
  type: Method
  package: com.azure.storage.file.datalake
  summary: >-
    Flushes (writes) data previously appended to the file through a call to append. The previously uploaded data must be contiguous.


    **Code Samples**


    ```java

    boolean overwrite = true;
     client.flush(position, overwrite);
     System.out.println("Flush data completed");
    ```


    For more information, see the [Azure Docs][]



    [Azure Docs]: https://docs.microsoft.com/en-us/rest/api/storageservices/datalakestoragegen2/path/update
  syntax:
    content: public PathInfo flush(long position, boolean overwrite)
    parameters:
    - id: position
      type: long
      description: The length of the file after all data has been written.
    - id: overwrite
      type: boolean
      description: Whether or not to overwrite, should data exist on the file.
    return:
      type: com.azure.storage.file.datalake.models.PathInfo
      description: Information about the created resource.
- uid: com.azure.storage.file.datalake.DataLakeFileClient.flushWithResponse(long,boolean,boolean,com.azure.storage.file.datalake.models.PathHttpHeaders,com.azure.storage.file.datalake.models.DataLakeRequestConditions,java.time.Duration,com.azure.core.util.Context)
  id: flushWithResponse(long,boolean,boolean,com.azure.storage.file.datalake.models.PathHttpHeaders,com.azure.storage.file.datalake.models.DataLakeRequestConditions,java.time.Duration,com.azure.core.util.Context)
  artifact: com.azure:azure-storage-file-datalake:12.3.0
  parent: com.azure.storage.file.datalake.DataLakeFileClient
  langs:
  - java
  name: flushWithResponse(long position, boolean retainUncommittedData, boolean close, PathHttpHeaders httpHeaders, DataLakeRequestConditions requestConditions, Duration timeout, Context context)
  nameWithType: DataLakeFileClient.flushWithResponse(long position, boolean retainUncommittedData, boolean close, PathHttpHeaders httpHeaders, DataLakeRequestConditions requestConditions, Duration timeout, Context context)
  fullName: com.azure.storage.file.datalake.DataLakeFileClient.flushWithResponse(long position, boolean retainUncommittedData, boolean close, PathHttpHeaders httpHeaders, DataLakeRequestConditions requestConditions, Duration timeout, Context context)
  overload: com.azure.storage.file.datalake.DataLakeFileClient.flushWithResponse*
  type: Method
  package: com.azure.storage.file.datalake
  summary: "Flushes (writes) data previously appended to the file through a call to append. The previously uploaded data must be contiguous.\n\n**Code Samples**\n\n```java\nFileRange range = new FileRange(1024, 2048L);\n DownloadRetryOptions options = new DownloadRetryOptions().setMaxRetryRequests(5);\n byte[] contentMd5 = new byte[0]; // Replace with valid md5\n boolean retainUncommittedData = false;\n boolean close = false;\n PathHttpHeaders httpHeaders = new PathHttpHeaders()\n     .setContentLanguage(\"en-US\")\n     .setContentType(\"binary\");\n DataLakeRequestConditions requestConditions = new DataLakeRequestConditions()\n     .setLeaseId(leaseId);\n \n Response<PathInfo> response = client.flushWithResponse(position, retainUncommittedData, close, httpHeaders,\n     requestConditions, timeout, new Context(key1, value1));\n System.out.printf(\"Flush data completed with status %d%n\", response.getStatusCode());\n```\n\nFor more information, see the [Azure Docs][]\n\n\n[Azure Docs]: https://docs.microsoft.com/en-us/rest/api/storageservices/datalakestoragegen2/path/update"
  syntax:
    content: public Response<PathInfo> flushWithResponse(long position, boolean retainUncommittedData, boolean close, PathHttpHeaders httpHeaders, DataLakeRequestConditions requestConditions, Duration timeout, Context context)
    parameters:
    - id: position
      type: long
      description: The length of the file after all data has been written.
    - id: retainUncommittedData
      type: boolean
      description: Whether or not uncommitted data is to be retained after the operation.
    - id: close
      type: boolean
      description: Whether or not a file changed event raised indicates completion (true) or modification (false).
    - id: httpHeaders
      type: com.azure.storage.file.datalake.models.PathHttpHeaders
      description: <xref uid="com.azure.storage.file.datalake.models.PathHttpHeaders" data-throw-if-not-resolved="false">httpHeaders</xref>
    - id: requestConditions
      type: com.azure.storage.file.datalake.models.DataLakeRequestConditions
      description: <xref uid="com.azure.storage.file.datalake.models.DataLakeRequestConditions" data-throw-if-not-resolved="false">requestConditions</xref>
    - id: timeout
      type: java.time.Duration
      description: An optional timeout value beyond which a <xref uid="" data-throw-if-not-resolved="false">RuntimeException</xref> will be raised.
    - id: context
      type: com.azure.core.util.Context
      description: Additional context that is passed through the Http pipeline during the service call.
    return:
      type: com.azure.core.http.rest.Response<com.azure.storage.file.datalake.models.PathInfo>
      description: A response containing the information of the created resource.
- uid: com.azure.storage.file.datalake.DataLakeFileClient.getFileName()
  id: getFileName()
  artifact: com.azure:azure-storage-file-datalake:12.3.0
  parent: com.azure.storage.file.datalake.DataLakeFileClient
  langs:
  - java
  name: getFileName()
  nameWithType: DataLakeFileClient.getFileName()
  fullName: com.azure.storage.file.datalake.DataLakeFileClient.getFileName()
  overload: com.azure.storage.file.datalake.DataLakeFileClient.getFileName*
  type: Method
  package: com.azure.storage.file.datalake
  summary: Gets the name of this file, not including its full path.
  syntax:
    content: public String getFileName()
    return:
      type: java.lang.String
      description: The name of the file.
- uid: com.azure.storage.file.datalake.DataLakeFileClient.getFilePath()
  id: getFilePath()
  artifact: com.azure:azure-storage-file-datalake:12.3.0
  parent: com.azure.storage.file.datalake.DataLakeFileClient
  langs:
  - java
  name: getFilePath()
  nameWithType: DataLakeFileClient.getFilePath()
  fullName: com.azure.storage.file.datalake.DataLakeFileClient.getFilePath()
  overload: com.azure.storage.file.datalake.DataLakeFileClient.getFilePath*
  type: Method
  package: com.azure.storage.file.datalake
  summary: Gets the path of this file, not including the name of the resource itself.
  syntax:
    content: public String getFilePath()
    return:
      type: java.lang.String
      description: The path of the file.
- uid: com.azure.storage.file.datalake.DataLakeFileClient.getFileUrl()
  id: getFileUrl()
  artifact: com.azure:azure-storage-file-datalake:12.3.0
  parent: com.azure.storage.file.datalake.DataLakeFileClient
  langs:
  - java
  name: getFileUrl()
  nameWithType: DataLakeFileClient.getFileUrl()
  fullName: com.azure.storage.file.datalake.DataLakeFileClient.getFileUrl()
  overload: com.azure.storage.file.datalake.DataLakeFileClient.getFileUrl*
  type: Method
  package: com.azure.storage.file.datalake
  summary: Gets the URL of the file represented by this client on the Data Lake service.
  syntax:
    content: public String getFileUrl()
    return:
      type: java.lang.String
      description: the URL.
- uid: com.azure.storage.file.datalake.DataLakeFileClient.openQueryInputStream(java.lang.String)
  id: openQueryInputStream(java.lang.String)
  artifact: com.azure:azure-storage-file-datalake:12.3.0
  parent: com.azure.storage.file.datalake.DataLakeFileClient
  langs:
  - java
  name: openQueryInputStream(String expression)
  nameWithType: DataLakeFileClient.openQueryInputStream(String expression)
  fullName: com.azure.storage.file.datalake.DataLakeFileClient.openQueryInputStream(String expression)
  overload: com.azure.storage.file.datalake.DataLakeFileClient.openQueryInputStream*
  type: Method
  package: com.azure.storage.file.datalake
  summary: >-
    Opens an input stream to query the file.


    For more information, see the [Azure Docs][]


    **Code Samples**


    ```java

    String expression = "SELECT * from BlobStorage";
     InputStream inputStream = client.openQueryInputStream(expression);
     // Now you can read from the input stream like you would normally.
    ```



    [Azure Docs]: https://docs.microsoft.com/en-us/rest/api/storageservices/query-blob-contents
  syntax:
    content: public InputStream openQueryInputStream(String expression)
    parameters:
    - id: expression
      type: java.lang.String
      description: The query expression.
    return:
      type: java.io.InputStream
      description: An <code>InputStream</code> object that represents the stream to use for reading the query response.
- uid: com.azure.storage.file.datalake.DataLakeFileClient.openQueryInputStreamWithResponse(com.azure.storage.file.datalake.options.FileQueryOptions)
  id: openQueryInputStreamWithResponse(com.azure.storage.file.datalake.options.FileQueryOptions)
  artifact: com.azure:azure-storage-file-datalake:12.3.0
  parent: com.azure.storage.file.datalake.DataLakeFileClient
  langs:
  - java
  name: openQueryInputStreamWithResponse(FileQueryOptions queryOptions)
  nameWithType: DataLakeFileClient.openQueryInputStreamWithResponse(FileQueryOptions queryOptions)
  fullName: com.azure.storage.file.datalake.DataLakeFileClient.openQueryInputStreamWithResponse(FileQueryOptions queryOptions)
  overload: com.azure.storage.file.datalake.DataLakeFileClient.openQueryInputStreamWithResponse*
  type: Method
  package: com.azure.storage.file.datalake
  summary: "Opens an input stream to query the file.\n\nFor more information, see the [Azure Docs][]\n\n**Code Samples**\n\n```java\nString expression = \"SELECT * from BlobStorage\";\n FileQuerySerialization input = new FileQueryDelimitedSerialization()\n     .setColumnSeparator(',')\n     .setEscapeChar('\\n')\n     .setRecordSeparator('\\n')\n     .setHeadersPresent(true)\n     .setFieldQuote('\"');\n FileQuerySerialization output = new FileQueryJsonSerialization()\n     .setRecordSeparator('\\n');\n DataLakeRequestConditions requestConditions = new DataLakeRequestConditions()\n     .setLeaseId(\"leaseId\");\n Consumer<FileQueryError> errorConsumer = System.out::println;\n Consumer<FileQueryProgress> progressConsumer = progress -> System.out.println(\"total file bytes read: \"\n     + progress.getBytesScanned());\n FileQueryOptions queryOptions = new FileQueryOptions(expression)\n     .setInputSerialization(input)\n     .setOutputSerialization(output)\n     .setRequestConditions(requestConditions)\n     .setErrorConsumer(errorConsumer)\n     .setProgressConsumer(progressConsumer);\n \n InputStream inputStream = client.openQueryInputStreamWithResponse(queryOptions).getValue();\n // Now you can read from the input stream like you would normally.\n```\n\n\n[Azure Docs]: https://docs.microsoft.com/en-us/rest/api/storageservices/query-blob-contents"
  syntax:
    content: public Response<InputStream> openQueryInputStreamWithResponse(FileQueryOptions queryOptions)
    parameters:
    - id: queryOptions
      type: com.azure.storage.file.datalake.options.FileQueryOptions
      description: <xref uid="com.azure.storage.file.datalake.options.FileQueryOptions" data-throw-if-not-resolved="false">The query options</xref>.
    return:
      type: com.azure.core.http.rest.Response<java.io.InputStream>
      description: >-
        A response containing status code and HTTP headers including an <code>InputStream</code> object
         that represents the stream to use for reading the query response.
- uid: com.azure.storage.file.datalake.DataLakeFileClient.query(java.io.OutputStream,java.lang.String)
  id: query(java.io.OutputStream,java.lang.String)
  artifact: com.azure:azure-storage-file-datalake:12.3.0
  parent: com.azure.storage.file.datalake.DataLakeFileClient
  langs:
  - java
  name: query(OutputStream stream, String expression)
  nameWithType: DataLakeFileClient.query(OutputStream stream, String expression)
  fullName: com.azure.storage.file.datalake.DataLakeFileClient.query(OutputStream stream, String expression)
  overload: com.azure.storage.file.datalake.DataLakeFileClient.query*
  type: Method
  package: com.azure.storage.file.datalake
  summary: >-
    Queries an entire file into an output stream.


    For more information, see the [Azure Docs][]


    **Code Samples**


    ```java

    ByteArrayOutputStream queryData = new ByteArrayOutputStream();
     String expression = "SELECT * from BlobStorage";
     client.query(queryData, expression);
     System.out.println("Query completed.");
    ```



    [Azure Docs]: https://docs.microsoft.com/en-us/rest/api/storageservices/query-blob-contents
  syntax:
    content: public void query(OutputStream stream, String expression)
    parameters:
    - id: stream
      type: java.io.OutputStream
      description: A non-null <xref uid="java.io.OutputStream" data-throw-if-not-resolved="false">OutputStream</xref> instance where the downloaded data will be written.
    - id: expression
      type: java.lang.String
      description: The query expression.
- uid: com.azure.storage.file.datalake.DataLakeFileClient.queryWithResponse(com.azure.storage.file.datalake.options.FileQueryOptions,java.time.Duration,com.azure.core.util.Context)
  id: queryWithResponse(com.azure.storage.file.datalake.options.FileQueryOptions,java.time.Duration,com.azure.core.util.Context)
  artifact: com.azure:azure-storage-file-datalake:12.3.0
  parent: com.azure.storage.file.datalake.DataLakeFileClient
  langs:
  - java
  name: queryWithResponse(FileQueryOptions queryOptions, Duration timeout, Context context)
  nameWithType: DataLakeFileClient.queryWithResponse(FileQueryOptions queryOptions, Duration timeout, Context context)
  fullName: com.azure.storage.file.datalake.DataLakeFileClient.queryWithResponse(FileQueryOptions queryOptions, Duration timeout, Context context)
  overload: com.azure.storage.file.datalake.DataLakeFileClient.queryWithResponse*
  type: Method
  package: com.azure.storage.file.datalake
  summary: >-
    Queries an entire file into an output stream.


    For more information, see the [Azure Docs][]


    **Code Samples**


    ```java

    ByteArrayOutputStream queryData = new ByteArrayOutputStream();
     String expression = "SELECT * from BlobStorage";
     FileQueryJsonSerialization input = new FileQueryJsonSerialization()
         .setRecordSeparator('\n');
     FileQueryDelimitedSerialization output = new FileQueryDelimitedSerialization()
         .setEscapeChar('\0')
         .setColumnSeparator(',')
         .setRecordSeparator('\n')
         .setFieldQuote('\'')
         .setHeadersPresent(true);
     DataLakeRequestConditions requestConditions = new DataLakeRequestConditions().setLeaseId(leaseId);
     Consumer<FileQueryError> errorConsumer = System.out::println;
     Consumer<FileQueryProgress> progressConsumer = progress -> System.out.println("total file bytes read: "
         + progress.getBytesScanned());
     FileQueryOptions queryOptions = new FileQueryOptions(expression, queryData)
         .setInputSerialization(input)
         .setOutputSerialization(output)
         .setRequestConditions(requestConditions)
         .setErrorConsumer(errorConsumer)
         .setProgressConsumer(progressConsumer);
     System.out.printf("Query completed with status %d%n",
         client.queryWithResponse(queryOptions, timeout, new Context(key1, value1))
             .getStatusCode());
    ```



    [Azure Docs]: https://docs.microsoft.com/en-us/rest/api/storageservices/query-blob-contents
  syntax:
    content: public FileQueryResponse queryWithResponse(FileQueryOptions queryOptions, Duration timeout, Context context)
    parameters:
    - id: queryOptions
      type: com.azure.storage.file.datalake.options.FileQueryOptions
      description: <xref uid="com.azure.storage.file.datalake.options.FileQueryOptions" data-throw-if-not-resolved="false">The query options</xref>.
    - id: timeout
      type: java.time.Duration
      description: An optional timeout value beyond which a <xref uid="" data-throw-if-not-resolved="false">RuntimeException</xref> will be raised.
    - id: context
      type: com.azure.core.util.Context
      description: Additional context that is passed through the Http pipeline during the service call.
    return:
      type: com.azure.storage.file.datalake.models.FileQueryResponse
      description: A response containing status code and HTTP headers.
- uid: com.azure.storage.file.datalake.DataLakeFileClient.read(java.io.OutputStream)
  id: read(java.io.OutputStream)
  artifact: com.azure:azure-storage-file-datalake:12.3.0
  parent: com.azure.storage.file.datalake.DataLakeFileClient
  langs:
  - java
  name: read(OutputStream stream)
  nameWithType: DataLakeFileClient.read(OutputStream stream)
  fullName: com.azure.storage.file.datalake.DataLakeFileClient.read(OutputStream stream)
  overload: com.azure.storage.file.datalake.DataLakeFileClient.read*
  type: Method
  package: com.azure.storage.file.datalake
  summary: >-
    Reads the entire file into an output stream.


    **Code Samples**


    ```java

    client.read(new ByteArrayOutputStream());
     System.out.println("Download completed.");
    ```


    For more information, see the [Azure Docs][]



    [Azure Docs]: https://docs.microsoft.com/en-us/rest/api/storageservices/get-blob
  syntax:
    content: public void read(OutputStream stream)
    parameters:
    - id: stream
      type: java.io.OutputStream
      description: A non-null <xref uid="java.io.OutputStream" data-throw-if-not-resolved="false">OutputStream</xref> instance where the downloaded data will be written.
- uid: com.azure.storage.file.datalake.DataLakeFileClient.readToFile(java.lang.String)
  id: readToFile(java.lang.String)
  artifact: com.azure:azure-storage-file-datalake:12.3.0
  parent: com.azure.storage.file.datalake.DataLakeFileClient
  langs:
  - java
  name: readToFile(String filePath)
  nameWithType: DataLakeFileClient.readToFile(String filePath)
  fullName: com.azure.storage.file.datalake.DataLakeFileClient.readToFile(String filePath)
  overload: com.azure.storage.file.datalake.DataLakeFileClient.readToFile*
  type: Method
  package: com.azure.storage.file.datalake
  summary: >-
    Reads the entire file into a file specified by the path.


    The file will be created and must not exist, if the file already exists a <xref uid="" data-throw-if-not-resolved="false">FileAlreadyExistsException</xref> will be thrown.


    **Code Samples**


    ```java

    client.readToFile(file);
     System.out.println("Completed download to file");
    ```


    For more information, see the [Azure Docs][]



    [Azure Docs]: https://docs.microsoft.com/en-us/rest/api/storageservices/get-blob
  syntax:
    content: public PathProperties readToFile(String filePath)
    parameters:
    - id: filePath
      type: java.lang.String
      description: A <xref uid="java.lang.String" data-throw-if-not-resolved="false">String</xref> representing the filePath where the downloaded data will be written.
    return:
      type: com.azure.storage.file.datalake.models.PathProperties
      description: The file properties and metadata.
- uid: com.azure.storage.file.datalake.DataLakeFileClient.readToFile(java.lang.String,boolean)
  id: readToFile(java.lang.String,boolean)
  artifact: com.azure:azure-storage-file-datalake:12.3.0
  parent: com.azure.storage.file.datalake.DataLakeFileClient
  langs:
  - java
  name: readToFile(String filePath, boolean overwrite)
  nameWithType: DataLakeFileClient.readToFile(String filePath, boolean overwrite)
  fullName: com.azure.storage.file.datalake.DataLakeFileClient.readToFile(String filePath, boolean overwrite)
  overload: com.azure.storage.file.datalake.DataLakeFileClient.readToFile*
  type: Method
  package: com.azure.storage.file.datalake
  summary: >-
    Reads the entire file into a file specified by the path.


    If overwrite is set to false, the file will be created and must not exist, if the file already exists a <xref uid="" data-throw-if-not-resolved="false">FileAlreadyExistsException</xref> will be thrown.


    **Code Samples**


    ```java

    boolean overwrite = false; // Default value
     client.readToFile(file, overwrite);
     System.out.println("Completed download to file");
    ```


    For more information, see the [Azure Docs][]



    [Azure Docs]: https://docs.microsoft.com/en-us/rest/api/storageservices/get-blob
  syntax:
    content: public PathProperties readToFile(String filePath, boolean overwrite)
    parameters:
    - id: filePath
      type: java.lang.String
      description: A <xref uid="java.lang.String" data-throw-if-not-resolved="false">String</xref> representing the filePath where the downloaded data will be written.
    - id: overwrite
      type: boolean
      description: Whether or not to overwrite the file, should the file exist.
    return:
      type: com.azure.storage.file.datalake.models.PathProperties
      description: The file properties and metadata.
- uid: com.azure.storage.file.datalake.DataLakeFileClient.readToFileWithResponse(java.lang.String,com.azure.storage.file.datalake.models.FileRange,com.azure.storage.common.ParallelTransferOptions,com.azure.storage.file.datalake.models.DownloadRetryOptions,com.azure.storage.file.datalake.models.DataLakeRequestConditions,boolean,java.util.Set<java.nio.file.OpenOption>,java.time.Duration,com.azure.core.util.Context)
  id: readToFileWithResponse(java.lang.String,com.azure.storage.file.datalake.models.FileRange,com.azure.storage.common.ParallelTransferOptions,com.azure.storage.file.datalake.models.DownloadRetryOptions,com.azure.storage.file.datalake.models.DataLakeRequestConditions,boolean,java.util.Set<java.nio.file.OpenOption>,java.time.Duration,com.azure.core.util.Context)
  artifact: com.azure:azure-storage-file-datalake:12.3.0
  parent: com.azure.storage.file.datalake.DataLakeFileClient
  langs:
  - java
  name: readToFileWithResponse(String filePath, FileRange range, ParallelTransferOptions parallelTransferOptions, DownloadRetryOptions downloadRetryOptions, DataLakeRequestConditions requestConditions, boolean rangeGetContentMd5, Set<OpenOption> openOptions, Duration timeout, Context context)
  nameWithType: DataLakeFileClient.readToFileWithResponse(String filePath, FileRange range, ParallelTransferOptions parallelTransferOptions, DownloadRetryOptions downloadRetryOptions, DataLakeRequestConditions requestConditions, boolean rangeGetContentMd5, Set<OpenOption> openOptions, Duration timeout, Context context)
  fullName: com.azure.storage.file.datalake.DataLakeFileClient.readToFileWithResponse(String filePath, FileRange range, ParallelTransferOptions parallelTransferOptions, DownloadRetryOptions downloadRetryOptions, DataLakeRequestConditions requestConditions, boolean rangeGetContentMd5, Set<OpenOption> openOptions, Duration timeout, Context context)
  overload: com.azure.storage.file.datalake.DataLakeFileClient.readToFileWithResponse*
  type: Method
  package: com.azure.storage.file.datalake
  summary: "Reads the entire file into a file specified by the path.\n\nBy default the file will be created and must not exist, if the file already exists a <xref uid=\"\" data-throw-if-not-resolved=\"false\">FileAlreadyExistsException</xref> will be thrown. To override this behavior, provide appropriate <xref uid=\"java.nio.file.OpenOption\" data-throw-if-not-resolved=\"false\">OpenOptions</xref>\n\n**Code Samples**\n\n```java\nFileRange fileRange = new FileRange(1024, 2048L);\n DownloadRetryOptions downloadRetryOptions = new DownloadRetryOptions().setMaxRetryRequests(5);\n Set<OpenOption> openOptions = new HashSet<>(Arrays.asList(StandardOpenOption.CREATE_NEW,\n     StandardOpenOption.WRITE, StandardOpenOption.READ)); // Default options\n \n client.readToFileWithResponse(file, fileRange, new ParallelTransferOptions().setBlockSizeLong(4L * Constants.MB),\n     downloadRetryOptions, null, false, openOptions, timeout, new Context(key2, value2));\n System.out.println(\"Completed download to file\");\n```\n\nFor more information, see the [Azure Docs][]\n\n\n[Azure Docs]: https://docs.microsoft.com/en-us/rest/api/storageservices/get-blob"
  syntax:
    content: public Response<PathProperties> readToFileWithResponse(String filePath, FileRange range, ParallelTransferOptions parallelTransferOptions, DownloadRetryOptions downloadRetryOptions, DataLakeRequestConditions requestConditions, boolean rangeGetContentMd5, Set<OpenOption> openOptions, Duration timeout, Context context)
    parameters:
    - id: filePath
      type: java.lang.String
      description: A <xref uid="java.lang.String" data-throw-if-not-resolved="false">String</xref> representing the filePath where the downloaded data will be written.
    - id: range
      type: com.azure.storage.file.datalake.models.FileRange
      description: <xref uid="com.azure.storage.file.datalake.models.FileRange" data-throw-if-not-resolved="false">FileRange</xref>
    - id: parallelTransferOptions
      type: com.azure.storage.common.ParallelTransferOptions
      description: >-
        <xref uid="com.azure.storage.common.ParallelTransferOptions" data-throw-if-not-resolved="false">ParallelTransferOptions</xref> to use to download to file. Number of parallel
         transfers parameter is ignored.
    - id: downloadRetryOptions
      type: com.azure.storage.file.datalake.models.DownloadRetryOptions
      description: <xref uid="com.azure.storage.file.datalake.models.DownloadRetryOptions" data-throw-if-not-resolved="false">DownloadRetryOptions</xref>
    - id: requestConditions
      type: com.azure.storage.file.datalake.models.DataLakeRequestConditions
      description: <xref uid="com.azure.storage.file.datalake.models.DataLakeRequestConditions" data-throw-if-not-resolved="false">DataLakeRequestConditions</xref>
    - id: rangeGetContentMd5
      type: boolean
      description: Whether the contentMD5 for the specified file range should be returned.
    - id: openOptions
      type: java.util.Set<java.nio.file.OpenOption>
      description: <xref uid="java.nio.file.OpenOption" data-throw-if-not-resolved="false">OpenOptions</xref> to use to configure how to open or create the file.
    - id: timeout
      type: java.time.Duration
      description: An optional timeout value beyond which a <xref uid="" data-throw-if-not-resolved="false">RuntimeException</xref> will be raised.
    - id: context
      type: com.azure.core.util.Context
      description: Additional context that is passed through the Http pipeline during the service call.
    return:
      type: com.azure.core.http.rest.Response<com.azure.storage.file.datalake.models.PathProperties>
      description: A response containing the file properties and metadata.
- uid: com.azure.storage.file.datalake.DataLakeFileClient.readWithResponse(java.io.OutputStream,com.azure.storage.file.datalake.models.FileRange,com.azure.storage.file.datalake.models.DownloadRetryOptions,com.azure.storage.file.datalake.models.DataLakeRequestConditions,boolean,java.time.Duration,com.azure.core.util.Context)
  id: readWithResponse(java.io.OutputStream,com.azure.storage.file.datalake.models.FileRange,com.azure.storage.file.datalake.models.DownloadRetryOptions,com.azure.storage.file.datalake.models.DataLakeRequestConditions,boolean,java.time.Duration,com.azure.core.util.Context)
  artifact: com.azure:azure-storage-file-datalake:12.3.0
  parent: com.azure.storage.file.datalake.DataLakeFileClient
  langs:
  - java
  name: readWithResponse(OutputStream stream, FileRange range, DownloadRetryOptions options, DataLakeRequestConditions requestConditions, boolean getRangeContentMd5, Duration timeout, Context context)
  nameWithType: DataLakeFileClient.readWithResponse(OutputStream stream, FileRange range, DownloadRetryOptions options, DataLakeRequestConditions requestConditions, boolean getRangeContentMd5, Duration timeout, Context context)
  fullName: com.azure.storage.file.datalake.DataLakeFileClient.readWithResponse(OutputStream stream, FileRange range, DownloadRetryOptions options, DataLakeRequestConditions requestConditions, boolean getRangeContentMd5, Duration timeout, Context context)
  overload: com.azure.storage.file.datalake.DataLakeFileClient.readWithResponse*
  type: Method
  package: com.azure.storage.file.datalake
  summary: "Reads a range of bytes from a file into an output stream.\n\n**Code Samples**\n\n```java\nFileRange range = new FileRange(1024, 2048L);\n DownloadRetryOptions options = new DownloadRetryOptions().setMaxRetryRequests(5);\n \n System.out.printf(\"Download completed with status %d%n\",\n     client.readWithResponse(new ByteArrayOutputStream(), range, options, null, false,\n         timeout, new Context(key2, value2)).getStatusCode());\n```\n\nFor more information, see the [Azure Docs][]\n\n\n[Azure Docs]: https://docs.microsoft.com/en-us/rest/api/storageservices/get-blob"
  syntax:
    content: public FileReadResponse readWithResponse(OutputStream stream, FileRange range, DownloadRetryOptions options, DataLakeRequestConditions requestConditions, boolean getRangeContentMd5, Duration timeout, Context context)
    parameters:
    - id: stream
      type: java.io.OutputStream
      description: A non-null <xref uid="java.io.OutputStream" data-throw-if-not-resolved="false">OutputStream</xref> instance where the downloaded data will be written.
    - id: range
      type: com.azure.storage.file.datalake.models.FileRange
      description: <xref uid="com.azure.storage.file.datalake.models.FileRange" data-throw-if-not-resolved="false">FileRange</xref>
    - id: options
      type: com.azure.storage.file.datalake.models.DownloadRetryOptions
      description: <xref uid="com.azure.storage.file.datalake.models.DownloadRetryOptions" data-throw-if-not-resolved="false">DownloadRetryOptions</xref>
    - id: requestConditions
      type: com.azure.storage.file.datalake.models.DataLakeRequestConditions
      description: <xref uid="com.azure.storage.file.datalake.models.DataLakeRequestConditions" data-throw-if-not-resolved="false">DataLakeRequestConditions</xref>
    - id: getRangeContentMd5
      type: boolean
      description: Whether the contentMD5 for the specified file range should be returned.
    - id: timeout
      type: java.time.Duration
      description: An optional timeout value beyond which a <xref uid="" data-throw-if-not-resolved="false">RuntimeException</xref> will be raised.
    - id: context
      type: com.azure.core.util.Context
      description: Additional context that is passed through the Http pipeline during the service call.
    return:
      type: com.azure.storage.file.datalake.models.FileReadResponse
      description: A response containing status code and HTTP headers.
- uid: com.azure.storage.file.datalake.DataLakeFileClient.rename(java.lang.String,java.lang.String)
  id: rename(java.lang.String,java.lang.String)
  artifact: com.azure:azure-storage-file-datalake:12.3.0
  parent: com.azure.storage.file.datalake.DataLakeFileClient
  langs:
  - java
  name: rename(String destinationFileSystem, String destinationPath)
  nameWithType: DataLakeFileClient.rename(String destinationFileSystem, String destinationPath)
  fullName: com.azure.storage.file.datalake.DataLakeFileClient.rename(String destinationFileSystem, String destinationPath)
  overload: com.azure.storage.file.datalake.DataLakeFileClient.rename*
  type: Method
  package: com.azure.storage.file.datalake
  summary: >-
    Moves the file to another location within the file system. For more information see the [Azure Docs][].


    **Code Samples**


    ```java

    DataLakeDirectoryAsyncClient renamedClient = client.rename(fileSystemName, destinationPath).block();
     System.out.println("Directory Client has been renamed");
    ```



    [Azure Docs]: https://docs.microsoft.com/en-us/rest/api/storageservices/datalakestoragegen2/path/create
  syntax:
    content: public DataLakeFileClient rename(String destinationFileSystem, String destinationPath)
    parameters:
    - id: destinationFileSystem
      type: java.lang.String
      description: >-
        The file system of the destination within the account.
         <code>null</code> for the current file system.
    - id: destinationPath
      type: java.lang.String
      description: >-
        Relative path from the file system to rename the file to, excludes the file system name.
         For example if you want to move a file with fileSystem = "myfilesystem", path = "mydir/hello.txt" to another path
         in myfilesystem (ex: newdir/hi.txt) then set the destinationPath = "newdir/hi.txt"
    return:
      type: com.azure.storage.file.datalake.DataLakeFileClient
      description: A <xref uid="com.azure.storage.file.datalake.DataLakeFileClient" data-throw-if-not-resolved="false">DataLakeFileClient</xref> used to interact with the new file created.
- uid: com.azure.storage.file.datalake.DataLakeFileClient.renameWithResponse(java.lang.String,java.lang.String,com.azure.storage.file.datalake.models.DataLakeRequestConditions,com.azure.storage.file.datalake.models.DataLakeRequestConditions,java.time.Duration,com.azure.core.util.Context)
  id: renameWithResponse(java.lang.String,java.lang.String,com.azure.storage.file.datalake.models.DataLakeRequestConditions,com.azure.storage.file.datalake.models.DataLakeRequestConditions,java.time.Duration,com.azure.core.util.Context)
  artifact: com.azure:azure-storage-file-datalake:12.3.0
  parent: com.azure.storage.file.datalake.DataLakeFileClient
  langs:
  - java
  name: renameWithResponse(String destinationFileSystem, String destinationPath, DataLakeRequestConditions sourceRequestConditions, DataLakeRequestConditions destinationRequestConditions, Duration timeout, Context context)
  nameWithType: DataLakeFileClient.renameWithResponse(String destinationFileSystem, String destinationPath, DataLakeRequestConditions sourceRequestConditions, DataLakeRequestConditions destinationRequestConditions, Duration timeout, Context context)
  fullName: com.azure.storage.file.datalake.DataLakeFileClient.renameWithResponse(String destinationFileSystem, String destinationPath, DataLakeRequestConditions sourceRequestConditions, DataLakeRequestConditions destinationRequestConditions, Duration timeout, Context context)
  overload: com.azure.storage.file.datalake.DataLakeFileClient.renameWithResponse*
  type: Method
  package: com.azure.storage.file.datalake
  summary: "Moves the file to another location within the file system. For more information, see the [Azure Docs][].\n\n**Code Samples**\n\n```java\nDataLakeRequestConditions sourceRequestConditions = new DataLakeRequestConditions()\n     .setLeaseId(leaseId);\n DataLakeRequestConditions destinationRequestConditions = new DataLakeRequestConditions();\n \n DataLakeFileClient newRenamedClient = client.renameWithResponse(fileSystemName, destinationPath,\n     sourceRequestConditions, destinationRequestConditions, timeout, new Context(key1, value1)).getValue();\n System.out.println(\"Directory Client has been renamed\");\n```\n\n\n[Azure Docs]: https://docs.microsoft.com/en-us/rest/api/storageservices/datalakestoragegen2/path/create"
  syntax:
    content: public Response<DataLakeFileClient> renameWithResponse(String destinationFileSystem, String destinationPath, DataLakeRequestConditions sourceRequestConditions, DataLakeRequestConditions destinationRequestConditions, Duration timeout, Context context)
    parameters:
    - id: destinationFileSystem
      type: java.lang.String
      description: >-
        The file system of the destination within the account.
         <code>null</code> for the current file system.
    - id: destinationPath
      type: java.lang.String
      description: >-
        Relative path from the file system to rename the file to, excludes the file system name.
         For example if you want to move a file with fileSystem = "myfilesystem", path = "mydir/hello.txt" to another path
         in myfilesystem (ex: newdir/hi.txt) then set the destinationPath = "newdir/hi.txt"
    - id: sourceRequestConditions
      type: com.azure.storage.file.datalake.models.DataLakeRequestConditions
      description: <xref uid="com.azure.storage.file.datalake.models.DataLakeRequestConditions" data-throw-if-not-resolved="false">DataLakeRequestConditions</xref> against the source.
    - id: destinationRequestConditions
      type: com.azure.storage.file.datalake.models.DataLakeRequestConditions
      description: <xref uid="com.azure.storage.file.datalake.models.DataLakeRequestConditions" data-throw-if-not-resolved="false">DataLakeRequestConditions</xref> against the destination.
    - id: timeout
      type: java.time.Duration
      description: An optional timeout value beyond which a <xref uid="" data-throw-if-not-resolved="false">RuntimeException</xref> will be raised.
    - id: context
      type: com.azure.core.util.Context
      description: Additional context that is passed through the Http pipeline during the service call.
    return:
      type: com.azure.core.http.rest.Response<com.azure.storage.file.datalake.DataLakeFileClient>
      description: >-
        A <xref uid="com.azure.core.http.rest.Response" data-throw-if-not-resolved="false">Response</xref> whose <xref uid="com.azure.core.http.rest.Response.getValue*" data-throw-if-not-resolved="false">value</xref> that contains a <xref uid="com.azure.storage.file.datalake.DataLakeFileClient" data-throw-if-not-resolved="false">DataLakeFileClient</xref>
         used to interact with the file created.
- uid: com.azure.storage.file.datalake.DataLakeFileClient.scheduleDeletion(com.azure.storage.file.datalake.options.FileScheduleDeletionOptions)
  id: scheduleDeletion(com.azure.storage.file.datalake.options.FileScheduleDeletionOptions)
  artifact: com.azure:azure-storage-file-datalake:12.3.0
  parent: com.azure.storage.file.datalake.DataLakeFileClient
  langs:
  - java
  name: scheduleDeletion(FileScheduleDeletionOptions options)
  nameWithType: DataLakeFileClient.scheduleDeletion(FileScheduleDeletionOptions options)
  fullName: com.azure.storage.file.datalake.DataLakeFileClient.scheduleDeletion(FileScheduleDeletionOptions options)
  overload: com.azure.storage.file.datalake.DataLakeFileClient.scheduleDeletion*
  type: Method
  package: com.azure.storage.file.datalake
  summary: >-
    Schedules the file for deletion.


    **Code Samples**


    ```java

    FileScheduleDeletionOptions options = new FileScheduleDeletionOptions(OffsetDateTime.now().plusDays(1));
     client.scheduleDeletion(options);
     System.out.println("File deletion has been scheduled");
    ```
  syntax:
    content: public void scheduleDeletion(FileScheduleDeletionOptions options)
    parameters:
    - id: options
      type: com.azure.storage.file.datalake.options.FileScheduleDeletionOptions
      description: Schedule deletion parameters.
- uid: com.azure.storage.file.datalake.DataLakeFileClient.scheduleDeletionWithResponse(com.azure.storage.file.datalake.options.FileScheduleDeletionOptions,java.time.Duration,com.azure.core.util.Context)
  id: scheduleDeletionWithResponse(com.azure.storage.file.datalake.options.FileScheduleDeletionOptions,java.time.Duration,com.azure.core.util.Context)
  artifact: com.azure:azure-storage-file-datalake:12.3.0
  parent: com.azure.storage.file.datalake.DataLakeFileClient
  langs:
  - java
  name: scheduleDeletionWithResponse(FileScheduleDeletionOptions options, Duration timeout, Context context)
  nameWithType: DataLakeFileClient.scheduleDeletionWithResponse(FileScheduleDeletionOptions options, Duration timeout, Context context)
  fullName: com.azure.storage.file.datalake.DataLakeFileClient.scheduleDeletionWithResponse(FileScheduleDeletionOptions options, Duration timeout, Context context)
  overload: com.azure.storage.file.datalake.DataLakeFileClient.scheduleDeletionWithResponse*
  type: Method
  package: com.azure.storage.file.datalake
  summary: "Schedules the file for deletion.\n\n**Code Samples**\n\n```java\nFileScheduleDeletionOptions options = new FileScheduleDeletionOptions(OffsetDateTime.now().plusDays(1));\n Context context = new Context(\"key\", \"value\");\n \n client.scheduleDeletionWithResponse(options, timeout, context);\n System.out.println(\"File deletion has been scheduled\");\n```"
  syntax:
    content: public Response<Void> scheduleDeletionWithResponse(FileScheduleDeletionOptions options, Duration timeout, Context context)
    parameters:
    - id: options
      type: com.azure.storage.file.datalake.options.FileScheduleDeletionOptions
      description: Schedule deletion parameters.
    - id: timeout
      type: java.time.Duration
      description: An optional timeout value beyond which a <xref uid="" data-throw-if-not-resolved="false">RuntimeException</xref> will be raised.
    - id: context
      type: com.azure.core.util.Context
      description: Additional context that is passed through the Http pipeline during the service call.
    return:
      type: com.azure.core.http.rest.Response<java.lang.Void>
      description: A response containing status code and HTTP headers.
- uid: com.azure.storage.file.datalake.DataLakeFileClient.upload(java.io.InputStream,long)
  id: upload(java.io.InputStream,long)
  artifact: com.azure:azure-storage-file-datalake:12.3.0
  parent: com.azure.storage.file.datalake.DataLakeFileClient
  langs:
  - java
  name: upload(InputStream data, long length)
  nameWithType: DataLakeFileClient.upload(InputStream data, long length)
  fullName: com.azure.storage.file.datalake.DataLakeFileClient.upload(InputStream data, long length)
  overload: com.azure.storage.file.datalake.DataLakeFileClient.upload*
  type: Method
  package: com.azure.storage.file.datalake
  summary: >-
    Creates a new file. By default this method will not overwrite an existing file.


    **Code Samples**


    ```java

    try {
         client.upload(data, length);
         System.out.println("Upload from file succeeded");
     } catch (UncheckedIOException ex) {
         System.err.printf("Failed to upload from file %s%n", ex.getMessage());
     }
    ```
  syntax:
    content: public PathInfo upload(InputStream data, long length)
    parameters:
    - id: data
      type: java.io.InputStream
      description: >-
        The data to write to the blob. The data must be markable. This is in order to support retries. If
         the data is not markable, consider wrapping your data source in a <xref uid="" data-throw-if-not-resolved="false">java.io.BufferedInputStream</xref> to add mark
         support.
    - id: length
      type: long
      description: >-
        The exact length of the data. It is important that this value match precisely the length of the
         data provided in the <xref uid="java.io.InputStream" data-throw-if-not-resolved="false">InputStream</xref>.
    return:
      type: com.azure.storage.file.datalake.models.PathInfo
      description: Information about the uploaded path.
- uid: com.azure.storage.file.datalake.DataLakeFileClient.upload(java.io.InputStream,long,boolean)
  id: upload(java.io.InputStream,long,boolean)
  artifact: com.azure:azure-storage-file-datalake:12.3.0
  parent: com.azure.storage.file.datalake.DataLakeFileClient
  langs:
  - java
  name: upload(InputStream data, long length, boolean overwrite)
  nameWithType: DataLakeFileClient.upload(InputStream data, long length, boolean overwrite)
  fullName: com.azure.storage.file.datalake.DataLakeFileClient.upload(InputStream data, long length, boolean overwrite)
  overload: com.azure.storage.file.datalake.DataLakeFileClient.upload*
  type: Method
  package: com.azure.storage.file.datalake
  summary: >-
    Creates a new file, or updates the content of an existing file.


    **Code Samples**


    ```java

    try {
         boolean overwrite = false;
         client.upload(data, length, overwrite);
         System.out.println("Upload from file succeeded");
     } catch (UncheckedIOException ex) {
         System.err.printf("Failed to upload from file %s%n", ex.getMessage());
     }
    ```
  syntax:
    content: public PathInfo upload(InputStream data, long length, boolean overwrite)
    parameters:
    - id: data
      type: java.io.InputStream
      description: >-
        The data to write to the blob. The data must be markable. This is in order to support retries. If
         the data is not markable, consider wrapping your data source in a <xref uid="" data-throw-if-not-resolved="false">java.io.BufferedInputStream</xref> to add mark
         support.
    - id: length
      type: long
      description: >-
        The exact length of the data. It is important that this value match precisely the length of the
         data provided in the <xref uid="java.io.InputStream" data-throw-if-not-resolved="false">InputStream</xref>.
    - id: overwrite
      type: boolean
      description: Whether or not to overwrite, should data exist on the bfilelob.
    return:
      type: com.azure.storage.file.datalake.models.PathInfo
      description: Information about the uploaded path.
- uid: com.azure.storage.file.datalake.DataLakeFileClient.uploadFromFile(java.lang.String)
  id: uploadFromFile(java.lang.String)
  artifact: com.azure:azure-storage-file-datalake:12.3.0
  parent: com.azure.storage.file.datalake.DataLakeFileClient
  langs:
  - java
  name: uploadFromFile(String filePath)
  nameWithType: DataLakeFileClient.uploadFromFile(String filePath)
  fullName: com.azure.storage.file.datalake.DataLakeFileClient.uploadFromFile(String filePath)
  overload: com.azure.storage.file.datalake.DataLakeFileClient.uploadFromFile*
  type: Method
  package: com.azure.storage.file.datalake
  summary: >-
    Creates a file, with the content of the specified file. By default this method will not overwrite an existing file.


    **Code Samples**


    ```java

    try {
         client.uploadFromFile(filePath);
         System.out.println("Upload from file succeeded");
     } catch (UncheckedIOException ex) {
         System.err.printf("Failed to upload from file %s%n", ex.getMessage());
     }
    ```
  syntax:
    content: public void uploadFromFile(String filePath)
    parameters:
    - id: filePath
      type: java.lang.String
      description: Path of the file to upload
- uid: com.azure.storage.file.datalake.DataLakeFileClient.uploadFromFile(java.lang.String,boolean)
  id: uploadFromFile(java.lang.String,boolean)
  artifact: com.azure:azure-storage-file-datalake:12.3.0
  parent: com.azure.storage.file.datalake.DataLakeFileClient
  langs:
  - java
  name: uploadFromFile(String filePath, boolean overwrite)
  nameWithType: DataLakeFileClient.uploadFromFile(String filePath, boolean overwrite)
  fullName: com.azure.storage.file.datalake.DataLakeFileClient.uploadFromFile(String filePath, boolean overwrite)
  overload: com.azure.storage.file.datalake.DataLakeFileClient.uploadFromFile*
  type: Method
  package: com.azure.storage.file.datalake
  summary: >-
    Creates a file, with the content of the specified file.


    **Code Samples**


    ```java

    try {
         boolean overwrite = false;
         client.uploadFromFile(filePath, overwrite);
         System.out.println("Upload from file succeeded");
     } catch (UncheckedIOException ex) {
         System.err.printf("Failed to upload from file %s%n", ex.getMessage());
     }
    ```
  syntax:
    content: public void uploadFromFile(String filePath, boolean overwrite)
    parameters:
    - id: filePath
      type: java.lang.String
      description: Path of the file to upload
    - id: overwrite
      type: boolean
      description: Whether or not to overwrite, should the file already exist
- uid: com.azure.storage.file.datalake.DataLakeFileClient.uploadFromFile(java.lang.String,com.azure.storage.common.ParallelTransferOptions,com.azure.storage.file.datalake.models.PathHttpHeaders,java.util.Map<java.lang.String,java.lang.String>,com.azure.storage.file.datalake.models.DataLakeRequestConditions,java.time.Duration)
  id: uploadFromFile(java.lang.String,com.azure.storage.common.ParallelTransferOptions,com.azure.storage.file.datalake.models.PathHttpHeaders,java.util.Map<java.lang.String,java.lang.String>,com.azure.storage.file.datalake.models.DataLakeRequestConditions,java.time.Duration)
  artifact: com.azure:azure-storage-file-datalake:12.3.0
  parent: com.azure.storage.file.datalake.DataLakeFileClient
  langs:
  - java
  name: uploadFromFile(String filePath, ParallelTransferOptions parallelTransferOptions, PathHttpHeaders headers, Map<String,String> metadata, DataLakeRequestConditions requestConditions, Duration timeout)
  nameWithType: DataLakeFileClient.uploadFromFile(String filePath, ParallelTransferOptions parallelTransferOptions, PathHttpHeaders headers, Map<String,String> metadata, DataLakeRequestConditions requestConditions, Duration timeout)
  fullName: com.azure.storage.file.datalake.DataLakeFileClient.uploadFromFile(String filePath, ParallelTransferOptions parallelTransferOptions, PathHttpHeaders headers, Map<String,String> metadata, DataLakeRequestConditions requestConditions, Duration timeout)
  overload: com.azure.storage.file.datalake.DataLakeFileClient.uploadFromFile*
  type: Method
  package: com.azure.storage.file.datalake
  summary: "Creates a file, with the content of the specified file.\n\nTo avoid overwriting, pass \"\\*\" to <xref uid=\"com.azure.storage.file.datalake.models.DataLakeRequestConditions.setIfNoneMatch(java.lang.String)\" data-throw-if-not-resolved=\"false\">DataLakeRequestConditions#setIfNoneMatch(String)</xref>.\n\n**Code Samples**\n\n```java\nPathHttpHeaders headers = new PathHttpHeaders()\n     .setContentMd5(\"data\".getBytes(StandardCharsets.UTF_8))\n     .setContentLanguage(\"en-US\")\n     .setContentType(\"binary\");\n \n Map<String, String> metadata = Collections.singletonMap(\"metadata\", \"value\");\n DataLakeRequestConditions requestConditions = new DataLakeRequestConditions()\n     .setLeaseId(leaseId)\n     .setIfUnmodifiedSince(OffsetDateTime.now().minusDays(3));\n Long blockSize = 100L * 1024L * 1024L; // 100 MB;\n ParallelTransferOptions parallelTransferOptions = new ParallelTransferOptions().setBlockSizeLong(blockSize);\n \n try {\n     client.uploadFromFile(filePath, parallelTransferOptions, headers, metadata, requestConditions, timeout);\n     System.out.println(\"Upload from file succeeded\");\n } catch (UncheckedIOException ex) {\n     System.err.printf(\"Failed to upload from file %s%n\", ex.getMessage());\n }\n```"
  syntax:
    content: public void uploadFromFile(String filePath, ParallelTransferOptions parallelTransferOptions, PathHttpHeaders headers, Map<String,String> metadata, DataLakeRequestConditions requestConditions, Duration timeout)
    parameters:
    - id: filePath
      type: java.lang.String
      description: Path of the file to upload
    - id: parallelTransferOptions
      type: com.azure.storage.common.ParallelTransferOptions
      description: <xref uid="com.azure.storage.common.ParallelTransferOptions" data-throw-if-not-resolved="false">ParallelTransferOptions</xref> used to configure buffered uploading.
    - id: headers
      type: com.azure.storage.file.datalake.models.PathHttpHeaders
      description: <xref uid="com.azure.storage.file.datalake.models.PathHttpHeaders" data-throw-if-not-resolved="false">PathHttpHeaders</xref>
    - id: metadata
      type: java.util.Map<java.lang.String,java.lang.String>
      description: Metadata to associate with the resource.
    - id: requestConditions
      type: com.azure.storage.file.datalake.models.DataLakeRequestConditions
      description: <xref uid="com.azure.storage.file.datalake.models.DataLakeRequestConditions" data-throw-if-not-resolved="false">DataLakeRequestConditions</xref>
    - id: timeout
      type: java.time.Duration
      description: An optional timeout value beyond which a <xref uid="" data-throw-if-not-resolved="false">RuntimeException</xref> will be raised.
- uid: com.azure.storage.file.datalake.DataLakeFileClient.uploadWithResponse(com.azure.storage.file.datalake.options.FileParallelUploadOptions,java.time.Duration,com.azure.core.util.Context)
  id: uploadWithResponse(com.azure.storage.file.datalake.options.FileParallelUploadOptions,java.time.Duration,com.azure.core.util.Context)
  artifact: com.azure:azure-storage-file-datalake:12.3.0
  parent: com.azure.storage.file.datalake.DataLakeFileClient
  langs:
  - java
  name: uploadWithResponse(FileParallelUploadOptions options, Duration timeout, Context context)
  nameWithType: DataLakeFileClient.uploadWithResponse(FileParallelUploadOptions options, Duration timeout, Context context)
  fullName: com.azure.storage.file.datalake.DataLakeFileClient.uploadWithResponse(FileParallelUploadOptions options, Duration timeout, Context context)
  overload: com.azure.storage.file.datalake.DataLakeFileClient.uploadWithResponse*
  type: Method
  package: com.azure.storage.file.datalake
  summary: "Creates a new file.\n\nTo avoid overwriting, pass \"\\*\" to <xref uid=\"com.azure.storage.file.datalake.models.DataLakeRequestConditions.setIfNoneMatch(java.lang.String)\" data-throw-if-not-resolved=\"false\">DataLakeRequestConditions#setIfNoneMatch(String)</xref>.\n\n**Code Samples**\n\n```java\nPathHttpHeaders headers = new PathHttpHeaders()\n     .setContentMd5(\"data\".getBytes(StandardCharsets.UTF_8))\n     .setContentLanguage(\"en-US\")\n     .setContentType(\"binary\");\n \n Map<String, String> metadata = Collections.singletonMap(\"metadata\", \"value\");\n DataLakeRequestConditions requestConditions = new DataLakeRequestConditions()\n     .setLeaseId(leaseId)\n     .setIfUnmodifiedSince(OffsetDateTime.now().minusDays(3));\n Long blockSize = 100L * 1024L * 1024L; // 100 MB;\n ParallelTransferOptions parallelTransferOptions = new ParallelTransferOptions().setBlockSizeLong(blockSize);\n \n try {\n     client.uploadWithResponse(new FileParallelUploadOptions(data, length)\n         .setParallelTransferOptions(parallelTransferOptions).setHeaders(headers)\n         .setMetadata(metadata).setRequestConditions(requestConditions)\n         .setPermissions(\"permissions\").setUmask(\"umask\"), timeout, new Context(\"key\", \"value\"));\n     System.out.println(\"Upload from file succeeded\");\n } catch (UncheckedIOException ex) {\n     System.err.printf(\"Failed to upload from file %s%n\", ex.getMessage());\n }\n```"
  syntax:
    content: public Response<PathInfo> uploadWithResponse(FileParallelUploadOptions options, Duration timeout, Context context)
    parameters:
    - id: options
      type: com.azure.storage.file.datalake.options.FileParallelUploadOptions
      description: <xref uid="com.azure.storage.file.datalake.options.FileParallelUploadOptions" data-throw-if-not-resolved="false">FileParallelUploadOptions</xref>
    - id: timeout
      type: java.time.Duration
      description: An optional timeout value beyond which a <xref uid="" data-throw-if-not-resolved="false">RuntimeException</xref> will be raised.
    - id: context
      type: com.azure.core.util.Context
      description: Additional context that is passed through the Http pipeline during the service call.
    return:
      type: com.azure.core.http.rest.Response<com.azure.storage.file.datalake.models.PathInfo>
      description: Information about the uploaded path.
references:
- uid: com.azure.storage.file.datalake.DataLakeFileAsyncClient
  name: DataLakeFileAsyncClient
  nameWithType: DataLakeFileAsyncClient
  fullName: com.azure.storage.file.datalake.DataLakeFileAsyncClient
- uid: com.azure.storage.blob.specialized.BlockBlobClient
  spec.java:
  - uid: com.azure.storage.blob.specialized.BlockBlobClient
    name: BlockBlobClient
    fullName: com.azure.storage.blob.specialized.BlockBlobClient
- uid: com.azure.storage.file.datalake.DataLakeFileClient.DataLakeFileClient*
  name: DataLakeFileClient
  nameWithType: DataLakeFileClient.DataLakeFileClient
  fullName: com.azure.storage.file.datalake.DataLakeFileClient.DataLakeFileClient
  package: com.azure.storage.file.datalake
- uid: com.azure.storage.file.datalake.DataLakePathClient
  name: DataLakePathClient
  nameWithType: DataLakePathClient
  fullName: com.azure.storage.file.datalake.DataLakePathClient
- uid: java.lang.String
  spec.java:
  - uid: java.lang.String
    name: String
    fullName: java.lang.String
- uid: com.azure.storage.file.datalake.DataLakeFileClient.getFileUrl*
  name: getFileUrl
  nameWithType: DataLakeFileClient.getFileUrl
  fullName: com.azure.storage.file.datalake.DataLakeFileClient.getFileUrl
  package: com.azure.storage.file.datalake
- uid: com.azure.storage.file.datalake.DataLakeFileClient.getFilePath*
  name: getFilePath
  nameWithType: DataLakeFileClient.getFilePath
  fullName: com.azure.storage.file.datalake.DataLakeFileClient.getFilePath
  package: com.azure.storage.file.datalake
- uid: com.azure.storage.file.datalake.DataLakeFileClient.getFileName*
  name: getFileName
  nameWithType: DataLakeFileClient.getFileName
  fullName: com.azure.storage.file.datalake.DataLakeFileClient.getFileName
  package: com.azure.storage.file.datalake
- uid: com.azure.storage.file.datalake.DataLakeFileClient.delete*
  name: delete
  nameWithType: DataLakeFileClient.delete
  fullName: com.azure.storage.file.datalake.DataLakeFileClient.delete
  package: com.azure.storage.file.datalake
- uid: com.azure.storage.file.datalake.models.DataLakeRequestConditions
  name: DataLakeRequestConditions
  nameWithType: DataLakeRequestConditions
  fullName: com.azure.storage.file.datalake.models.DataLakeRequestConditions
- uid: java.time.Duration
  spec.java:
  - uid: java.time.Duration
    name: Duration
    fullName: java.time.Duration
- uid: com.azure.core.util.Context
  spec.java:
  - uid: com.azure.core.util.Context
    name: Context
    fullName: com.azure.core.util.Context
- uid: com.azure.core.http.rest.Response<java.lang.Void>
  spec.java:
  - uid: com.azure.core.http.rest.Response
    name: Response
    fullName: com.azure.core.http.rest.Response
  - name: <
    fullName: <
  - uid: java.lang.Void
    name: Void
    fullName: java.lang.Void
  - name: '>'
    fullName: '>'
- uid: com.azure.storage.file.datalake.DataLakeFileClient.deleteWithResponse*
  name: deleteWithResponse
  nameWithType: DataLakeFileClient.deleteWithResponse
  fullName: com.azure.storage.file.datalake.DataLakeFileClient.deleteWithResponse
  package: com.azure.storage.file.datalake
- uid: java.io.InputStream
  spec.java:
  - uid: java.io.InputStream
    name: InputStream
    fullName: java.io.InputStream
- uid: long
  spec.java:
  - uid: long
    name: long
    fullName: long
- uid: com.azure.storage.file.datalake.models.PathInfo
  name: PathInfo
  nameWithType: PathInfo
  fullName: com.azure.storage.file.datalake.models.PathInfo
- uid: com.azure.storage.file.datalake.DataLakeFileClient.upload*
  name: upload
  nameWithType: DataLakeFileClient.upload
  fullName: com.azure.storage.file.datalake.DataLakeFileClient.upload
  package: com.azure.storage.file.datalake
- uid: boolean
  spec.java:
  - uid: boolean
    name: boolean
    fullName: boolean
- uid: com.azure.storage.file.datalake.options.FileParallelUploadOptions
  name: FileParallelUploadOptions
  nameWithType: FileParallelUploadOptions
  fullName: com.azure.storage.file.datalake.options.FileParallelUploadOptions
- uid: com.azure.core.http.rest.Response<com.azure.storage.file.datalake.models.PathInfo>
  spec.java:
  - uid: com.azure.core.http.rest.Response
    name: Response
    fullName: com.azure.core.http.rest.Response
  - name: <
    fullName: <
  - uid: com.azure.storage.file.datalake.models.PathInfo
    name: PathInfo
    fullName: com.azure.storage.file.datalake.models.PathInfo
  - name: '>'
    fullName: '>'
- uid: com.azure.storage.file.datalake.DataLakeFileClient.uploadWithResponse*
  name: uploadWithResponse
  nameWithType: DataLakeFileClient.uploadWithResponse
  fullName: com.azure.storage.file.datalake.DataLakeFileClient.uploadWithResponse
  package: com.azure.storage.file.datalake
- uid: com.azure.storage.file.datalake.DataLakeFileClient.uploadFromFile*
  name: uploadFromFile
  nameWithType: DataLakeFileClient.uploadFromFile
  fullName: com.azure.storage.file.datalake.DataLakeFileClient.uploadFromFile
  package: com.azure.storage.file.datalake
- uid: com.azure.storage.common.ParallelTransferOptions
  spec.java:
  - uid: com.azure.storage.common.ParallelTransferOptions
    name: ParallelTransferOptions
    fullName: com.azure.storage.common.ParallelTransferOptions
- uid: com.azure.storage.file.datalake.models.PathHttpHeaders
  name: PathHttpHeaders
  nameWithType: PathHttpHeaders
  fullName: com.azure.storage.file.datalake.models.PathHttpHeaders
- uid: java.util.Map<java.lang.String,java.lang.String>
  spec.java:
  - uid: java.util.Map
    name: Map
    fullName: java.util.Map
  - name: <
    fullName: <
  - uid: java.lang.String
    name: String
    fullName: java.lang.String
  - name: ','
    fullName: ','
  - uid: java.lang.String
    name: String
    fullName: java.lang.String
  - name: '>'
    fullName: '>'
- uid: com.azure.storage.file.datalake.DataLakeFileClient.append*
  name: append
  nameWithType: DataLakeFileClient.append
  fullName: com.azure.storage.file.datalake.DataLakeFileClient.append
  package: com.azure.storage.file.datalake
- uid: byte[]
  spec.java:
  - uid: byte
    name: byte
    fullName: byte
  - name: '[]'
    fullName: '[]'
- uid: com.azure.storage.file.datalake.DataLakeFileClient.appendWithResponse*
  name: appendWithResponse
  nameWithType: DataLakeFileClient.appendWithResponse
  fullName: com.azure.storage.file.datalake.DataLakeFileClient.appendWithResponse
  package: com.azure.storage.file.datalake
- uid: com.azure.storage.file.datalake.DataLakeFileClient.flush*
  name: flush
  nameWithType: DataLakeFileClient.flush
  fullName: com.azure.storage.file.datalake.DataLakeFileClient.flush
  package: com.azure.storage.file.datalake
- uid: com.azure.storage.file.datalake.DataLakeFileClient.flushWithResponse*
  name: flushWithResponse
  nameWithType: DataLakeFileClient.flushWithResponse
  fullName: com.azure.storage.file.datalake.DataLakeFileClient.flushWithResponse
  package: com.azure.storage.file.datalake
- uid: java.io.OutputStream
  spec.java:
  - uid: java.io.OutputStream
    name: OutputStream
    fullName: java.io.OutputStream
- uid: com.azure.storage.file.datalake.DataLakeFileClient.read*
  name: read
  nameWithType: DataLakeFileClient.read
  fullName: com.azure.storage.file.datalake.DataLakeFileClient.read
  package: com.azure.storage.file.datalake
- uid: com.azure.storage.file.datalake.models.FileRange
  name: FileRange
  nameWithType: FileRange
  fullName: com.azure.storage.file.datalake.models.FileRange
- uid: com.azure.storage.file.datalake.models.DownloadRetryOptions
  name: DownloadRetryOptions
  nameWithType: DownloadRetryOptions
  fullName: com.azure.storage.file.datalake.models.DownloadRetryOptions
- uid: com.azure.storage.file.datalake.models.FileReadResponse
  name: FileReadResponse
  nameWithType: FileReadResponse
  fullName: com.azure.storage.file.datalake.models.FileReadResponse
- uid: com.azure.storage.file.datalake.DataLakeFileClient.readWithResponse*
  name: readWithResponse
  nameWithType: DataLakeFileClient.readWithResponse
  fullName: com.azure.storage.file.datalake.DataLakeFileClient.readWithResponse
  package: com.azure.storage.file.datalake
- uid: com.azure.storage.file.datalake.models.PathProperties
  name: PathProperties
  nameWithType: PathProperties
  fullName: com.azure.storage.file.datalake.models.PathProperties
- uid: com.azure.storage.file.datalake.DataLakeFileClient.readToFile*
  name: readToFile
  nameWithType: DataLakeFileClient.readToFile
  fullName: com.azure.storage.file.datalake.DataLakeFileClient.readToFile
  package: com.azure.storage.file.datalake
- uid: java.util.Set<java.nio.file.OpenOption>
  spec.java:
  - uid: java.util.Set
    name: Set
    fullName: java.util.Set
  - name: <
    fullName: <
  - uid: java.nio.file.OpenOption
    name: OpenOption
    fullName: java.nio.file.OpenOption
  - name: '>'
    fullName: '>'
- uid: com.azure.core.http.rest.Response<com.azure.storage.file.datalake.models.PathProperties>
  spec.java:
  - uid: com.azure.core.http.rest.Response
    name: Response
    fullName: com.azure.core.http.rest.Response
  - name: <
    fullName: <
  - uid: com.azure.storage.file.datalake.models.PathProperties
    name: PathProperties
    fullName: com.azure.storage.file.datalake.models.PathProperties
  - name: '>'
    fullName: '>'
- uid: com.azure.storage.file.datalake.DataLakeFileClient.readToFileWithResponse*
  name: readToFileWithResponse
  nameWithType: DataLakeFileClient.readToFileWithResponse
  fullName: com.azure.storage.file.datalake.DataLakeFileClient.readToFileWithResponse
  package: com.azure.storage.file.datalake
- uid: com.azure.storage.file.datalake.DataLakeFileClient.rename*
  name: rename
  nameWithType: DataLakeFileClient.rename
  fullName: com.azure.storage.file.datalake.DataLakeFileClient.rename
  package: com.azure.storage.file.datalake
- uid: com.azure.core.http.rest.Response<com.azure.storage.file.datalake.DataLakeFileClient>
  spec.java:
  - uid: com.azure.core.http.rest.Response
    name: Response
    fullName: com.azure.core.http.rest.Response
  - name: <
    fullName: <
  - uid: com.azure.storage.file.datalake.DataLakeFileClient
    name: DataLakeFileClient
    fullName: com.azure.storage.file.datalake.DataLakeFileClient
  - name: '>'
    fullName: '>'
- uid: com.azure.storage.file.datalake.DataLakeFileClient.renameWithResponse*
  name: renameWithResponse
  nameWithType: DataLakeFileClient.renameWithResponse
  fullName: com.azure.storage.file.datalake.DataLakeFileClient.renameWithResponse
  package: com.azure.storage.file.datalake
- uid: com.azure.storage.file.datalake.DataLakeFileClient.openQueryInputStream*
  name: openQueryInputStream
  nameWithType: DataLakeFileClient.openQueryInputStream
  fullName: com.azure.storage.file.datalake.DataLakeFileClient.openQueryInputStream
  package: com.azure.storage.file.datalake
- uid: com.azure.storage.file.datalake.options.FileQueryOptions
  name: FileQueryOptions
  nameWithType: FileQueryOptions
  fullName: com.azure.storage.file.datalake.options.FileQueryOptions
- uid: com.azure.core.http.rest.Response<java.io.InputStream>
  spec.java:
  - uid: com.azure.core.http.rest.Response
    name: Response
    fullName: com.azure.core.http.rest.Response
  - name: <
    fullName: <
  - uid: java.io.InputStream
    name: InputStream
    fullName: java.io.InputStream
  - name: '>'
    fullName: '>'
- uid: com.azure.storage.file.datalake.DataLakeFileClient.openQueryInputStreamWithResponse*
  name: openQueryInputStreamWithResponse
  nameWithType: DataLakeFileClient.openQueryInputStreamWithResponse
  fullName: com.azure.storage.file.datalake.DataLakeFileClient.openQueryInputStreamWithResponse
  package: com.azure.storage.file.datalake
- uid: com.azure.storage.file.datalake.DataLakeFileClient.query*
  name: query
  nameWithType: DataLakeFileClient.query
  fullName: com.azure.storage.file.datalake.DataLakeFileClient.query
  package: com.azure.storage.file.datalake
- uid: com.azure.storage.file.datalake.models.FileQueryResponse
  name: FileQueryResponse
  nameWithType: FileQueryResponse
  fullName: com.azure.storage.file.datalake.models.FileQueryResponse
- uid: com.azure.storage.file.datalake.DataLakeFileClient.queryWithResponse*
  name: queryWithResponse
  nameWithType: DataLakeFileClient.queryWithResponse
  fullName: com.azure.storage.file.datalake.DataLakeFileClient.queryWithResponse
  package: com.azure.storage.file.datalake
- uid: com.azure.storage.file.datalake.options.FileScheduleDeletionOptions
  name: FileScheduleDeletionOptions
  nameWithType: FileScheduleDeletionOptions
  fullName: com.azure.storage.file.datalake.options.FileScheduleDeletionOptions
- uid: com.azure.storage.file.datalake.DataLakeFileClient.scheduleDeletion*
  name: scheduleDeletion
  nameWithType: DataLakeFileClient.scheduleDeletion
  fullName: com.azure.storage.file.datalake.DataLakeFileClient.scheduleDeletion
  package: com.azure.storage.file.datalake
- uid: com.azure.storage.file.datalake.DataLakeFileClient.scheduleDeletionWithResponse*
  name: scheduleDeletionWithResponse
  nameWithType: DataLakeFileClient.scheduleDeletionWithResponse
  fullName: com.azure.storage.file.datalake.DataLakeFileClient.scheduleDeletionWithResponse
  package: com.azure.storage.file.datalake
- uid: java.lang.Object.wait()
  name: Object.wait()
  nameWithType: Object.wait()
  fullName: java.lang.Object.wait()
- uid: java.lang.Object.finalize()
  name: Object.finalize()
  nameWithType: Object.finalize()
  fullName: java.lang.Object.finalize()
- uid: java.lang.Object.clone()
  name: Object.clone()
  nameWithType: Object.clone()
  fullName: java.lang.Object.clone()
- uid: com.azure.storage.file.datalake.DataLakePathClient.getProperties()
  name: DataLakePathClient.getProperties()
  nameWithType: DataLakePathClient.getProperties()
  fullName: com.azure.storage.file.datalake.DataLakePathClient.getProperties()
- uid: com.azure.storage.file.datalake.DataLakePathClient.existsWithResponse(java.time.Duration,com.azure.core.util.Context)
  name: DataLakePathClient.existsWithResponse(Duration,Context)
  nameWithType: DataLakePathClient.existsWithResponse(Duration,Context)
  fullName: com.azure.storage.file.datalake.DataLakePathClient.existsWithResponse(java.time.Duration,com.azure.core.util.Context)
- uid: com.azure.storage.file.datalake.DataLakePathClient.setAccessControlRecursive(java.util.List<com.azure.storage.file.datalake.models.PathAccessControlEntry>)
  name: DataLakePathClient.setAccessControlRecursive(List<PathAccessControlEntry>)
  nameWithType: DataLakePathClient.setAccessControlRecursive(List<PathAccessControlEntry>)
  fullName: com.azure.storage.file.datalake.DataLakePathClient.setAccessControlRecursive(java.util.List<com.azure.storage.file.datalake.models.PathAccessControlEntry>)
- uid: com.azure.storage.file.datalake.DataLakePathClient.removeAccessControlRecursive(java.util.List<com.azure.storage.file.datalake.models.PathRemoveAccessControlEntry>)
  name: DataLakePathClient.removeAccessControlRecursive(List<PathRemoveAccessControlEntry>)
  nameWithType: DataLakePathClient.removeAccessControlRecursive(List<PathRemoveAccessControlEntry>)
  fullName: com.azure.storage.file.datalake.DataLakePathClient.removeAccessControlRecursive(java.util.List<com.azure.storage.file.datalake.models.PathRemoveAccessControlEntry>)
- uid: com.azure.storage.file.datalake.DataLakePathClient.setAccessControlList(java.util.List<com.azure.storage.file.datalake.models.PathAccessControlEntry>,java.lang.String,java.lang.String)
  name: DataLakePathClient.setAccessControlList(List<PathAccessControlEntry>,String,String)
  nameWithType: DataLakePathClient.setAccessControlList(List<PathAccessControlEntry>,String,String)
  fullName: com.azure.storage.file.datalake.DataLakePathClient.setAccessControlList(java.util.List<com.azure.storage.file.datalake.models.PathAccessControlEntry>,java.lang.String,java.lang.String)
- uid: com.azure.storage.file.datalake.DataLakePathClient.create()
  name: DataLakePathClient.create()
  nameWithType: DataLakePathClient.create()
  fullName: com.azure.storage.file.datalake.DataLakePathClient.create()
- uid: com.azure.storage.file.datalake.DataLakePathClient.createWithResponse(java.lang.String,java.lang.String,com.azure.storage.file.datalake.models.PathHttpHeaders,java.util.Map<java.lang.String,java.lang.String>,com.azure.storage.file.datalake.models.DataLakeRequestConditions,java.time.Duration,com.azure.core.util.Context)
  name: DataLakePathClient.createWithResponse(String,String,PathHttpHeaders,Map<String,String>,DataLakeRequestConditions,Duration,Context)
  nameWithType: DataLakePathClient.createWithResponse(String,String,PathHttpHeaders,Map<String,String>,DataLakeRequestConditions,Duration,Context)
  fullName: com.azure.storage.file.datalake.DataLakePathClient.createWithResponse(java.lang.String,java.lang.String,com.azure.storage.file.datalake.models.PathHttpHeaders,java.util.Map<java.lang.String,java.lang.String>,com.azure.storage.file.datalake.models.DataLakeRequestConditions,java.time.Duration,com.azure.core.util.Context)
- uid: com.azure.storage.file.datalake.DataLakePathClient.getAccessControl()
  name: DataLakePathClient.getAccessControl()
  nameWithType: DataLakePathClient.getAccessControl()
  fullName: com.azure.storage.file.datalake.DataLakePathClient.getAccessControl()
- uid: com.azure.storage.file.datalake.DataLakePathClient.setMetadataWithResponse(java.util.Map<java.lang.String,java.lang.String>,com.azure.storage.file.datalake.models.DataLakeRequestConditions,java.time.Duration,com.azure.core.util.Context)
  name: DataLakePathClient.setMetadataWithResponse(Map<String,String>,DataLakeRequestConditions,Duration,Context)
  nameWithType: DataLakePathClient.setMetadataWithResponse(Map<String,String>,DataLakeRequestConditions,Duration,Context)
  fullName: com.azure.storage.file.datalake.DataLakePathClient.setMetadataWithResponse(java.util.Map<java.lang.String,java.lang.String>,com.azure.storage.file.datalake.models.DataLakeRequestConditions,java.time.Duration,com.azure.core.util.Context)
- uid: java.lang.Object.wait(long)
  name: Object.wait(long)
  nameWithType: Object.wait(long)
  fullName: java.lang.Object.wait(long)
- uid: java.lang.Object.getClass()
  name: Object.getClass()
  nameWithType: Object.getClass()
  fullName: java.lang.Object.getClass()
- uid: java.lang.Object.hashCode()
  name: Object.hashCode()
  nameWithType: Object.hashCode()
  fullName: java.lang.Object.hashCode()
- uid: com.azure.storage.file.datalake.DataLakePathClient.getServiceVersion()
  name: DataLakePathClient.getServiceVersion()
  nameWithType: DataLakePathClient.getServiceVersion()
  fullName: com.azure.storage.file.datalake.DataLakePathClient.getServiceVersion()
- uid: java.lang.Object.wait(long,int)
  name: Object.wait(long,int)
  nameWithType: Object.wait(long,int)
  fullName: java.lang.Object.wait(long,int)
- uid: com.azure.storage.file.datalake.DataLakePathClient.setAccessControlRecursiveWithResponse(com.azure.storage.file.datalake.options.PathSetAccessControlRecursiveOptions,java.time.Duration,com.azure.core.util.Context)
  name: DataLakePathClient.setAccessControlRecursiveWithResponse(PathSetAccessControlRecursiveOptions,Duration,Context)
  nameWithType: DataLakePathClient.setAccessControlRecursiveWithResponse(PathSetAccessControlRecursiveOptions,Duration,Context)
  fullName: com.azure.storage.file.datalake.DataLakePathClient.setAccessControlRecursiveWithResponse(com.azure.storage.file.datalake.options.PathSetAccessControlRecursiveOptions,java.time.Duration,com.azure.core.util.Context)
- uid: com.azure.storage.file.datalake.DataLakePathClient.getAccountName()
  name: DataLakePathClient.getAccountName()
  nameWithType: DataLakePathClient.getAccountName()
  fullName: com.azure.storage.file.datalake.DataLakePathClient.getAccountName()
- uid: java.lang.Object.notify()
  name: Object.notify()
  nameWithType: Object.notify()
  fullName: java.lang.Object.notify()
- uid: com.azure.storage.file.datalake.DataLakePathClient.getAccessControlWithResponse(boolean,com.azure.storage.file.datalake.models.DataLakeRequestConditions,java.time.Duration,com.azure.core.util.Context)
  name: DataLakePathClient.getAccessControlWithResponse(boolean,DataLakeRequestConditions,Duration,Context)
  nameWithType: DataLakePathClient.getAccessControlWithResponse(boolean,DataLakeRequestConditions,Duration,Context)
  fullName: com.azure.storage.file.datalake.DataLakePathClient.getAccessControlWithResponse(boolean,com.azure.storage.file.datalake.models.DataLakeRequestConditions,java.time.Duration,com.azure.core.util.Context)
- uid: com.azure.storage.file.datalake.DataLakePathClient.updateAccessControlRecursiveWithResponse(com.azure.storage.file.datalake.options.PathUpdateAccessControlRecursiveOptions,java.time.Duration,com.azure.core.util.Context)
  name: DataLakePathClient.updateAccessControlRecursiveWithResponse(PathUpdateAccessControlRecursiveOptions,Duration,Context)
  nameWithType: DataLakePathClient.updateAccessControlRecursiveWithResponse(PathUpdateAccessControlRecursiveOptions,Duration,Context)
  fullName: com.azure.storage.file.datalake.DataLakePathClient.updateAccessControlRecursiveWithResponse(com.azure.storage.file.datalake.options.PathUpdateAccessControlRecursiveOptions,java.time.Duration,com.azure.core.util.Context)
- uid: java.lang.Object.notifyAll()
  name: Object.notifyAll()
  nameWithType: Object.notifyAll()
  fullName: java.lang.Object.notifyAll()
- uid: com.azure.storage.file.datalake.DataLakePathClient.generateSas(com.azure.storage.file.datalake.sas.DataLakeServiceSasSignatureValues)
  name: DataLakePathClient.generateSas(DataLakeServiceSasSignatureValues)
  nameWithType: DataLakePathClient.generateSas(DataLakeServiceSasSignatureValues)
  fullName: com.azure.storage.file.datalake.DataLakePathClient.generateSas(com.azure.storage.file.datalake.sas.DataLakeServiceSasSignatureValues)
- uid: java.lang.Object.equals(java.lang.Object)
  name: Object.equals(Object)
  nameWithType: Object.equals(Object)
  fullName: java.lang.Object.equals(java.lang.Object)
- uid: com.azure.storage.file.datalake.DataLakePathClient.setMetadata(java.util.Map<java.lang.String,java.lang.String>)
  name: DataLakePathClient.setMetadata(Map<String,String>)
  nameWithType: DataLakePathClient.setMetadata(Map<String,String>)
  fullName: com.azure.storage.file.datalake.DataLakePathClient.setMetadata(java.util.Map<java.lang.String,java.lang.String>)
- uid: com.azure.storage.file.datalake.DataLakePathClient.setHttpHeadersWithResponse(com.azure.storage.file.datalake.models.PathHttpHeaders,com.azure.storage.file.datalake.models.DataLakeRequestConditions,java.time.Duration,com.azure.core.util.Context)
  name: DataLakePathClient.setHttpHeadersWithResponse(PathHttpHeaders,DataLakeRequestConditions,Duration,Context)
  nameWithType: DataLakePathClient.setHttpHeadersWithResponse(PathHttpHeaders,DataLakeRequestConditions,Duration,Context)
  fullName: com.azure.storage.file.datalake.DataLakePathClient.setHttpHeadersWithResponse(com.azure.storage.file.datalake.models.PathHttpHeaders,com.azure.storage.file.datalake.models.DataLakeRequestConditions,java.time.Duration,com.azure.core.util.Context)
- uid: com.azure.storage.file.datalake.DataLakePathClient.exists()
  name: DataLakePathClient.exists()
  nameWithType: DataLakePathClient.exists()
  fullName: com.azure.storage.file.datalake.DataLakePathClient.exists()
- uid: com.azure.storage.file.datalake.DataLakePathClient.generateUserDelegationSas(com.azure.storage.file.datalake.sas.DataLakeServiceSasSignatureValues,com.azure.storage.file.datalake.models.UserDelegationKey)
  name: DataLakePathClient.generateUserDelegationSas(DataLakeServiceSasSignatureValues,UserDelegationKey)
  nameWithType: DataLakePathClient.generateUserDelegationSas(DataLakeServiceSasSignatureValues,UserDelegationKey)
  fullName: com.azure.storage.file.datalake.DataLakePathClient.generateUserDelegationSas(com.azure.storage.file.datalake.sas.DataLakeServiceSasSignatureValues,com.azure.storage.file.datalake.models.UserDelegationKey)
- uid: com.azure.storage.file.datalake.DataLakePathClient.setPermissions(com.azure.storage.file.datalake.models.PathPermissions,java.lang.String,java.lang.String)
  name: DataLakePathClient.setPermissions(PathPermissions,String,String)
  nameWithType: DataLakePathClient.setPermissions(PathPermissions,String,String)
  fullName: com.azure.storage.file.datalake.DataLakePathClient.setPermissions(com.azure.storage.file.datalake.models.PathPermissions,java.lang.String,java.lang.String)
- uid: java.lang.Object.toString()
  name: Object.toString()
  nameWithType: Object.toString()
  fullName: java.lang.Object.toString()
- uid: com.azure.storage.file.datalake.DataLakePathClient.getFileSystemName()
  name: DataLakePathClient.getFileSystemName()
  nameWithType: DataLakePathClient.getFileSystemName()
  fullName: com.azure.storage.file.datalake.DataLakePathClient.getFileSystemName()
- uid: com.azure.storage.file.datalake.DataLakePathClient.create(boolean)
  name: DataLakePathClient.create(boolean)
  nameWithType: DataLakePathClient.create(boolean)
  fullName: com.azure.storage.file.datalake.DataLakePathClient.create(boolean)
- uid: com.azure.storage.file.datalake.DataLakePathClient.setPermissionsWithResponse(com.azure.storage.file.datalake.models.PathPermissions,java.lang.String,java.lang.String,com.azure.storage.file.datalake.models.DataLakeRequestConditions,java.time.Duration,com.azure.core.util.Context)
  name: DataLakePathClient.setPermissionsWithResponse(PathPermissions,String,String,DataLakeRequestConditions,Duration,Context)
  nameWithType: DataLakePathClient.setPermissionsWithResponse(PathPermissions,String,String,DataLakeRequestConditions,Duration,Context)
  fullName: com.azure.storage.file.datalake.DataLakePathClient.setPermissionsWithResponse(com.azure.storage.file.datalake.models.PathPermissions,java.lang.String,java.lang.String,com.azure.storage.file.datalake.models.DataLakeRequestConditions,java.time.Duration,com.azure.core.util.Context)
- uid: com.azure.storage.file.datalake.DataLakePathClient.setHttpHeaders(com.azure.storage.file.datalake.models.PathHttpHeaders)
  name: DataLakePathClient.setHttpHeaders(PathHttpHeaders)
  nameWithType: DataLakePathClient.setHttpHeaders(PathHttpHeaders)
  fullName: com.azure.storage.file.datalake.DataLakePathClient.setHttpHeaders(com.azure.storage.file.datalake.models.PathHttpHeaders)
- uid: com.azure.storage.file.datalake.DataLakePathClient.getHttpPipeline()
  name: DataLakePathClient.getHttpPipeline()
  nameWithType: DataLakePathClient.getHttpPipeline()
  fullName: com.azure.storage.file.datalake.DataLakePathClient.getHttpPipeline()
- uid: com.azure.storage.file.datalake.DataLakePathClient.setAccessControlListWithResponse(java.util.List<com.azure.storage.file.datalake.models.PathAccessControlEntry>,java.lang.String,java.lang.String,com.azure.storage.file.datalake.models.DataLakeRequestConditions,java.time.Duration,com.azure.core.util.Context)
  name: DataLakePathClient.setAccessControlListWithResponse(List<PathAccessControlEntry>,String,String,DataLakeRequestConditions,Duration,Context)
  nameWithType: DataLakePathClient.setAccessControlListWithResponse(List<PathAccessControlEntry>,String,String,DataLakeRequestConditions,Duration,Context)
  fullName: com.azure.storage.file.datalake.DataLakePathClient.setAccessControlListWithResponse(java.util.List<com.azure.storage.file.datalake.models.PathAccessControlEntry>,java.lang.String,java.lang.String,com.azure.storage.file.datalake.models.DataLakeRequestConditions,java.time.Duration,com.azure.core.util.Context)
- uid: com.azure.storage.file.datalake.DataLakePathClient.updateAccessControlRecursive(java.util.List<com.azure.storage.file.datalake.models.PathAccessControlEntry>)
  name: DataLakePathClient.updateAccessControlRecursive(List<PathAccessControlEntry>)
  nameWithType: DataLakePathClient.updateAccessControlRecursive(List<PathAccessControlEntry>)
  fullName: com.azure.storage.file.datalake.DataLakePathClient.updateAccessControlRecursive(java.util.List<com.azure.storage.file.datalake.models.PathAccessControlEntry>)
- uid: com.azure.storage.file.datalake.DataLakePathClient.getPropertiesWithResponse(com.azure.storage.file.datalake.models.DataLakeRequestConditions,java.time.Duration,com.azure.core.util.Context)
  name: DataLakePathClient.getPropertiesWithResponse(DataLakeRequestConditions,Duration,Context)
  nameWithType: DataLakePathClient.getPropertiesWithResponse(DataLakeRequestConditions,Duration,Context)
  fullName: com.azure.storage.file.datalake.DataLakePathClient.getPropertiesWithResponse(com.azure.storage.file.datalake.models.DataLakeRequestConditions,java.time.Duration,com.azure.core.util.Context)
- uid: com.azure.storage.file.datalake.DataLakePathClient.removeAccessControlRecursiveWithResponse(com.azure.storage.file.datalake.options.PathRemoveAccessControlRecursiveOptions,java.time.Duration,com.azure.core.util.Context)
  name: DataLakePathClient.removeAccessControlRecursiveWithResponse(PathRemoveAccessControlRecursiveOptions,Duration,Context)
  nameWithType: DataLakePathClient.removeAccessControlRecursiveWithResponse(PathRemoveAccessControlRecursiveOptions,Duration,Context)
  fullName: com.azure.storage.file.datalake.DataLakePathClient.removeAccessControlRecursiveWithResponse(com.azure.storage.file.datalake.options.PathRemoveAccessControlRecursiveOptions,java.time.Duration,com.azure.core.util.Context)
- uid: java.lang.Void
  name: Void
  nameWithType: Void
  fullName: java.lang.Void
- uid: com.azure.core.http.rest.Response
  name: Response
  nameWithType: Response
  fullName: com.azure.core.http.rest.Response
- uid: java.util.Map
  name: Map
  nameWithType: Map
  fullName: java.util.Map
- uid: java.lang.String,java.lang.String
  name: String,String
  nameWithType: String,String
  fullName: java.lang.String,java.lang.String
- uid: java.nio.file.OpenOption
  name: OpenOption
  nameWithType: OpenOption
  fullName: java.nio.file.OpenOption
- uid: java.util.Set
  name: Set
  nameWithType: Set
  fullName: java.util.Set
- uid: com.azure.storage.file.datalake.DataLakePathClient.setAccessControlRecursive(java.util.List
  name: DataLakePathClient.setAccessControlRecursive(List
  nameWithType: DataLakePathClient.setAccessControlRecursive(List
  fullName: com.azure.storage.file.datalake.DataLakePathClient.setAccessControlRecursive(java.util.List
- uid: com.azure.storage.file.datalake.models.PathAccessControlEntry>)
  name: PathAccessControlEntry>)
  nameWithType: PathAccessControlEntry>)
  fullName: com.azure.storage.file.datalake.models.PathAccessControlEntry>)
- uid: com.azure.storage.file.datalake.DataLakePathClient.removeAccessControlRecursive(java.util.List
  name: DataLakePathClient.removeAccessControlRecursive(List
  nameWithType: DataLakePathClient.removeAccessControlRecursive(List
  fullName: com.azure.storage.file.datalake.DataLakePathClient.removeAccessControlRecursive(java.util.List
- uid: com.azure.storage.file.datalake.models.PathRemoveAccessControlEntry>)
  name: PathRemoveAccessControlEntry>)
  nameWithType: PathRemoveAccessControlEntry>)
  fullName: com.azure.storage.file.datalake.models.PathRemoveAccessControlEntry>)
- uid: com.azure.storage.file.datalake.models.PathAccessControlEntry>,java.lang.String,java.lang.String)
  name: PathAccessControlEntry>,String,String)
  nameWithType: PathAccessControlEntry>,String,String)
  fullName: com.azure.storage.file.datalake.models.PathAccessControlEntry>,java.lang.String,java.lang.String)
- uid: com.azure.storage.file.datalake.DataLakePathClient.setAccessControlList(java.util.List
  name: DataLakePathClient.setAccessControlList(List
  nameWithType: DataLakePathClient.setAccessControlList(List
  fullName: com.azure.storage.file.datalake.DataLakePathClient.setAccessControlList(java.util.List
- uid: com.azure.storage.file.datalake.DataLakePathClient.createWithResponse(java.lang.String,java.lang.String,com.azure.storage.file.datalake.models.PathHttpHeaders,java.util.Map
  name: DataLakePathClient.createWithResponse(String,String,PathHttpHeaders,Map
  nameWithType: DataLakePathClient.createWithResponse(String,String,PathHttpHeaders,Map
  fullName: com.azure.storage.file.datalake.DataLakePathClient.createWithResponse(java.lang.String,java.lang.String,com.azure.storage.file.datalake.models.PathHttpHeaders,java.util.Map
- uid: java.lang.String,java.lang.String>,com.azure.storage.file.datalake.models.DataLakeRequestConditions,java.time.Duration,com.azure.core.util.Context)
  name: String,String>,DataLakeRequestConditions,Duration,Context)
  nameWithType: String,String>,DataLakeRequestConditions,Duration,Context)
  fullName: java.lang.String,java.lang.String>,com.azure.storage.file.datalake.models.DataLakeRequestConditions,java.time.Duration,com.azure.core.util.Context)
- uid: com.azure.storage.file.datalake.DataLakePathClient.setMetadataWithResponse(java.util.Map
  name: DataLakePathClient.setMetadataWithResponse(Map
  nameWithType: DataLakePathClient.setMetadataWithResponse(Map
  fullName: com.azure.storage.file.datalake.DataLakePathClient.setMetadataWithResponse(java.util.Map
- uid: java.lang.String,java.lang.String>)
  name: String,String>)
  nameWithType: String,String>)
  fullName: java.lang.String,java.lang.String>)
- uid: com.azure.storage.file.datalake.DataLakePathClient.setMetadata(java.util.Map
  name: DataLakePathClient.setMetadata(Map
  nameWithType: DataLakePathClient.setMetadata(Map
  fullName: com.azure.storage.file.datalake.DataLakePathClient.setMetadata(java.util.Map
- uid: com.azure.storage.file.datalake.DataLakePathClient.setAccessControlListWithResponse(java.util.List
  name: DataLakePathClient.setAccessControlListWithResponse(List
  nameWithType: DataLakePathClient.setAccessControlListWithResponse(List
  fullName: com.azure.storage.file.datalake.DataLakePathClient.setAccessControlListWithResponse(java.util.List
- uid: com.azure.storage.file.datalake.models.PathAccessControlEntry>,java.lang.String,java.lang.String,com.azure.storage.file.datalake.models.DataLakeRequestConditions,java.time.Duration,com.azure.core.util.Context)
  name: PathAccessControlEntry>,String,String,DataLakeRequestConditions,Duration,Context)
  nameWithType: PathAccessControlEntry>,String,String,DataLakeRequestConditions,Duration,Context)
  fullName: com.azure.storage.file.datalake.models.PathAccessControlEntry>,java.lang.String,java.lang.String,com.azure.storage.file.datalake.models.DataLakeRequestConditions,java.time.Duration,com.azure.core.util.Context)
- uid: com.azure.storage.file.datalake.DataLakePathClient.updateAccessControlRecursive(java.util.List
  name: DataLakePathClient.updateAccessControlRecursive(List
  nameWithType: DataLakePathClient.updateAccessControlRecursive(List
  fullName: com.azure.storage.file.datalake.DataLakePathClient.updateAccessControlRecursive(java.util.List
