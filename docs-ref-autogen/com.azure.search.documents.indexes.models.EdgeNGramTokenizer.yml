### YamlMime:ManagedReference
items:
- uid: com.azure.search.documents.indexes.models.EdgeNGramTokenizer
  id: EdgeNGramTokenizer
  artifact: com.azure:azure-search-documents:11.1.2
  parent: com.azure.search.documents.indexes.models
  children:
  - com.azure.search.documents.indexes.models.EdgeNGramTokenizer.EdgeNGramTokenizer(java.lang.String)
  - com.azure.search.documents.indexes.models.EdgeNGramTokenizer.getMaxGram()
  - com.azure.search.documents.indexes.models.EdgeNGramTokenizer.getMinGram()
  - com.azure.search.documents.indexes.models.EdgeNGramTokenizer.getTokenChars()
  - com.azure.search.documents.indexes.models.EdgeNGramTokenizer.setMaxGram(java.lang.Integer)
  - com.azure.search.documents.indexes.models.EdgeNGramTokenizer.setMinGram(java.lang.Integer)
  - com.azure.search.documents.indexes.models.EdgeNGramTokenizer.setTokenChars(com.azure.search.documents.indexes.models.TokenCharacterKind...)
  - com.azure.search.documents.indexes.models.EdgeNGramTokenizer.setTokenChars(java.util.List<com.azure.search.documents.indexes.models.TokenCharacterKind>)
  langs:
  - java
  name: EdgeNGramTokenizer
  nameWithType: EdgeNGramTokenizer
  fullName: com.azure.search.documents.indexes.models.EdgeNGramTokenizer
  type: Class
  package: com.azure.search.documents.indexes.models
  summary: Tokenizes the input from an edge into n-grams of the given size(s). This tokenizer is implemented using Apache Lucene.
  syntax:
    content: public final class EdgeNGramTokenizer extends LexicalTokenizer
  inheritance:
  - java.lang.Object
  - com.azure.search.documents.indexes.models.LexicalTokenizer
  inheritedMembers:
  - com.azure.search.documents.indexes.models.LexicalTokenizer.getName()
  - java.lang.Object.clone()
  - java.lang.Object.equals(java.lang.Object)
  - java.lang.Object.finalize()
  - java.lang.Object.getClass()
  - java.lang.Object.hashCode()
  - java.lang.Object.notify()
  - java.lang.Object.notifyAll()
  - java.lang.Object.toString()
  - java.lang.Object.wait()
  - java.lang.Object.wait(long)
  - java.lang.Object.wait(long,int)
- uid: com.azure.search.documents.indexes.models.EdgeNGramTokenizer.EdgeNGramTokenizer(java.lang.String)
  id: EdgeNGramTokenizer(java.lang.String)
  artifact: com.azure:azure-search-documents:11.1.2
  parent: com.azure.search.documents.indexes.models.EdgeNGramTokenizer
  langs:
  - java
  name: EdgeNGramTokenizer(String name)
  nameWithType: EdgeNGramTokenizer.EdgeNGramTokenizer(String name)
  fullName: com.azure.search.documents.indexes.models.EdgeNGramTokenizer.EdgeNGramTokenizer(String name)
  overload: com.azure.search.documents.indexes.models.EdgeNGramTokenizer.EdgeNGramTokenizer*
  type: Constructor
  package: com.azure.search.documents.indexes.models
  summary: Constructor of <xref uid="com.azure.search.documents.indexes.models.LexicalTokenizer" data-throw-if-not-resolved="false">LexicalTokenizer</xref>.
  syntax:
    content: public EdgeNGramTokenizer(String name)
    parameters:
    - id: name
      type: java.lang.String
      description: >-
        The name of the tokenizer. It must only contain letters, digits, spaces,
         dashes or underscores, can only start and end with alphanumeric
         characters, and is limited to 128 characters.
- uid: com.azure.search.documents.indexes.models.EdgeNGramTokenizer.getMaxGram()
  id: getMaxGram()
  artifact: com.azure:azure-search-documents:11.1.2
  parent: com.azure.search.documents.indexes.models.EdgeNGramTokenizer
  langs:
  - java
  name: getMaxGram()
  nameWithType: EdgeNGramTokenizer.getMaxGram()
  fullName: com.azure.search.documents.indexes.models.EdgeNGramTokenizer.getMaxGram()
  overload: com.azure.search.documents.indexes.models.EdgeNGramTokenizer.getMaxGram*
  type: Method
  package: com.azure.search.documents.indexes.models
  summary: 'Get the maxGram property: The maximum n-gram length. Default is 2. Maximum is 300.'
  syntax:
    content: public Integer getMaxGram()
    return:
      type: java.lang.Integer
      description: the maxGram value.
- uid: com.azure.search.documents.indexes.models.EdgeNGramTokenizer.getMinGram()
  id: getMinGram()
  artifact: com.azure:azure-search-documents:11.1.2
  parent: com.azure.search.documents.indexes.models.EdgeNGramTokenizer
  langs:
  - java
  name: getMinGram()
  nameWithType: EdgeNGramTokenizer.getMinGram()
  fullName: com.azure.search.documents.indexes.models.EdgeNGramTokenizer.getMinGram()
  overload: com.azure.search.documents.indexes.models.EdgeNGramTokenizer.getMinGram*
  type: Method
  package: com.azure.search.documents.indexes.models
  summary: 'Get the minGram property: The minimum n-gram length. Default is 1. Maximum is 300. Must be less than the value of maxGram.'
  syntax:
    content: public Integer getMinGram()
    return:
      type: java.lang.Integer
      description: the minGram value.
- uid: com.azure.search.documents.indexes.models.EdgeNGramTokenizer.getTokenChars()
  id: getTokenChars()
  artifact: com.azure:azure-search-documents:11.1.2
  parent: com.azure.search.documents.indexes.models.EdgeNGramTokenizer
  langs:
  - java
  name: getTokenChars()
  nameWithType: EdgeNGramTokenizer.getTokenChars()
  fullName: com.azure.search.documents.indexes.models.EdgeNGramTokenizer.getTokenChars()
  overload: com.azure.search.documents.indexes.models.EdgeNGramTokenizer.getTokenChars*
  type: Method
  package: com.azure.search.documents.indexes.models
  summary: 'Get the tokenChars property: Character classes to keep in the tokens.'
  syntax:
    content: public List<TokenCharacterKind> getTokenChars()
    return:
      type: java.util.List<com.azure.search.documents.indexes.models.TokenCharacterKind>
      description: the tokenChars value.
- uid: com.azure.search.documents.indexes.models.EdgeNGramTokenizer.setMaxGram(java.lang.Integer)
  id: setMaxGram(java.lang.Integer)
  artifact: com.azure:azure-search-documents:11.1.2
  parent: com.azure.search.documents.indexes.models.EdgeNGramTokenizer
  langs:
  - java
  name: setMaxGram(Integer maxGram)
  nameWithType: EdgeNGramTokenizer.setMaxGram(Integer maxGram)
  fullName: com.azure.search.documents.indexes.models.EdgeNGramTokenizer.setMaxGram(Integer maxGram)
  overload: com.azure.search.documents.indexes.models.EdgeNGramTokenizer.setMaxGram*
  type: Method
  package: com.azure.search.documents.indexes.models
  summary: 'Set the maxGram property: The maximum n-gram length. Default is 2. Maximum is 300.'
  syntax:
    content: public EdgeNGramTokenizer setMaxGram(Integer maxGram)
    parameters:
    - id: maxGram
      type: java.lang.Integer
      description: the maxGram value to set.
    return:
      type: com.azure.search.documents.indexes.models.EdgeNGramTokenizer
      description: the EdgeNGramTokenizer object itself.
- uid: com.azure.search.documents.indexes.models.EdgeNGramTokenizer.setMinGram(java.lang.Integer)
  id: setMinGram(java.lang.Integer)
  artifact: com.azure:azure-search-documents:11.1.2
  parent: com.azure.search.documents.indexes.models.EdgeNGramTokenizer
  langs:
  - java
  name: setMinGram(Integer minGram)
  nameWithType: EdgeNGramTokenizer.setMinGram(Integer minGram)
  fullName: com.azure.search.documents.indexes.models.EdgeNGramTokenizer.setMinGram(Integer minGram)
  overload: com.azure.search.documents.indexes.models.EdgeNGramTokenizer.setMinGram*
  type: Method
  package: com.azure.search.documents.indexes.models
  summary: 'Set the minGram property: The minimum n-gram length. Default is 1. Maximum is 300. Must be less than the value of maxGram.'
  syntax:
    content: public EdgeNGramTokenizer setMinGram(Integer minGram)
    parameters:
    - id: minGram
      type: java.lang.Integer
      description: the minGram value to set.
    return:
      type: com.azure.search.documents.indexes.models.EdgeNGramTokenizer
      description: the EdgeNGramTokenizer object itself.
- uid: com.azure.search.documents.indexes.models.EdgeNGramTokenizer.setTokenChars(com.azure.search.documents.indexes.models.TokenCharacterKind...)
  id: setTokenChars(com.azure.search.documents.indexes.models.TokenCharacterKind...)
  artifact: com.azure:azure-search-documents:11.1.2
  parent: com.azure.search.documents.indexes.models.EdgeNGramTokenizer
  langs:
  - java
  name: setTokenChars(TokenCharacterKind[] tokenChars)
  nameWithType: EdgeNGramTokenizer.setTokenChars(TokenCharacterKind[] tokenChars)
  fullName: com.azure.search.documents.indexes.models.EdgeNGramTokenizer.setTokenChars(TokenCharacterKind[] tokenChars)
  overload: com.azure.search.documents.indexes.models.EdgeNGramTokenizer.setTokenChars*
  type: Method
  package: com.azure.search.documents.indexes.models
  summary: 'Set the tokenChars property: Character classes to keep in the tokens.'
  syntax:
    content: public EdgeNGramTokenizer setTokenChars(TokenCharacterKind[] tokenChars)
    parameters:
    - id: tokenChars
      type: com.azure.search.documents.indexes.models.TokenCharacterKind[]
      description: the tokenChars value to set.
    return:
      type: com.azure.search.documents.indexes.models.EdgeNGramTokenizer
      description: the EdgeNGramTokenizer object itself.
- uid: com.azure.search.documents.indexes.models.EdgeNGramTokenizer.setTokenChars(java.util.List<com.azure.search.documents.indexes.models.TokenCharacterKind>)
  id: setTokenChars(java.util.List<com.azure.search.documents.indexes.models.TokenCharacterKind>)
  artifact: com.azure:azure-search-documents:11.1.2
  parent: com.azure.search.documents.indexes.models.EdgeNGramTokenizer
  langs:
  - java
  name: setTokenChars(List<TokenCharacterKind> tokenChars)
  nameWithType: EdgeNGramTokenizer.setTokenChars(List<TokenCharacterKind> tokenChars)
  fullName: com.azure.search.documents.indexes.models.EdgeNGramTokenizer.setTokenChars(List<TokenCharacterKind> tokenChars)
  overload: com.azure.search.documents.indexes.models.EdgeNGramTokenizer.setTokenChars*
  type: Method
  package: com.azure.search.documents.indexes.models
  summary: 'Set the tokenChars property: Character classes to keep in the tokens.'
  syntax:
    content: public EdgeNGramTokenizer setTokenChars(List<TokenCharacterKind> tokenChars)
    parameters:
    - id: tokenChars
      type: java.util.List<com.azure.search.documents.indexes.models.TokenCharacterKind>
      description: the tokenChars value to set.
    return:
      type: com.azure.search.documents.indexes.models.EdgeNGramTokenizer
      description: the EdgeNGramTokenizer object itself.
references:
- uid: java.lang.String
  spec.java:
  - uid: java.lang.String
    name: String
    fullName: java.lang.String
- uid: com.azure.search.documents.indexes.models.EdgeNGramTokenizer.EdgeNGramTokenizer*
  name: EdgeNGramTokenizer
  nameWithType: EdgeNGramTokenizer.EdgeNGramTokenizer
  fullName: com.azure.search.documents.indexes.models.EdgeNGramTokenizer.EdgeNGramTokenizer
  package: com.azure.search.documents.indexes.models
- uid: java.lang.Integer
  spec.java:
  - uid: java.lang.Integer
    name: Integer
    fullName: java.lang.Integer
- uid: com.azure.search.documents.indexes.models.EdgeNGramTokenizer.getMinGram*
  name: getMinGram
  nameWithType: EdgeNGramTokenizer.getMinGram
  fullName: com.azure.search.documents.indexes.models.EdgeNGramTokenizer.getMinGram
  package: com.azure.search.documents.indexes.models
- uid: com.azure.search.documents.indexes.models.EdgeNGramTokenizer.setMinGram*
  name: setMinGram
  nameWithType: EdgeNGramTokenizer.setMinGram
  fullName: com.azure.search.documents.indexes.models.EdgeNGramTokenizer.setMinGram
  package: com.azure.search.documents.indexes.models
- uid: com.azure.search.documents.indexes.models.EdgeNGramTokenizer.getMaxGram*
  name: getMaxGram
  nameWithType: EdgeNGramTokenizer.getMaxGram
  fullName: com.azure.search.documents.indexes.models.EdgeNGramTokenizer.getMaxGram
  package: com.azure.search.documents.indexes.models
- uid: com.azure.search.documents.indexes.models.EdgeNGramTokenizer.setMaxGram*
  name: setMaxGram
  nameWithType: EdgeNGramTokenizer.setMaxGram
  fullName: com.azure.search.documents.indexes.models.EdgeNGramTokenizer.setMaxGram
  package: com.azure.search.documents.indexes.models
- uid: java.util.List<com.azure.search.documents.indexes.models.TokenCharacterKind>
  spec.java:
  - uid: java.util.List
    name: List
    fullName: java.util.List
  - name: <
    fullName: <
  - uid: com.azure.search.documents.indexes.models.TokenCharacterKind
    name: TokenCharacterKind
    fullName: com.azure.search.documents.indexes.models.TokenCharacterKind
  - name: '>'
    fullName: '>'
- uid: com.azure.search.documents.indexes.models.EdgeNGramTokenizer.getTokenChars*
  name: getTokenChars
  nameWithType: EdgeNGramTokenizer.getTokenChars
  fullName: com.azure.search.documents.indexes.models.EdgeNGramTokenizer.getTokenChars
  package: com.azure.search.documents.indexes.models
- uid: com.azure.search.documents.indexes.models.TokenCharacterKind[]
  spec.java:
  - uid: com.azure.search.documents.indexes.models.TokenCharacterKind
    name: TokenCharacterKind
    fullName: com.azure.search.documents.indexes.models.TokenCharacterKind
  - name: '[]'
    fullName: '[]'
- uid: com.azure.search.documents.indexes.models.EdgeNGramTokenizer.setTokenChars*
  name: setTokenChars
  nameWithType: EdgeNGramTokenizer.setTokenChars
  fullName: com.azure.search.documents.indexes.models.EdgeNGramTokenizer.setTokenChars
  package: com.azure.search.documents.indexes.models
- uid: com.azure.search.documents.indexes.models.LexicalTokenizer
  name: LexicalTokenizer
  nameWithType: LexicalTokenizer
  fullName: com.azure.search.documents.indexes.models.LexicalTokenizer
- uid: java.lang.Object.notify()
  name: Object.notify()
  nameWithType: Object.notify()
  fullName: java.lang.Object.notify()
- uid: java.lang.Object.wait()
  name: Object.wait()
  nameWithType: Object.wait()
  fullName: java.lang.Object.wait()
- uid: java.lang.Object.finalize()
  name: Object.finalize()
  nameWithType: Object.finalize()
  fullName: java.lang.Object.finalize()
- uid: java.lang.Object.clone()
  name: Object.clone()
  nameWithType: Object.clone()
  fullName: java.lang.Object.clone()
- uid: java.lang.Object.notifyAll()
  name: Object.notifyAll()
  nameWithType: Object.notifyAll()
  fullName: java.lang.Object.notifyAll()
- uid: com.azure.search.documents.indexes.models.LexicalTokenizer.getName()
  name: LexicalTokenizer.getName()
  nameWithType: LexicalTokenizer.getName()
  fullName: com.azure.search.documents.indexes.models.LexicalTokenizer.getName()
- uid: java.lang.Object.equals(java.lang.Object)
  name: Object.equals(Object)
  nameWithType: Object.equals(Object)
  fullName: java.lang.Object.equals(java.lang.Object)
- uid: java.lang.Object.getClass()
  name: Object.getClass()
  nameWithType: Object.getClass()
  fullName: java.lang.Object.getClass()
- uid: java.lang.Object.wait(long)
  name: Object.wait(long)
  nameWithType: Object.wait(long)
  fullName: java.lang.Object.wait(long)
- uid: java.lang.Object.hashCode()
  name: Object.hashCode()
  nameWithType: Object.hashCode()
  fullName: java.lang.Object.hashCode()
- uid: java.lang.Object.wait(long,int)
  name: Object.wait(long,int)
  nameWithType: Object.wait(long,int)
  fullName: java.lang.Object.wait(long,int)
- uid: java.lang.Object.toString()
  name: Object.toString()
  nameWithType: Object.toString()
  fullName: java.lang.Object.toString()
- uid: java.util.List
  name: List
  nameWithType: List
  fullName: java.util.List
- uid: com.azure.search.documents.indexes.models.TokenCharacterKind
  name: TokenCharacterKind
  nameWithType: TokenCharacterKind
  fullName: com.azure.search.documents.indexes.models.TokenCharacterKind
