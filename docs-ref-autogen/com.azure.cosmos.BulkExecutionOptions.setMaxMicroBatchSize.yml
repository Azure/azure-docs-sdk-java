### YamlMime:JavaMember
uid: "com.azure.cosmos.BulkExecutionOptions.setMaxMicroBatchSize*"
fullName: "com.azure.cosmos.BulkExecutionOptions.setMaxMicroBatchSize"
name: "setMaxMicroBatchSize"
nameWithType: "BulkExecutionOptions.setMaxMicroBatchSize"
members:
- uid: "com.azure.cosmos.BulkExecutionOptions.setMaxMicroBatchSize(int)"
  fullName: "com.azure.cosmos.BulkExecutionOptions.setMaxMicroBatchSize(int maxMicroBatchSize)"
  name: "setMaxMicroBatchSize(int maxMicroBatchSize)"
  nameWithType: "BulkExecutionOptions.setMaxMicroBatchSize(int maxMicroBatchSize)"
  summary: "The maximum batching size for bulk operations. This value determines number of operations executed in one request. There is an upper limit on both number of operations and sum of size of operations. Any overflow is internally retried. Another instance is: Currently we support a max limit of 200KB, and user select batch size to be 100 and individual documents are of size 20KB, approximately 90 operations will always be retried. So it's better to choose a batch size of 10 here if user is aware of there workload. If sizes are totally unknown and user cannot put a number on it then retries are handled, so no issues as such. If the retry rate exceeds \\`getMaxMicroBatchInterval\\` the micro batch size gets dynamically reduced at runtime"
  parameters:
  - description: "batching size."
    name: "maxMicroBatchSize"
    type: "<xref href=\"int?alt=int&text=int\" data-throw-if-not-resolved=\"False\" />"
  syntax: "public BulkExecutionOptions setMaxMicroBatchSize(int maxMicroBatchSize)"
  returns:
    description: "the bulk processing options."
    type: "<xref href=\"com.azure.cosmos.BulkExecutionOptions?alt=com.azure.cosmos.BulkExecutionOptions&text=BulkExecutionOptions\" data-throw-if-not-resolved=\"False\" />"
type: "method"
metadata: {}
package: "com.azure.cosmos"
artifact: com.azure:azure-cosmos:4.20.1
