### YamlMime:JavaType
uid: "com.azure.search.documents.indexes.models.NGramTokenizer"
fullName: "com.azure.search.documents.indexes.models.NGramTokenizer"
name: "NGramTokenizer"
nameWithType: "NGramTokenizer"
summary: "Tokenizes the input into n-grams of the given size(s). This tokenizer is implemented using Apache Lucene."
inheritances:
- "<xref href=\"java.lang.Object\" data-throw-if-not-resolved=\"False\" />"
- "<xref href=\"com.azure.search.documents.indexes.models.LexicalTokenizer\" data-throw-if-not-resolved=\"False\" />"
inheritedMembers:
- "com.azure.search.documents.indexes.models.LexicalTokenizer.getName()"
- "java.lang.Object.clone()"
- "java.lang.Object.equals(java.lang.Object)"
- "java.lang.Object.finalize()"
- "java.lang.Object.getClass()"
- "java.lang.Object.hashCode()"
- "java.lang.Object.notify()"
- "java.lang.Object.notifyAll()"
- "java.lang.Object.toString()"
- "java.lang.Object.wait()"
- "java.lang.Object.wait(long)"
- "java.lang.Object.wait(long,int)"
syntax: "public final class NGramTokenizer extends LexicalTokenizer"
constructors:
- "com.azure.search.documents.indexes.models.NGramTokenizer.NGramTokenizer(java.lang.String)"
methods:
- "com.azure.search.documents.indexes.models.NGramTokenizer.getMaxGram()"
- "com.azure.search.documents.indexes.models.NGramTokenizer.getMinGram()"
- "com.azure.search.documents.indexes.models.NGramTokenizer.getTokenChars()"
- "com.azure.search.documents.indexes.models.NGramTokenizer.setMaxGram(java.lang.Integer)"
- "com.azure.search.documents.indexes.models.NGramTokenizer.setMinGram(java.lang.Integer)"
- "com.azure.search.documents.indexes.models.NGramTokenizer.setTokenChars(com.azure.search.documents.indexes.models.TokenCharacterKind...)"
- "com.azure.search.documents.indexes.models.NGramTokenizer.setTokenChars(java.util.List<com.azure.search.documents.indexes.models.TokenCharacterKind>)"
type: "class"
metadata: {}
package: "com.azure.search.documents.indexes.models"
artifact: com.azure:azure-search-documents:11.1.3
