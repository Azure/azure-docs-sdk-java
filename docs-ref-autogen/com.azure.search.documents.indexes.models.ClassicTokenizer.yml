### YamlMime:ManagedReference
items:
- uid: com.azure.search.documents.indexes.models.ClassicTokenizer
  id: ClassicTokenizer
  artifact: com.azure:azure-search-documents:11.0.0
  parent: com.azure.search.documents.indexes.models
  children:
  - com.azure.search.documents.indexes.models.ClassicTokenizer.ClassicTokenizer(java.lang.String)
  - com.azure.search.documents.indexes.models.ClassicTokenizer.getMaxTokenLength()
  - com.azure.search.documents.indexes.models.ClassicTokenizer.setMaxTokenLength(java.lang.Integer)
  langs:
  - java
  name: ClassicTokenizer
  nameWithType: ClassicTokenizer
  fullName: com.azure.search.documents.indexes.models.ClassicTokenizer
  type: Class
  package: com.azure.search.documents.indexes.models
  summary: Grammar-based tokenizer that is suitable for processing most European-language documents. This tokenizer is implemented using Apache Lucene.
  syntax:
    content: public final class ClassicTokenizer extends LexicalTokenizer
  inheritance:
  - java.lang.Object
  - com.azure.search.documents.indexes.models.LexicalTokenizer
  inheritedMembers:
  - com.azure.search.documents.indexes.models.LexicalTokenizer.getName()
  - java.lang.Object.clone()
  - java.lang.Object.equals(java.lang.Object)
  - java.lang.Object.finalize()
  - java.lang.Object.getClass()
  - java.lang.Object.hashCode()
  - java.lang.Object.notify()
  - java.lang.Object.notifyAll()
  - java.lang.Object.toString()
  - java.lang.Object.wait()
  - java.lang.Object.wait(long)
  - java.lang.Object.wait(long,int)
- uid: com.azure.search.documents.indexes.models.ClassicTokenizer.ClassicTokenizer(java.lang.String)
  id: ClassicTokenizer(java.lang.String)
  artifact: com.azure:azure-search-documents:11.0.0
  parent: com.azure.search.documents.indexes.models.ClassicTokenizer
  langs:
  - java
  name: ClassicTokenizer(String name)
  nameWithType: ClassicTokenizer.ClassicTokenizer(String name)
  fullName: com.azure.search.documents.indexes.models.ClassicTokenizer.ClassicTokenizer(String name)
  overload: com.azure.search.documents.indexes.models.ClassicTokenizer.ClassicTokenizer*
  type: Constructor
  package: com.azure.search.documents.indexes.models
  summary: Constructor of <xref uid="com.azure.search.documents.indexes.models.ClassicTokenizer" data-throw-if-not-resolved="false">ClassicTokenizer</xref>.
  syntax:
    content: public ClassicTokenizer(String name)
    parameters:
    - id: name
      type: java.lang.String
      description: >-
        The name of the token filter. It must only contain letters, digits,
         spaces, dashes or underscores, can only start and end with alphanumeric
         characters, and is limited to 128 characters.
- uid: com.azure.search.documents.indexes.models.ClassicTokenizer.getMaxTokenLength()
  id: getMaxTokenLength()
  artifact: com.azure:azure-search-documents:11.0.0
  parent: com.azure.search.documents.indexes.models.ClassicTokenizer
  langs:
  - java
  name: getMaxTokenLength()
  nameWithType: ClassicTokenizer.getMaxTokenLength()
  fullName: com.azure.search.documents.indexes.models.ClassicTokenizer.getMaxTokenLength()
  overload: com.azure.search.documents.indexes.models.ClassicTokenizer.getMaxTokenLength*
  type: Method
  package: com.azure.search.documents.indexes.models
  summary: 'Get the maxTokenLength property: The maximum token length. Default is 255. Tokens longer than the maximum length are split. The maximum token length that can be used is 300 characters.'
  syntax:
    content: public Integer getMaxTokenLength()
    return:
      type: java.lang.Integer
      description: the maxTokenLength value.
- uid: com.azure.search.documents.indexes.models.ClassicTokenizer.setMaxTokenLength(java.lang.Integer)
  id: setMaxTokenLength(java.lang.Integer)
  artifact: com.azure:azure-search-documents:11.0.0
  parent: com.azure.search.documents.indexes.models.ClassicTokenizer
  langs:
  - java
  name: setMaxTokenLength(Integer maxTokenLength)
  nameWithType: ClassicTokenizer.setMaxTokenLength(Integer maxTokenLength)
  fullName: com.azure.search.documents.indexes.models.ClassicTokenizer.setMaxTokenLength(Integer maxTokenLength)
  overload: com.azure.search.documents.indexes.models.ClassicTokenizer.setMaxTokenLength*
  type: Method
  package: com.azure.search.documents.indexes.models
  summary: 'Set the maxTokenLength property: The maximum token length. Default is 255. Tokens longer than the maximum length are split. The maximum token length that can be used is 300 characters.'
  syntax:
    content: public ClassicTokenizer setMaxTokenLength(Integer maxTokenLength)
    parameters:
    - id: maxTokenLength
      type: java.lang.Integer
      description: the maxTokenLength value to set.
    return:
      type: com.azure.search.documents.indexes.models.ClassicTokenizer
      description: the ClassicTokenizer object itself.
references:
- uid: java.lang.String
  spec.java:
  - uid: java.lang.String
    name: String
    fullName: java.lang.String
- uid: com.azure.search.documents.indexes.models.ClassicTokenizer.ClassicTokenizer*
  name: ClassicTokenizer
  nameWithType: ClassicTokenizer.ClassicTokenizer
  fullName: com.azure.search.documents.indexes.models.ClassicTokenizer.ClassicTokenizer
  package: com.azure.search.documents.indexes.models
- uid: java.lang.Integer
  spec.java:
  - uid: java.lang.Integer
    name: Integer
    fullName: java.lang.Integer
- uid: com.azure.search.documents.indexes.models.ClassicTokenizer.getMaxTokenLength*
  name: getMaxTokenLength
  nameWithType: ClassicTokenizer.getMaxTokenLength
  fullName: com.azure.search.documents.indexes.models.ClassicTokenizer.getMaxTokenLength
  package: com.azure.search.documents.indexes.models
- uid: com.azure.search.documents.indexes.models.ClassicTokenizer.setMaxTokenLength*
  name: setMaxTokenLength
  nameWithType: ClassicTokenizer.setMaxTokenLength
  fullName: com.azure.search.documents.indexes.models.ClassicTokenizer.setMaxTokenLength
  package: com.azure.search.documents.indexes.models
- uid: com.azure.search.documents.indexes.models.LexicalTokenizer
  name: LexicalTokenizer
  nameWithType: LexicalTokenizer
  fullName: com.azure.search.documents.indexes.models.LexicalTokenizer
- uid: java.lang.Object.notify()
  name: Object.notify()
  nameWithType: Object.notify()
  fullName: java.lang.Object.notify()
- uid: java.lang.Object.wait()
  name: Object.wait()
  nameWithType: Object.wait()
  fullName: java.lang.Object.wait()
- uid: java.lang.Object.finalize()
  name: Object.finalize()
  nameWithType: Object.finalize()
  fullName: java.lang.Object.finalize()
- uid: java.lang.Object.clone()
  name: Object.clone()
  nameWithType: Object.clone()
  fullName: java.lang.Object.clone()
- uid: java.lang.Object.notifyAll()
  name: Object.notifyAll()
  nameWithType: Object.notifyAll()
  fullName: java.lang.Object.notifyAll()
- uid: com.azure.search.documents.indexes.models.LexicalTokenizer.getName()
  name: LexicalTokenizer.getName()
  nameWithType: LexicalTokenizer.getName()
  fullName: com.azure.search.documents.indexes.models.LexicalTokenizer.getName()
- uid: java.lang.Object.equals(java.lang.Object)
  name: Object.equals(Object)
  nameWithType: Object.equals(Object)
  fullName: java.lang.Object.equals(java.lang.Object)
- uid: java.lang.Object.getClass()
  name: Object.getClass()
  nameWithType: Object.getClass()
  fullName: java.lang.Object.getClass()
- uid: java.lang.Object.wait(long)
  name: Object.wait(long)
  nameWithType: Object.wait(long)
  fullName: java.lang.Object.wait(long)
- uid: java.lang.Object.hashCode()
  name: Object.hashCode()
  nameWithType: Object.hashCode()
  fullName: java.lang.Object.hashCode()
- uid: java.lang.Object.wait(long,int)
  name: Object.wait(long,int)
  nameWithType: Object.wait(long,int)
  fullName: java.lang.Object.wait(long,int)
- uid: java.lang.Object.toString()
  name: Object.toString()
  nameWithType: Object.toString()
  fullName: java.lang.Object.toString()
